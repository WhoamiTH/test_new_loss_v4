/home/z5102138/anaconda3/envs/py36/bin/python
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.09/record_1/MLP_20000_0.09_1
----------------------



epoch 1, loss 0.6805, train acc 36.26%, f1 0.5023, precision 0.3374, recall 0.9821, auc 0.5215
epoch 501, loss 0.2679, train acc 83.63%, f1 0.7778, precision 0.7000, recall 0.8750, auc 0.8462
epoch 1001, loss 0.3620, train acc 84.80%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8686
epoch 1501, loss 0.3680, train acc 85.96%, f1 0.8125, precision 0.7222, recall 0.9286, auc 0.8773
epoch 2001, loss 0.2663, train acc 87.13%, f1 0.8254, precision 0.7429, recall 0.9286, auc 0.8860
epoch 2501, loss 0.2079, train acc 87.72%, f1 0.8320, precision 0.7536, recall 0.9286, auc 0.8904
epoch 3001, loss 0.3106, train acc 88.30%, f1 0.8387, precision 0.7647, recall 0.9286, auc 0.8947
epoch 3501, loss 0.1697, train acc 90.06%, f1 0.8595, precision 0.8000, recall 0.9286, auc 0.9078
epoch 4001, loss 0.2573, train acc 90.64%, f1 0.8689, precision 0.8030, recall 0.9464, auc 0.9167
epoch 4501, loss 0.2897, train acc 90.64%, f1 0.8689, precision 0.8030, recall 0.9464, auc 0.9167
epoch 5001, loss 0.2503, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 5501, loss 0.2434, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 6001, loss 0.1275, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 6501, loss 0.1767, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 7001, loss 0.1336, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 7501, loss 0.2205, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 8001, loss 0.1230, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 8501, loss 0.2508, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9001, loss 0.1472, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9501, loss 0.1141, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10001, loss 0.0931, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10501, loss 0.1786, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11001, loss 0.1220, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11501, loss 0.0581, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12001, loss 0.1128, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12501, loss 0.1207, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13001, loss 0.0748, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13501, loss 0.0780, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14001, loss 0.0881, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.0542, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15001, loss 0.0545, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15501, loss 0.0480, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16001, loss 0.0825, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16501, loss 0.0687, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.0599, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.1146, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.1256, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.0417, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.0951, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.1171, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 20.155085389
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.09
normal_0.5
./test_glass0/model_MLP_20000_0.09/record_1/MLP_20000_0.09_1
./test_glass0/result_MLP_20000_0.09_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.09/record_1/MLP_20000_0.09_2
----------------------



epoch 1, loss 0.6439, train acc 64.91%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4826
epoch 501, loss 0.2915, train acc 83.63%, f1 0.7705, precision 0.7121, recall 0.8393, auc 0.8370
epoch 1001, loss 0.3136, train acc 85.96%, f1 0.8033, precision 0.7424, recall 0.8750, auc 0.8636
epoch 1501, loss 0.2399, train acc 88.30%, f1 0.8361, precision 0.7727, recall 0.9107, auc 0.8901
epoch 2001, loss 0.3016, train acc 89.47%, f1 0.8525, precision 0.7879, recall 0.9286, auc 0.9034
epoch 2501, loss 0.2405, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 3001, loss 0.2513, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 3501, loss 0.1494, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 4001, loss 0.1735, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 4501, loss 0.2028, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 5001, loss 0.0901, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 5501, loss 0.1387, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 6001, loss 0.1122, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 6501, loss 0.1779, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 7001, loss 0.0804, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 7501, loss 0.0865, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 8001, loss 0.1640, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 8501, loss 0.0673, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 9001, loss 0.1246, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 9501, loss 0.0807, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10001, loss 0.0569, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10501, loss 0.0516, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11001, loss 0.0463, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11501, loss 0.1006, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 12001, loss 0.0597, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 12501, loss 0.0537, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 13001, loss 0.0568, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 13501, loss 0.1078, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 14001, loss 0.0484, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 14501, loss 0.0540, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 15001, loss 0.0945, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 15501, loss 0.0546, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 16001, loss 0.0868, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 16501, loss 0.0457, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 17001, loss 0.0902, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 17501, loss 0.0899, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18001, loss 0.0441, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18501, loss 0.0278, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19001, loss 0.0447, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19501, loss 0.0980, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 20.079423412
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.09
normal_0.5
./test_glass0/model_MLP_20000_0.09/record_1/MLP_20000_0.09_2
./test_glass0/result_MLP_20000_0.09_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.09/record_1/MLP_20000_0.09_3
----------------------



epoch 1, loss 0.6512, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3858, train acc 80.70%, f1 0.7317, precision 0.6716, recall 0.8036, auc 0.8061
epoch 1001, loss 0.3105, train acc 83.63%, f1 0.7742, precision 0.7059, recall 0.8571, auc 0.8416
epoch 1501, loss 0.2394, train acc 85.96%, f1 0.8000, precision 0.7500, recall 0.8571, auc 0.8590
epoch 2001, loss 0.2974, train acc 88.30%, f1 0.8305, precision 0.7903, recall 0.8750, auc 0.8810
epoch 2501, loss 0.2146, train acc 88.89%, f1 0.8455, precision 0.7761, recall 0.9286, auc 0.8991
epoch 3001, loss 0.2497, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 3501, loss 0.2312, train acc 91.81%, f1 0.8793, precision 0.8500, recall 0.9107, auc 0.9162
epoch 4001, loss 0.1583, train acc 91.23%, f1 0.8718, precision 0.8361, recall 0.9107, auc 0.9119
epoch 4501, loss 0.0969, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 5001, loss 0.1753, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 5501, loss 0.1494, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 6001, loss 0.1473, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 6501, loss 0.1590, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 7001, loss 0.1119, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 7501, loss 0.1088, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 8001, loss 0.1445, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 8501, loss 0.1022, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 9001, loss 0.0936, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 9501, loss 0.0864, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 10001, loss 0.1098, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 10501, loss 0.0865, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 11001, loss 0.1004, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 11501, loss 0.1019, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 12001, loss 0.0837, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 12501, loss 0.0538, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 13001, loss 0.0621, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 13501, loss 0.0571, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 14001, loss 0.0616, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 14501, loss 0.0628, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 15001, loss 0.1018, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 15501, loss 0.0602, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 16001, loss 0.0354, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 16501, loss 0.0573, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 17001, loss 0.0458, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 17501, loss 0.1155, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18001, loss 0.0646, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18501, loss 0.0832, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19001, loss 0.0919, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19501, loss 0.0594, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 20.179806562
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.09
normal_0.5
./test_glass0/model_MLP_20000_0.09/record_1/MLP_20000_0.09_3
./test_glass0/result_MLP_20000_0.09_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.09/record_1/MLP_20000_0.09_4
----------------------



epoch 1, loss 0.6366, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3577, train acc 79.53%, f1 0.7244, precision 0.6479, recall 0.8214, auc 0.8020
epoch 1001, loss 0.3371, train acc 81.87%, f1 0.7520, precision 0.6812, recall 0.8393, auc 0.8240
epoch 1501, loss 0.3675, train acc 85.96%, f1 0.8000, precision 0.7500, recall 0.8571, auc 0.8590
epoch 2001, loss 0.2040, train acc 86.55%, f1 0.8130, precision 0.7463, recall 0.8929, auc 0.8725
epoch 2501, loss 0.2153, train acc 89.47%, f1 0.8448, precision 0.8167, recall 0.8750, auc 0.8897
epoch 3001, loss 0.3066, train acc 90.64%, f1 0.8644, precision 0.8226, recall 0.9107, auc 0.9075
epoch 3501, loss 0.2607, train acc 90.64%, f1 0.8644, precision 0.8226, recall 0.9107, auc 0.9075
epoch 4001, loss 0.2971, train acc 91.23%, f1 0.8718, precision 0.8361, recall 0.9107, auc 0.9119
epoch 4501, loss 0.1720, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 5001, loss 0.1401, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 5501, loss 0.2238, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 6001, loss 0.1892, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 6501, loss 0.1876, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 7001, loss 0.1782, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 7501, loss 0.1298, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 8001, loss 0.1137, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 8501, loss 0.1613, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 9001, loss 0.1739, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 9501, loss 0.1291, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 10001, loss 0.1251, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 10501, loss 0.0802, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 11001, loss 0.1881, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 11501, loss 0.0636, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 12001, loss 0.0988, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 12501, loss 0.1290, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 13001, loss 0.1692, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 13501, loss 0.1499, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 14001, loss 0.1024, train acc 95.91%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9558
epoch 14501, loss 0.1415, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 15001, loss 0.1267, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 15501, loss 0.0914, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 16001, loss 0.1082, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 16501, loss 0.1145, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 17001, loss 0.1287, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17501, loss 0.0809, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18001, loss 0.0430, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18501, loss 0.0338, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 19001, loss 0.0486, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 19501, loss 0.1162, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
running_time is 20.131436665
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.09
normal_0.5
./test_glass0/model_MLP_20000_0.09/record_1/MLP_20000_0.09_4
./test_glass0/result_MLP_20000_0.09_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.09/record_1/MLP_20000_0.09_5
----------------------



epoch 1, loss 0.6369, train acc 63.95%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4741
epoch 501, loss 0.3516, train acc 80.81%, f1 0.7442, precision 0.6575, recall 0.8571, auc 0.8208
epoch 1001, loss 0.3169, train acc 83.72%, f1 0.7778, precision 0.7000, recall 0.8750, auc 0.8470
epoch 1501, loss 0.3306, train acc 86.63%, f1 0.8189, precision 0.7324, recall 0.9286, auc 0.8824
epoch 2001, loss 0.3129, train acc 86.05%, f1 0.8095, precision 0.7286, recall 0.9107, auc 0.8735
epoch 2501, loss 0.2195, train acc 87.21%, f1 0.8226, precision 0.7500, recall 0.9107, auc 0.8821
epoch 3001, loss 0.3027, train acc 87.21%, f1 0.8226, precision 0.7500, recall 0.9107, auc 0.8821
epoch 3501, loss 0.2274, train acc 88.37%, f1 0.8387, precision 0.7647, recall 0.9286, auc 0.8953
epoch 4001, loss 0.1915, train acc 91.28%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9261
epoch 4501, loss 0.1836, train acc 91.86%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9304
epoch 5001, loss 0.2069, train acc 91.86%, f1 0.8814, precision 0.8387, recall 0.9286, auc 0.9212
epoch 5501, loss 0.1923, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 6001, loss 0.1506, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 6501, loss 0.1327, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 7001, loss 0.1880, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 7501, loss 0.2122, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 8001, loss 0.1066, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 8501, loss 0.1656, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 9001, loss 0.1934, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 9501, loss 0.1889, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 10001, loss 0.1493, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 10501, loss 0.2025, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 11001, loss 0.1222, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 11501, loss 0.1545, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 12001, loss 0.0960, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 12501, loss 0.0655, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 13001, loss 0.1258, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 13501, loss 0.1008, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 14001, loss 0.0724, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 14501, loss 0.0950, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 15001, loss 0.0911, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 15501, loss 0.1269, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 16001, loss 0.1217, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 16501, loss 0.1791, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 17001, loss 0.1013, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 17501, loss 0.0744, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 18001, loss 0.1649, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 18501, loss 0.1074, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 19001, loss 0.0910, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 19501, loss 0.1201, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
running_time is 20.24576325
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.09
normal_0.5
./test_glass0/model_MLP_20000_0.09/record_1/MLP_20000_0.09_5
./test_glass0/result_MLP_20000_0.09_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.1/record_1/MLP_20000_0.1_1
----------------------



epoch 1, loss 0.6488, train acc 64.91%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4826
epoch 501, loss 0.3227, train acc 83.63%, f1 0.7778, precision 0.7000, recall 0.8750, auc 0.8462
epoch 1001, loss 0.3943, train acc 85.38%, f1 0.8062, precision 0.7123, recall 0.9286, auc 0.8730
epoch 1501, loss 0.2574, train acc 85.96%, f1 0.8125, precision 0.7222, recall 0.9286, auc 0.8773
epoch 2001, loss 0.2623, train acc 88.30%, f1 0.8387, precision 0.7647, recall 0.9286, auc 0.8947
epoch 2501, loss 0.1840, train acc 90.06%, f1 0.8618, precision 0.7910, recall 0.9464, auc 0.9123
epoch 3001, loss 0.1779, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 3501, loss 0.2690, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 4001, loss 0.2197, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 4501, loss 0.1634, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 5001, loss 0.1887, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 5501, loss 0.1217, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 6001, loss 0.1010, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 6501, loss 0.1337, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 7001, loss 0.1576, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 7501, loss 0.1462, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 8001, loss 0.1280, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 8501, loss 0.0825, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9001, loss 0.0772, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 9501, loss 0.1110, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10001, loss 0.0694, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10501, loss 0.0495, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11001, loss 0.0639, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 11501, loss 0.0609, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12001, loss 0.0491, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12501, loss 0.0990, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 13001, loss 0.0518, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 13501, loss 0.0925, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 14001, loss 0.0989, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 14501, loss 0.0390, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 15001, loss 0.0546, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 15501, loss 0.0748, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 16001, loss 0.1375, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 16501, loss 0.0554, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17001, loss 0.0436, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17501, loss 0.0922, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18001, loss 0.0970, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18501, loss 0.0342, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19001, loss 0.0481, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19501, loss 0.0799, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 20.006266796
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.1
normal_0.5
./test_glass0/model_MLP_20000_0.1/record_1/MLP_20000_0.1_1
./test_glass0/result_MLP_20000_0.1_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.1/record_1/MLP_20000_0.1_2
----------------------



epoch 1, loss 0.6809, train acc 58.48%, f1 0.6034, precision 0.4390, recall 0.9643, auc 0.6821
epoch 501, loss 0.2974, train acc 83.04%, f1 0.7717, precision 0.6901, recall 0.8750, auc 0.8418
epoch 1001, loss 0.3038, train acc 84.80%, f1 0.7937, precision 0.7143, recall 0.8929, auc 0.8595
epoch 1501, loss 0.3431, train acc 87.72%, f1 0.8293, precision 0.7612, recall 0.9107, auc 0.8858
epoch 2001, loss 0.1884, train acc 89.47%, f1 0.8525, precision 0.7879, recall 0.9286, auc 0.9034
epoch 2501, loss 0.1862, train acc 89.47%, f1 0.8525, precision 0.7879, recall 0.9286, auc 0.9034
epoch 3001, loss 0.1897, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 3501, loss 0.2166, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 4001, loss 0.1914, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 4501, loss 0.1394, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 5001, loss 0.1648, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 5501, loss 0.1949, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 6001, loss 0.1476, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 6501, loss 0.1497, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 7001, loss 0.1031, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 7501, loss 0.1746, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 8001, loss 0.1892, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 8501, loss 0.1978, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 9001, loss 0.0903, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 9501, loss 0.1742, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 10001, loss 0.1430, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 10501, loss 0.1083, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 11001, loss 0.1196, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 11501, loss 0.1278, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 12001, loss 0.1447, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 12501, loss 0.0869, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 13001, loss 0.1266, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 13501, loss 0.1077, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 14001, loss 0.0909, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 14501, loss 0.1142, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 15001, loss 0.0535, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 15501, loss 0.1408, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 16001, loss 0.1400, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 16501, loss 0.1286, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 17001, loss 0.1289, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 17501, loss 0.0779, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 18001, loss 0.0631, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 18501, loss 0.1195, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 19001, loss 0.0832, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19501, loss 0.1006, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
running_time is 20.122494784
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.1
normal_0.5
./test_glass0/model_MLP_20000_0.1/record_1/MLP_20000_0.1_2
./test_glass0/result_MLP_20000_0.1_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.1/record_1/MLP_20000_0.1_3
----------------------



epoch 1, loss 0.6485, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3239, train acc 81.87%, f1 0.7520, precision 0.6812, recall 0.8393, auc 0.8240
epoch 1001, loss 0.3787, train acc 84.21%, f1 0.7805, precision 0.7164, recall 0.8571, auc 0.8460
epoch 1501, loss 0.3463, train acc 84.80%, f1 0.7869, precision 0.7273, recall 0.8571, auc 0.8503
epoch 2001, loss 0.2177, train acc 87.13%, f1 0.8226, precision 0.7500, recall 0.9107, auc 0.8814
epoch 2501, loss 0.2591, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 3001, loss 0.1995, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 3501, loss 0.2037, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 4001, loss 0.1387, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 4501, loss 0.1599, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 5001, loss 0.1968, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 5501, loss 0.1883, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 6001, loss 0.1285, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 6501, loss 0.1723, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 7001, loss 0.1061, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 7501, loss 0.1576, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8001, loss 0.0967, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8501, loss 0.0777, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 9001, loss 0.1103, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 9501, loss 0.1469, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10001, loss 0.1085, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10501, loss 0.0828, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11001, loss 0.1788, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11501, loss 0.0435, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 12001, loss 0.1000, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 12501, loss 0.1060, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 13001, loss 0.1094, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 13501, loss 0.0625, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14001, loss 0.0598, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14501, loss 0.1056, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15001, loss 0.0585, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 15501, loss 0.1041, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 16001, loss 0.0645, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 16501, loss 0.0589, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 17001, loss 0.0749, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 17501, loss 0.0653, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18001, loss 0.0911, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18501, loss 0.0412, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19001, loss 0.0618, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19501, loss 0.0656, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 20.087746745
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.1
normal_0.5
./test_glass0/model_MLP_20000_0.1/record_1/MLP_20000_0.1_3
./test_glass0/result_MLP_20000_0.1_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.1/record_1/MLP_20000_0.1_4
----------------------



epoch 1, loss 0.6326, train acc 63.16%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4696
epoch 501, loss 0.4233, train acc 80.12%, f1 0.7344, precision 0.6528, recall 0.8393, auc 0.8109
epoch 1001, loss 0.3429, train acc 82.46%, f1 0.7692, precision 0.6757, recall 0.8929, auc 0.8421
epoch 1501, loss 0.2672, train acc 83.04%, f1 0.7752, precision 0.6849, recall 0.8929, auc 0.8464
epoch 2001, loss 0.2675, train acc 85.96%, f1 0.8125, precision 0.7222, recall 0.9286, auc 0.8773
epoch 2501, loss 0.3169, train acc 86.55%, f1 0.8160, precision 0.7391, recall 0.9107, auc 0.8771
epoch 3001, loss 0.2064, train acc 88.89%, f1 0.8455, precision 0.7761, recall 0.9286, auc 0.8991
epoch 3501, loss 0.1932, train acc 91.23%, f1 0.8718, precision 0.8361, recall 0.9107, auc 0.9119
epoch 4001, loss 0.2295, train acc 90.64%, f1 0.8667, precision 0.8125, recall 0.9286, auc 0.9121
epoch 4501, loss 0.2438, train acc 90.64%, f1 0.8667, precision 0.8125, recall 0.9286, auc 0.9121
epoch 5001, loss 0.2096, train acc 91.81%, f1 0.8814, precision 0.8387, recall 0.9286, auc 0.9208
epoch 5501, loss 0.1351, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 6001, loss 0.1917, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 6501, loss 0.1706, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 7001, loss 0.2112, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 7501, loss 0.2156, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 8001, loss 0.1763, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 8501, loss 0.1158, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 9001, loss 0.1332, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 9501, loss 0.1013, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 10001, loss 0.1774, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 10501, loss 0.0908, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 11001, loss 0.1304, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 11501, loss 0.1161, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 12001, loss 0.1484, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 12501, loss 0.1086, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 13001, loss 0.1258, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 13501, loss 0.0648, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 14001, loss 0.1157, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 14501, loss 0.1093, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 15001, loss 0.0866, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15501, loss 0.1246, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 16001, loss 0.1230, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 16501, loss 0.1064, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.1201, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.0787, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.0454, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18501, loss 0.0761, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.0833, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19501, loss 0.0633, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 20.110636129
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.1
normal_0.5
./test_glass0/model_MLP_20000_0.1/record_1/MLP_20000_0.1_4
./test_glass0/result_MLP_20000_0.1_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.1/record_1/MLP_20000_0.1_5
----------------------



epoch 1, loss 0.6164, train acc 62.21%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4612
epoch 501, loss 0.4249, train acc 80.23%, f1 0.7385, precision 0.6486, recall 0.8571, auc 0.8165
epoch 1001, loss 0.3286, train acc 82.56%, f1 0.7692, precision 0.6757, recall 0.8929, auc 0.8430
epoch 1501, loss 0.3778, train acc 83.14%, f1 0.7786, precision 0.6800, recall 0.9107, auc 0.8519
epoch 2001, loss 0.3103, train acc 85.47%, f1 0.8062, precision 0.7123, recall 0.9286, auc 0.8738
epoch 2501, loss 0.3965, train acc 87.21%, f1 0.8281, precision 0.7361, recall 0.9464, auc 0.8913
epoch 3001, loss 0.3004, train acc 87.79%, f1 0.8346, precision 0.7465, recall 0.9464, auc 0.8956
epoch 3501, loss 0.2589, train acc 88.95%, f1 0.8480, precision 0.7681, recall 0.9464, auc 0.9042
epoch 4001, loss 0.2020, train acc 90.12%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9175
epoch 4501, loss 0.1855, train acc 91.28%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9261
epoch 5001, loss 0.2279, train acc 91.28%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9307
epoch 5501, loss 0.1902, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 6001, loss 0.1472, train acc 92.44%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9393
epoch 6501, loss 0.1566, train acc 92.44%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9393
epoch 7001, loss 0.1316, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 7501, loss 0.1777, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 8001, loss 0.1238, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 8501, loss 0.1458, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 9001, loss 0.1246, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 9501, loss 0.0924, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 10001, loss 0.0926, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 10501, loss 0.1107, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 11001, loss 0.1498, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 11501, loss 0.0997, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 12001, loss 0.1100, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 12501, loss 0.0575, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 13001, loss 0.0927, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 13501, loss 0.1097, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 14001, loss 0.0763, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 14501, loss 0.0804, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 15001, loss 0.1242, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 15501, loss 0.0858, train acc 97.67%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9781
epoch 16001, loss 0.0868, train acc 97.67%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9781
epoch 16501, loss 0.0603, train acc 98.26%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9825
epoch 17001, loss 0.0687, train acc 98.26%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9825
epoch 17501, loss 0.0906, train acc 98.84%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9868
epoch 18001, loss 0.0751, train acc 98.26%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9825
epoch 18501, loss 0.0681, train acc 98.84%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9868
epoch 19001, loss 0.0729, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19501, loss 0.0425, train acc 98.84%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9868
running_time is 19.994792032
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.1
normal_0.5
./test_glass0/model_MLP_20000_0.1/record_1/MLP_20000_0.1_5
./test_glass0/result_MLP_20000_0.1_normal_0.5/record_1/
----------------------



the AUC is 0.5357142857142857

the Fscore is 0.13333333333333333

the precision is 1.0

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.11/record_1/MLP_20000_0.11_1
----------------------



epoch 1, loss 0.6195, train acc 66.67%, f1 0.3596, precision 0.4848, recall 0.2857, auc 0.5689
epoch 501, loss 0.3533, train acc 84.21%, f1 0.7907, precision 0.6986, recall 0.9107, auc 0.8597
epoch 1001, loss 0.2832, train acc 84.80%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8686
epoch 1501, loss 0.3472, train acc 84.80%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8686
epoch 2001, loss 0.3666, train acc 84.80%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8686
epoch 2501, loss 0.2950, train acc 86.55%, f1 0.8189, precision 0.7324, recall 0.9286, auc 0.8817
epoch 3001, loss 0.3851, train acc 87.13%, f1 0.8254, precision 0.7429, recall 0.9286, auc 0.8860
epoch 3501, loss 0.2412, train acc 87.72%, f1 0.8346, precision 0.7465, recall 0.9464, auc 0.8950
epoch 4001, loss 0.1583, train acc 87.72%, f1 0.8320, precision 0.7536, recall 0.9286, auc 0.8904
epoch 4501, loss 0.2953, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 5001, loss 0.2034, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 5501, loss 0.2248, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 6001, loss 0.1600, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 6501, loss 0.1305, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 7001, loss 0.1042, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 7501, loss 0.1067, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 8001, loss 0.1474, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 8501, loss 0.1494, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9001, loss 0.1827, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9501, loss 0.1487, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10001, loss 0.0995, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10501, loss 0.1348, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 11001, loss 0.0889, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 11501, loss 0.0739, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 12001, loss 0.0781, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 12501, loss 0.0517, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 13001, loss 0.0888, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 13501, loss 0.0571, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 14001, loss 0.0564, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 14501, loss 0.0972, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 15001, loss 0.0448, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 15501, loss 0.0895, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 16001, loss 0.0615, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 16501, loss 0.0837, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 17001, loss 0.0453, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 17501, loss 0.0855, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18001, loss 0.0384, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18501, loss 0.0387, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19001, loss 0.0352, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19501, loss 0.0401, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 20.177646788
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.11
normal_0.5
./test_glass0/model_MLP_20000_0.11/record_1/MLP_20000_0.11_1
./test_glass0/result_MLP_20000_0.11_normal_0.5/record_1/
----------------------



the AUC is 0.5357142857142857

the Fscore is 0.13333333333333333

the precision is 1.0

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.11/record_1/MLP_20000_0.11_2
----------------------



epoch 1, loss 0.6284, train acc 68.42%, f1 0.4130, precision 0.5278, recall 0.3393, auc 0.5957
epoch 501, loss 0.3900, train acc 82.46%, f1 0.7581, precision 0.6912, recall 0.8393, auc 0.8283
epoch 1001, loss 0.2758, train acc 84.21%, f1 0.7731, precision 0.7302, recall 0.8214, auc 0.8368
epoch 1501, loss 0.2799, train acc 89.47%, f1 0.8525, precision 0.7879, recall 0.9286, auc 0.9034
epoch 2001, loss 0.1919, train acc 90.06%, f1 0.8618, precision 0.7910, recall 0.9464, auc 0.9123
epoch 2501, loss 0.1946, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 3001, loss 0.2123, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 3501, loss 0.2669, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 4001, loss 0.1892, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 4501, loss 0.1171, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 5001, loss 0.1481, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 5501, loss 0.1096, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 6001, loss 0.1387, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 6501, loss 0.0702, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 7001, loss 0.1110, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 7501, loss 0.1375, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 8001, loss 0.1126, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 8501, loss 0.0938, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 9001, loss 0.0858, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 9501, loss 0.1219, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 10001, loss 0.0974, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10501, loss 0.1159, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11001, loss 0.0502, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11501, loss 0.1054, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 12001, loss 0.0646, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12501, loss 0.0793, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13001, loss 0.1306, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13501, loss 0.0818, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14001, loss 0.0866, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14501, loss 0.0606, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15001, loss 0.0732, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 15501, loss 0.1012, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 16001, loss 0.0503, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 16501, loss 0.1338, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17001, loss 0.0546, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17501, loss 0.1190, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18001, loss 0.0998, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18501, loss 0.0794, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19001, loss 0.0520, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19501, loss 0.0298, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 20.110563098
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.11
normal_0.5
./test_glass0/model_MLP_20000_0.11/record_1/MLP_20000_0.11_2
./test_glass0/result_MLP_20000_0.11_normal_0.5/record_1/
----------------------



the AUC is 0.46551724137931033

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.11/record_1/MLP_20000_0.11_3
----------------------



epoch 1, loss 0.5927, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3477, train acc 80.70%, f1 0.7360, precision 0.6667, recall 0.8214, auc 0.8107
epoch 1001, loss 0.3062, train acc 83.63%, f1 0.7778, precision 0.7000, recall 0.8750, auc 0.8462
epoch 1501, loss 0.3026, train acc 85.38%, f1 0.8000, precision 0.7246, recall 0.8929, auc 0.8638
epoch 2001, loss 0.3530, train acc 85.38%, f1 0.8000, precision 0.7246, recall 0.8929, auc 0.8638
epoch 2501, loss 0.2974, train acc 87.72%, f1 0.8293, precision 0.7612, recall 0.9107, auc 0.8858
epoch 3001, loss 0.2204, train acc 89.47%, f1 0.8548, precision 0.7794, recall 0.9464, auc 0.9080
epoch 3501, loss 0.2163, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 4001, loss 0.1572, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 4501, loss 0.1482, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 5001, loss 0.1285, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 5501, loss 0.0984, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 6001, loss 0.1079, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 6501, loss 0.1354, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 7001, loss 0.0988, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 7501, loss 0.1062, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 8001, loss 0.1276, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 8501, loss 0.0680, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 9001, loss 0.0586, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 9501, loss 0.0936, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 10001, loss 0.1442, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 10501, loss 0.1107, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 11001, loss 0.1429, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 11501, loss 0.1451, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 12001, loss 0.1256, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 12501, loss 0.1187, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13001, loss 0.0953, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 13501, loss 0.0959, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 14001, loss 0.1431, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 14501, loss 0.1398, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 15001, loss 0.0888, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15501, loss 0.0572, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16001, loss 0.0948, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 16501, loss 0.1106, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17001, loss 0.1026, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17501, loss 0.0743, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18001, loss 0.0631, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 18501, loss 0.0521, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.0412, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 19501, loss 0.0613, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
running_time is 20.226850945
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.11
normal_0.5
./test_glass0/model_MLP_20000_0.11/record_1/MLP_20000_0.11_3
./test_glass0/result_MLP_20000_0.11_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.11/record_1/MLP_20000_0.11_4
----------------------



epoch 1, loss 0.6286, train acc 59.06%, f1 0.0278, precision 0.0625, recall 0.0179, auc 0.4437
epoch 501, loss 0.4106, train acc 78.95%, f1 0.7187, precision 0.6389, recall 0.8214, auc 0.7977
epoch 1001, loss 0.3759, train acc 82.46%, f1 0.7692, precision 0.6757, recall 0.8929, auc 0.8421
epoch 1501, loss 0.3765, train acc 85.96%, f1 0.8125, precision 0.7222, recall 0.9286, auc 0.8773
epoch 2001, loss 0.3052, train acc 86.55%, f1 0.8160, precision 0.7391, recall 0.9107, auc 0.8771
epoch 2501, loss 0.3114, train acc 88.30%, f1 0.8361, precision 0.7727, recall 0.9107, auc 0.8901
epoch 3001, loss 0.2975, train acc 88.30%, f1 0.8387, precision 0.7647, recall 0.9286, auc 0.8947
epoch 3501, loss 0.1343, train acc 88.89%, f1 0.8455, precision 0.7761, recall 0.9286, auc 0.8991
epoch 4001, loss 0.2671, train acc 90.06%, f1 0.8595, precision 0.8000, recall 0.9286, auc 0.9078
epoch 4501, loss 0.1921, train acc 90.64%, f1 0.8689, precision 0.8030, recall 0.9464, auc 0.9167
epoch 5001, loss 0.2269, train acc 91.23%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9210
epoch 5501, loss 0.2474, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 6001, loss 0.2631, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 6501, loss 0.2417, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 7001, loss 0.1710, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 7501, loss 0.2741, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 8001, loss 0.2487, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 8501, loss 0.1551, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 9001, loss 0.1898, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 9501, loss 0.2548, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 10001, loss 0.2595, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 10501, loss 0.1543, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 11001, loss 0.2002, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 11501, loss 0.1288, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 12001, loss 0.1699, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 12501, loss 0.1943, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 13001, loss 0.2152, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 13501, loss 0.1603, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 14001, loss 0.1586, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 14501, loss 0.1745, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 15001, loss 0.1044, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 15501, loss 0.0629, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 16001, loss 0.1558, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 16501, loss 0.2119, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 17001, loss 0.1638, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 17501, loss 0.1515, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 18001, loss 0.1177, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 18501, loss 0.1489, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 19001, loss 0.0891, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 19501, loss 0.0776, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
running_time is 20.301910032000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.11
normal_0.5
./test_glass0/model_MLP_20000_0.11/record_1/MLP_20000_0.11_4
./test_glass0/result_MLP_20000_0.11_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.11/record_1/MLP_20000_0.11_5
----------------------



epoch 1, loss 0.6553, train acc 67.44%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.4020, train acc 79.65%, f1 0.7368, precision 0.6364, recall 0.8750, auc 0.8168
epoch 1001, loss 0.3691, train acc 82.56%, f1 0.7727, precision 0.6711, recall 0.9107, auc 0.8476
epoch 1501, loss 0.2910, train acc 85.47%, f1 0.8062, precision 0.7123, recall 0.9286, auc 0.8738
epoch 2001, loss 0.2668, train acc 86.05%, f1 0.8125, precision 0.7222, recall 0.9286, auc 0.8781
epoch 2501, loss 0.3134, train acc 87.21%, f1 0.8226, precision 0.7500, recall 0.9107, auc 0.8821
epoch 3001, loss 0.2200, train acc 86.05%, f1 0.8033, precision 0.7424, recall 0.8750, auc 0.8642
epoch 3501, loss 0.2251, train acc 87.79%, f1 0.8320, precision 0.7536, recall 0.9286, auc 0.8910
epoch 4001, loss 0.2463, train acc 89.53%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9132
epoch 4501, loss 0.2326, train acc 91.28%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9215
epoch 5001, loss 0.2223, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 5501, loss 0.1030, train acc 91.86%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9304
epoch 6001, loss 0.1982, train acc 91.86%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9304
epoch 6501, loss 0.2073, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 7001, loss 0.2188, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 7501, loss 0.1129, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 8001, loss 0.1586, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 8501, loss 0.2405, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 9001, loss 0.2472, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 9501, loss 0.1731, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 10001, loss 0.1487, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 10501, loss 0.1927, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 11001, loss 0.1329, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 11501, loss 0.1225, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 12001, loss 0.1839, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 12501, loss 0.1466, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 13001, loss 0.1687, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 13501, loss 0.1554, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 14001, loss 0.1610, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 14501, loss 0.1467, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 15001, loss 0.0816, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 15501, loss 0.1550, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 16001, loss 0.1297, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 16501, loss 0.1199, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 17001, loss 0.1369, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 17501, loss 0.1670, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 18001, loss 0.1383, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 18501, loss 0.1084, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 19001, loss 0.1147, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 19501, loss 0.0922, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
running_time is 20.216215340999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.11
normal_0.5
./test_glass0/model_MLP_20000_0.11/record_1/MLP_20000_0.11_5
./test_glass0/result_MLP_20000_0.11_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.12/record_1/MLP_20000_0.12_1
----------------------



epoch 1, loss 0.6446, train acc 63.16%, f1 0.0597, precision 0.1818, recall 0.0357, auc 0.4787
epoch 501, loss 0.2958, train acc 84.21%, f1 0.7907, precision 0.6986, recall 0.9107, auc 0.8597
epoch 1001, loss 0.3742, train acc 84.21%, f1 0.7939, precision 0.6933, recall 0.9286, auc 0.8643
epoch 1501, loss 0.2525, train acc 84.80%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8686
epoch 2001, loss 0.2057, train acc 86.55%, f1 0.8189, precision 0.7324, recall 0.9286, auc 0.8817
epoch 2501, loss 0.3101, train acc 86.55%, f1 0.8189, precision 0.7324, recall 0.9286, auc 0.8817
epoch 3001, loss 0.2962, train acc 89.47%, f1 0.8548, precision 0.7794, recall 0.9464, auc 0.9080
epoch 3501, loss 0.1788, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 4001, loss 0.2044, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 4501, loss 0.1852, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 5001, loss 0.2248, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 5501, loss 0.1389, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 6001, loss 0.1370, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 6501, loss 0.1279, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 7001, loss 0.0821, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 7501, loss 0.1483, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 8001, loss 0.1756, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 8501, loss 0.1910, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9001, loss 0.0792, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9501, loss 0.1393, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10001, loss 0.1540, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10501, loss 0.1407, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11001, loss 0.0723, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11501, loss 0.0544, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12001, loss 0.1596, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12501, loss 0.0852, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13001, loss 0.0783, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13501, loss 0.0423, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14001, loss 0.0910, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.0474, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15001, loss 0.1244, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.0957, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.0428, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.0424, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.0861, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.0815, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.0968, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.1421, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.1467, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.0852, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 20.110507825
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.12
normal_0.5
./test_glass0/model_MLP_20000_0.12/record_1/MLP_20000_0.12_1
./test_glass0/result_MLP_20000_0.12_normal_0.5/record_1/
----------------------



the AUC is 0.4839901477832512

the Fscore is 0.11111111111111112

the precision is 0.25

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.12/record_1/MLP_20000_0.12_2
----------------------



epoch 1, loss 0.6151, train acc 69.01%, f1 0.1846, precision 0.6667, recall 0.1071, auc 0.5405
epoch 501, loss 0.3579, train acc 81.29%, f1 0.7538, precision 0.6622, recall 0.8750, auc 0.8288
epoch 1001, loss 0.3094, train acc 84.80%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8686
epoch 1501, loss 0.1972, train acc 88.30%, f1 0.8413, precision 0.7571, recall 0.9464, auc 0.8993
epoch 2001, loss 0.2693, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 2501, loss 0.2046, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 3001, loss 0.2356, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 3501, loss 0.1663, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 4001, loss 0.1256, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 4501, loss 0.2057, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 5001, loss 0.0969, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 5501, loss 0.1184, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 6001, loss 0.1733, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 6501, loss 0.1433, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 7001, loss 0.1097, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 7501, loss 0.1028, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 8001, loss 0.1040, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 8501, loss 0.0756, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 9001, loss 0.1192, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 9501, loss 0.1509, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 10001, loss 0.0877, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 10501, loss 0.0701, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 11001, loss 0.1263, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 11501, loss 0.0588, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 12001, loss 0.1672, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 12501, loss 0.0762, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 13001, loss 0.1043, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 13501, loss 0.0491, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 14001, loss 0.1182, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 14501, loss 0.1162, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15001, loss 0.0920, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15501, loss 0.0914, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16001, loss 0.0562, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16501, loss 0.1177, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17001, loss 0.0479, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.1386, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 18001, loss 0.0453, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.0725, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19001, loss 0.0754, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 19501, loss 0.0598, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
running_time is 20.015097574000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.12
normal_0.5
./test_glass0/model_MLP_20000_0.12/record_1/MLP_20000_0.12_2
./test_glass0/result_MLP_20000_0.12_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.12/record_1/MLP_20000_0.12_3
----------------------



epoch 1, loss 0.6348, train acc 51.46%, f1 0.3025, precision 0.2857, recall 0.3214, auc 0.4651
epoch 501, loss 0.4162, train acc 79.53%, f1 0.7287, precision 0.6438, recall 0.8393, auc 0.8066
epoch 1001, loss 0.3703, train acc 80.70%, f1 0.7442, precision 0.6575, recall 0.8571, auc 0.8199
epoch 1501, loss 0.3404, train acc 82.46%, f1 0.7656, precision 0.6806, recall 0.8750, auc 0.8375
epoch 2001, loss 0.3085, train acc 87.13%, f1 0.8197, precision 0.7576, recall 0.8929, auc 0.8769
epoch 2501, loss 0.2710, train acc 88.89%, f1 0.8403, precision 0.7937, recall 0.8929, auc 0.8899
epoch 3001, loss 0.2417, train acc 90.06%, f1 0.8595, precision 0.8000, recall 0.9286, auc 0.9078
epoch 3501, loss 0.2329, train acc 91.23%, f1 0.8718, precision 0.8361, recall 0.9107, auc 0.9119
epoch 4001, loss 0.1370, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 4501, loss 0.2060, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 5001, loss 0.1195, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 5501, loss 0.1615, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 6001, loss 0.0826, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 6501, loss 0.1162, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 7001, loss 0.1619, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 7501, loss 0.1694, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 8001, loss 0.1176, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8501, loss 0.1914, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9001, loss 0.1081, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 9501, loss 0.1378, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 10001, loss 0.0711, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 10501, loss 0.1142, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 11001, loss 0.1459, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 11501, loss 0.2108, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 12001, loss 0.1393, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12501, loss 0.1176, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 13001, loss 0.1036, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 13501, loss 0.0732, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 14001, loss 0.1076, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 14501, loss 0.1079, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 15001, loss 0.1272, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 15501, loss 0.0808, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 16001, loss 0.1422, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 16501, loss 0.0965, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 17001, loss 0.1298, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 17501, loss 0.0835, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 18001, loss 0.1398, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 18501, loss 0.1205, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 19001, loss 0.1349, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 19501, loss 0.0413, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
running_time is 20.187733481000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.12
normal_0.5
./test_glass0/model_MLP_20000_0.12/record_1/MLP_20000_0.12_3
./test_glass0/result_MLP_20000_0.12_normal_0.5/record_1/
----------------------



the AUC is 0.5849753694581281

the Fscore is 0.5306122448979592

the precision is 0.37142857142857144

the recall is 0.9285714285714286

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.12/record_1/MLP_20000_0.12_4
----------------------



epoch 1, loss 0.6642, train acc 58.48%, f1 0.0274, precision 0.0588, recall 0.0179, auc 0.4394
epoch 501, loss 0.4652, train acc 77.78%, f1 0.7206, precision 0.6125, recall 0.8750, auc 0.8027
epoch 1001, loss 0.3161, train acc 83.04%, f1 0.7717, precision 0.6901, recall 0.8750, auc 0.8418
epoch 1501, loss 0.3206, train acc 83.04%, f1 0.7752, precision 0.6849, recall 0.8929, auc 0.8464
epoch 2001, loss 0.2960, train acc 85.96%, f1 0.8033, precision 0.7424, recall 0.8750, auc 0.8636
epoch 2501, loss 0.2429, train acc 85.96%, f1 0.8095, precision 0.7286, recall 0.9107, auc 0.8727
epoch 3001, loss 0.3131, train acc 87.72%, f1 0.8293, precision 0.7612, recall 0.9107, auc 0.8858
epoch 3501, loss 0.3152, train acc 87.72%, f1 0.8320, precision 0.7536, recall 0.9286, auc 0.8904
epoch 4001, loss 0.1502, train acc 89.47%, f1 0.8525, precision 0.7879, recall 0.9286, auc 0.9034
epoch 4501, loss 0.2124, train acc 90.06%, f1 0.8595, precision 0.8000, recall 0.9286, auc 0.9078
epoch 5001, loss 0.2847, train acc 90.64%, f1 0.8667, precision 0.8125, recall 0.9286, auc 0.9121
epoch 5501, loss 0.2105, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 6001, loss 0.2336, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 6501, loss 0.2126, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 7001, loss 0.2096, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 7501, loss 0.1713, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 8001, loss 0.1744, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 8501, loss 0.2213, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 9001, loss 0.1559, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 9501, loss 0.2194, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 10001, loss 0.1935, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 10501, loss 0.1497, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 11001, loss 0.1183, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 11501, loss 0.1878, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 12001, loss 0.1775, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 12501, loss 0.0748, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 13001, loss 0.1922, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 13501, loss 0.1933, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 14001, loss 0.1423, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 14501, loss 0.1598, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 15001, loss 0.1963, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 15501, loss 0.1486, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 16001, loss 0.1104, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 16501, loss 0.1853, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 17001, loss 0.0900, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 17501, loss 0.1745, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 18001, loss 0.1215, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 18501, loss 0.0934, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 19001, loss 0.1510, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 19501, loss 0.1798, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
running_time is 20.12165917
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.12
normal_0.5
./test_glass0/model_MLP_20000_0.12/record_1/MLP_20000_0.12_4
./test_glass0/result_MLP_20000_0.12_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.12/record_1/MLP_20000_0.12_5
----------------------



epoch 1, loss 0.5955, train acc 67.44%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.4120, train acc 79.65%, f1 0.7407, precision 0.6329, recall 0.8929, auc 0.8214
epoch 1001, loss 0.3679, train acc 81.98%, f1 0.7669, precision 0.6623, recall 0.9107, auc 0.8433
epoch 1501, loss 0.3124, train acc 83.14%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8565
epoch 2001, loss 0.4200, train acc 84.88%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8695
epoch 2501, loss 0.2538, train acc 85.47%, f1 0.8062, precision 0.7123, recall 0.9286, auc 0.8738
epoch 3001, loss 0.2602, train acc 86.63%, f1 0.8217, precision 0.7260, recall 0.9464, auc 0.8870
epoch 3501, loss 0.3044, train acc 87.21%, f1 0.8254, precision 0.7429, recall 0.9286, auc 0.8867
epoch 4001, loss 0.2301, train acc 88.95%, f1 0.8455, precision 0.7761, recall 0.9286, auc 0.8996
epoch 4501, loss 0.2471, train acc 90.70%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9218
epoch 5001, loss 0.2288, train acc 89.53%, f1 0.8500, precision 0.7969, recall 0.9107, auc 0.8993
epoch 5501, loss 0.1723, train acc 90.12%, f1 0.8595, precision 0.8000, recall 0.9286, auc 0.9083
epoch 6001, loss 0.2623, train acc 92.44%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9255
epoch 6501, loss 0.1726, train acc 92.44%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9301
epoch 7001, loss 0.1966, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 7501, loss 0.2137, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 8001, loss 0.2100, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 8501, loss 0.1541, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 9001, loss 0.1362, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 9501, loss 0.1249, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 10001, loss 0.2083, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 10501, loss 0.1592, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 11001, loss 0.1300, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 11501, loss 0.1268, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 12001, loss 0.0912, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 12501, loss 0.1150, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 13001, loss 0.1496, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 13501, loss 0.1129, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 14001, loss 0.1360, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 14501, loss 0.1729, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 15001, loss 0.1105, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 15501, loss 0.1503, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 16001, loss 0.1510, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 16501, loss 0.1057, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 17001, loss 0.0733, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 17501, loss 0.1215, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 18001, loss 0.0805, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 18501, loss 0.0634, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 19001, loss 0.1127, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 19501, loss 0.2029, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
running_time is 20.386889896000003
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.12
normal_0.5
./test_glass0/model_MLP_20000_0.12/record_1/MLP_20000_0.12_5
./test_glass0/result_MLP_20000_0.12_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.13/record_1/MLP_20000_0.13_1
----------------------



epoch 1, loss 0.6430, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3228, train acc 83.63%, f1 0.7879, precision 0.6842, recall 0.9286, auc 0.8599
epoch 1001, loss 0.3795, train acc 83.63%, f1 0.7879, precision 0.6842, recall 0.9286, auc 0.8599
epoch 1501, loss 0.3607, train acc 84.80%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8686
epoch 2001, loss 0.2893, train acc 85.38%, f1 0.8062, precision 0.7123, recall 0.9286, auc 0.8730
epoch 2501, loss 0.2119, train acc 88.30%, f1 0.8413, precision 0.7571, recall 0.9464, auc 0.8993
epoch 3001, loss 0.2230, train acc 90.06%, f1 0.8618, precision 0.7910, recall 0.9464, auc 0.9123
epoch 3501, loss 0.2009, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 4001, loss 0.2614, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 4501, loss 0.1231, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 5001, loss 0.1861, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 5501, loss 0.1822, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 6001, loss 0.1690, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 6501, loss 0.1640, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 7001, loss 0.1174, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 7501, loss 0.0985, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 8001, loss 0.1350, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 8501, loss 0.1879, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 9001, loss 0.1367, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 9501, loss 0.1019, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10001, loss 0.0962, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10501, loss 0.2248, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11001, loss 0.1113, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11501, loss 0.0553, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12001, loss 0.1119, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12501, loss 0.0986, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13001, loss 0.1774, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13501, loss 0.0434, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14001, loss 0.0771, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.1136, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15001, loss 0.0910, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.0867, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.0593, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.0689, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.0923, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.0544, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.0811, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.1380, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.1383, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.0942, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 20.017719576
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.13
normal_0.5
./test_glass0/model_MLP_20000_0.13/record_1/MLP_20000_0.13_1
./test_glass0/result_MLP_20000_0.13_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.13/record_1/MLP_20000_0.13_2
----------------------



epoch 1, loss 0.6642, train acc 44.44%, f1 0.5274, precision 0.3655, recall 0.9464, auc 0.5732
epoch 501, loss 0.4182, train acc 82.46%, f1 0.7692, precision 0.6757, recall 0.8929, auc 0.8421
epoch 1001, loss 0.3403, train acc 83.63%, f1 0.7846, precision 0.6892, recall 0.9107, auc 0.8554
epoch 1501, loss 0.3029, train acc 85.96%, f1 0.8095, precision 0.7286, recall 0.9107, auc 0.8727
epoch 2001, loss 0.2656, train acc 87.72%, f1 0.8346, precision 0.7465, recall 0.9464, auc 0.8950
epoch 2501, loss 0.2539, train acc 87.72%, f1 0.8293, precision 0.7612, recall 0.9107, auc 0.8858
epoch 3001, loss 0.1908, train acc 89.47%, f1 0.8548, precision 0.7794, recall 0.9464, auc 0.9080
epoch 3501, loss 0.2441, train acc 90.64%, f1 0.8689, precision 0.8030, recall 0.9464, auc 0.9167
epoch 4001, loss 0.2251, train acc 91.23%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9210
epoch 4501, loss 0.1467, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 5001, loss 0.1764, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 5501, loss 0.1388, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 6001, loss 0.1365, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 6501, loss 0.1626, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 7001, loss 0.1654, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 7501, loss 0.1435, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 8001, loss 0.0937, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 8501, loss 0.1252, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 9001, loss 0.1120, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 9501, loss 0.0824, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10001, loss 0.1110, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10501, loss 0.0930, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 11001, loss 0.0680, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 11501, loss 0.0760, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 12001, loss 0.1009, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 12501, loss 0.0773, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 13001, loss 0.0699, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 13501, loss 0.0876, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14001, loss 0.1072, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14501, loss 0.0581, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 15001, loss 0.0760, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 15501, loss 0.0696, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 16001, loss 0.0830, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 16501, loss 0.0572, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17001, loss 0.0684, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 17501, loss 0.0911, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18001, loss 0.0520, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18501, loss 0.0541, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19001, loss 0.0606, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19501, loss 0.0577, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 20.153344979
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.13
normal_0.5
./test_glass0/model_MLP_20000_0.13/record_1/MLP_20000_0.13_2
./test_glass0/result_MLP_20000_0.13_normal_0.5/record_1/
----------------------



the AUC is 0.6342364532019704

the Fscore is 0.55

the precision is 0.4230769230769231

the recall is 0.7857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.13/record_1/MLP_20000_0.13_3
----------------------



epoch 1, loss 0.6322, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.4165, train acc 78.95%, f1 0.7313, precision 0.6282, recall 0.8750, auc 0.8114
epoch 1001, loss 0.3811, train acc 80.70%, f1 0.7481, precision 0.6533, recall 0.8750, auc 0.8245
epoch 1501, loss 0.2818, train acc 83.04%, f1 0.7717, precision 0.6901, recall 0.8750, auc 0.8418
epoch 2001, loss 0.2746, train acc 85.96%, f1 0.8033, precision 0.7424, recall 0.8750, auc 0.8636
epoch 2501, loss 0.3416, train acc 86.55%, f1 0.8099, precision 0.7538, recall 0.8750, auc 0.8679
epoch 3001, loss 0.2495, train acc 86.55%, f1 0.8130, precision 0.7463, recall 0.8929, auc 0.8725
epoch 3501, loss 0.1923, train acc 87.72%, f1 0.8320, precision 0.7536, recall 0.9286, auc 0.8904
epoch 4001, loss 0.3039, train acc 88.30%, f1 0.8413, precision 0.7571, recall 0.9464, auc 0.8993
epoch 4501, loss 0.2007, train acc 90.64%, f1 0.8689, precision 0.8030, recall 0.9464, auc 0.9167
epoch 5001, loss 0.2729, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 5501, loss 0.1539, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 6001, loss 0.1600, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 6501, loss 0.1757, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 7001, loss 0.1799, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 7501, loss 0.1839, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 8001, loss 0.1510, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 8501, loss 0.1504, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 9001, loss 0.0802, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 9501, loss 0.1501, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 10001, loss 0.1391, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 10501, loss 0.1401, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 11001, loss 0.1425, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 11501, loss 0.1078, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 12001, loss 0.1013, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 12501, loss 0.0519, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 13001, loss 0.0754, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 13501, loss 0.0967, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 14001, loss 0.1667, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 14501, loss 0.1370, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 15001, loss 0.0836, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 15501, loss 0.1043, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 16001, loss 0.1650, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 16501, loss 0.0727, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 17001, loss 0.1334, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 17501, loss 0.0775, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18001, loss 0.1243, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18501, loss 0.0560, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 19001, loss 0.1259, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 19501, loss 0.1311, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
running_time is 19.999762224999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.13
normal_0.5
./test_glass0/model_MLP_20000_0.13/record_1/MLP_20000_0.13_3
./test_glass0/result_MLP_20000_0.13_normal_0.5/record_1/
----------------------



the AUC is 0.5899014778325122

the Fscore is 0.3333333333333333

the precision is 0.75

the recall is 0.21428571428571427

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.13/record_1/MLP_20000_0.13_4
----------------------



epoch 1, loss 0.5685, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3783, train acc 79.53%, f1 0.7368, precision 0.6364, recall 0.8750, auc 0.8158
epoch 1001, loss 0.3729, train acc 82.46%, f1 0.7656, precision 0.6806, recall 0.8750, auc 0.8375
epoch 1501, loss 0.3287, train acc 84.80%, f1 0.7969, precision 0.7083, recall 0.9107, auc 0.8641
epoch 2001, loss 0.4250, train acc 86.55%, f1 0.8217, precision 0.7260, recall 0.9464, auc 0.8863
epoch 2501, loss 0.2804, train acc 87.72%, f1 0.8293, precision 0.7612, recall 0.9107, auc 0.8858
epoch 3001, loss 0.2083, train acc 89.47%, f1 0.8525, precision 0.7879, recall 0.9286, auc 0.9034
epoch 3501, loss 0.2255, train acc 88.89%, f1 0.8455, precision 0.7761, recall 0.9286, auc 0.8991
epoch 4001, loss 0.2363, train acc 90.64%, f1 0.8689, precision 0.8030, recall 0.9464, auc 0.9167
epoch 4501, loss 0.1433, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 5001, loss 0.3274, train acc 91.23%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9210
epoch 5501, loss 0.2708, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 6001, loss 0.2277, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 6501, loss 0.2106, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 7001, loss 0.1763, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 7501, loss 0.1861, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 8001, loss 0.1874, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 8501, loss 0.2224, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 9001, loss 0.1453, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 9501, loss 0.2167, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 10001, loss 0.1455, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 10501, loss 0.1866, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 11001, loss 0.1769, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 11501, loss 0.1948, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 12001, loss 0.1048, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 12501, loss 0.1588, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 13001, loss 0.2227, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 13501, loss 0.2084, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 14001, loss 0.1147, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 14501, loss 0.2067, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 15001, loss 0.1316, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 15501, loss 0.1542, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 16001, loss 0.0591, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 16501, loss 0.1864, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 17001, loss 0.1387, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 17501, loss 0.0786, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 18001, loss 0.1767, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 18501, loss 0.1396, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 19001, loss 0.0951, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 19501, loss 0.1327, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
running_time is 20.0576605
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.13
normal_0.5
./test_glass0/model_MLP_20000_0.13/record_1/MLP_20000_0.13_4
./test_glass0/result_MLP_20000_0.13_normal_0.5/record_1/
----------------------



the AUC is 0.5357142857142857

the Fscore is 0.13333333333333333

the precision is 1.0

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.13/record_1/MLP_20000_0.13_5
----------------------



epoch 1, loss 0.6215, train acc 66.28%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4914
epoch 501, loss 0.3613, train acc 80.23%, f1 0.7536, precision 0.6341, recall 0.9286, auc 0.8350
epoch 1001, loss 0.3192, train acc 81.40%, f1 0.7647, precision 0.6500, recall 0.9286, auc 0.8436
epoch 1501, loss 0.3332, train acc 81.98%, f1 0.7704, precision 0.6582, recall 0.9286, auc 0.8479
epoch 2001, loss 0.2467, train acc 86.05%, f1 0.8125, precision 0.7222, recall 0.9286, auc 0.8781
epoch 2501, loss 0.2881, train acc 85.47%, f1 0.8062, precision 0.7123, recall 0.9286, auc 0.8738
epoch 3001, loss 0.2074, train acc 87.79%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.9002
epoch 3501, loss 0.2588, train acc 87.79%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.9002
epoch 4001, loss 0.2575, train acc 88.95%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9089
epoch 4501, loss 0.2036, train acc 90.70%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9264
epoch 5001, loss 0.2119, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 5501, loss 0.1508, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 6001, loss 0.1938, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 6501, loss 0.1145, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 7001, loss 0.1168, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 7501, loss 0.1942, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 8001, loss 0.1752, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 8501, loss 0.1760, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 9001, loss 0.2758, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 9501, loss 0.1058, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 10001, loss 0.1016, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 10501, loss 0.2320, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 11001, loss 0.1670, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 11501, loss 0.1383, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 12001, loss 0.0943, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 12501, loss 0.1656, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 13001, loss 0.0788, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 13501, loss 0.1610, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 14001, loss 0.1059, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 14501, loss 0.1565, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 15001, loss 0.1106, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 15501, loss 0.0699, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 16001, loss 0.1134, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 16501, loss 0.1148, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 17001, loss 0.1974, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 17501, loss 0.1422, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 18001, loss 0.1425, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 18501, loss 0.1114, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 19001, loss 0.0888, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 19501, loss 0.0943, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
running_time is 20.095949041
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.13
normal_0.5
./test_glass0/model_MLP_20000_0.13/record_1/MLP_20000_0.13_5
./test_glass0/result_MLP_20000_0.13_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.14/record_1/MLP_20000_0.14_1
----------------------



epoch 1, loss 0.6075, train acc 61.40%, f1 0.2500, precision 0.3438, recall 0.1964, auc 0.5069
epoch 501, loss 0.4492, train acc 81.87%, f1 0.7737, precision 0.6543, recall 0.9464, auc 0.8515
epoch 1001, loss 0.3545, train acc 84.80%, f1 0.7969, precision 0.7083, recall 0.9107, auc 0.8641
epoch 1501, loss 0.2688, train acc 85.38%, f1 0.8092, precision 0.7067, recall 0.9464, auc 0.8776
epoch 2001, loss 0.3350, train acc 85.96%, f1 0.8154, precision 0.7162, recall 0.9464, auc 0.8819
epoch 2501, loss 0.4083, train acc 87.72%, f1 0.8346, precision 0.7465, recall 0.9464, auc 0.8950
epoch 3001, loss 0.2319, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 3501, loss 0.1402, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 4001, loss 0.2341, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 4501, loss 0.1632, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 5001, loss 0.1419, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 5501, loss 0.1373, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 6001, loss 0.1573, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 6501, loss 0.1375, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 7001, loss 0.0888, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 7501, loss 0.1010, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 8001, loss 0.1110, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 8501, loss 0.1248, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9001, loss 0.1031, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 9501, loss 0.1101, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 10001, loss 0.0867, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 10501, loss 0.0627, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 11001, loss 0.0766, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 11501, loss 0.0609, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 12001, loss 0.0617, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 12501, loss 0.0708, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0821, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 13501, loss 0.0695, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0634, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 14501, loss 0.0657, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0530, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0582, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0572, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16501, loss 0.0585, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0717, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0590, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0556, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0535, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0528, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0558, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
running_time is 20.122102386
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.14
normal_0.5
./test_glass0/model_MLP_20000_0.14/record_1/MLP_20000_0.14_1
./test_glass0/result_MLP_20000_0.14_normal_0.5/record_1/
----------------------



the AUC is 0.46674876847290636

the Fscore is 0.10526315789473682

the precision is 0.2

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.14/record_1/MLP_20000_0.14_2
----------------------



epoch 1, loss 0.5612, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.4176, train acc 81.87%, f1 0.7597, precision 0.6712, recall 0.8750, auc 0.8332
epoch 1001, loss 0.3479, train acc 85.38%, f1 0.8031, precision 0.7183, recall 0.9107, auc 0.8684
epoch 1501, loss 0.3077, train acc 89.47%, f1 0.8525, precision 0.7879, recall 0.9286, auc 0.9034
epoch 2001, loss 0.2049, train acc 89.47%, f1 0.8548, precision 0.7794, recall 0.9464, auc 0.9080
epoch 2501, loss 0.3227, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 3001, loss 0.1892, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 3501, loss 0.1468, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 4001, loss 0.1623, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 4501, loss 0.1748, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 5001, loss 0.1616, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 5501, loss 0.1009, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 6001, loss 0.1174, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 6501, loss 0.1129, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 7001, loss 0.0925, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 7501, loss 0.0828, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 8001, loss 0.1275, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 8501, loss 0.1631, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 9001, loss 0.1092, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 9501, loss 0.1286, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10001, loss 0.1346, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10501, loss 0.0883, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11001, loss 0.0835, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11501, loss 0.1028, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12001, loss 0.0820, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12501, loss 0.1057, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13001, loss 0.0734, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13501, loss 0.1200, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14001, loss 0.0737, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14501, loss 0.1184, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15001, loss 0.0660, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15501, loss 0.0596, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 16001, loss 0.0632, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 16501, loss 0.0757, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17001, loss 0.0622, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17501, loss 0.0774, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18001, loss 0.1013, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18501, loss 0.1068, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19001, loss 0.1016, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19501, loss 0.0733, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 20.265245584000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.14
normal_0.5
./test_glass0/model_MLP_20000_0.14/record_1/MLP_20000_0.14_2
./test_glass0/result_MLP_20000_0.14_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.14/record_1/MLP_20000_0.14_3
----------------------



epoch 1, loss 0.6419, train acc 66.08%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4913
epoch 501, loss 0.4199, train acc 77.78%, f1 0.7246, precision 0.6098, recall 0.8929, auc 0.8073
epoch 1001, loss 0.3300, train acc 81.87%, f1 0.7597, precision 0.6712, recall 0.8750, auc 0.8332
epoch 1501, loss 0.3473, train acc 84.21%, f1 0.7874, precision 0.7042, recall 0.8929, auc 0.8551
epoch 2001, loss 0.2946, train acc 88.30%, f1 0.8361, precision 0.7727, recall 0.9107, auc 0.8901
epoch 2501, loss 0.3617, train acc 87.72%, f1 0.8293, precision 0.7612, recall 0.9107, auc 0.8858
epoch 3001, loss 0.2256, train acc 88.30%, f1 0.8413, precision 0.7571, recall 0.9464, auc 0.8993
epoch 3501, loss 0.2281, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 4001, loss 0.2293, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 4501, loss 0.2261, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 5001, loss 0.1691, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 5501, loss 0.2346, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 6001, loss 0.1932, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 6501, loss 0.1899, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 7001, loss 0.1993, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 7501, loss 0.0900, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8001, loss 0.1032, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 8501, loss 0.1110, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 9001, loss 0.1038, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9501, loss 0.1509, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 10001, loss 0.0985, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10501, loss 0.1198, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 11001, loss 0.1253, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 11501, loss 0.2237, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 12001, loss 0.1523, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 12501, loss 0.1316, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 13001, loss 0.1013, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 13501, loss 0.1739, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 14001, loss 0.1507, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 14501, loss 0.1770, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 15001, loss 0.1017, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 15501, loss 0.1126, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 16001, loss 0.1040, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 16501, loss 0.1276, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 17001, loss 0.1202, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 17501, loss 0.1249, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18001, loss 0.1287, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18501, loss 0.1225, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 19001, loss 0.1207, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 19501, loss 0.1139, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
running_time is 20.374940481
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.14
normal_0.5
./test_glass0/model_MLP_20000_0.14/record_1/MLP_20000_0.14_3
./test_glass0/result_MLP_20000_0.14_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.14/record_1/MLP_20000_0.14_4
----------------------



epoch 1, loss 0.6649, train acc 43.86%, f1 0.4783, precision 0.3438, recall 0.7857, auc 0.5276
epoch 501, loss 0.3377, train acc 77.78%, f1 0.7206, precision 0.6125, recall 0.8750, auc 0.8027
epoch 1001, loss 0.4239, train acc 81.87%, f1 0.7634, precision 0.6667, recall 0.8929, auc 0.8377
epoch 1501, loss 0.3413, train acc 82.46%, f1 0.7727, precision 0.6711, recall 0.9107, auc 0.8467
epoch 2001, loss 0.2249, train acc 87.13%, f1 0.8197, precision 0.7576, recall 0.8929, auc 0.8769
epoch 2501, loss 0.3303, train acc 87.72%, f1 0.8293, precision 0.7612, recall 0.9107, auc 0.8858
epoch 3001, loss 0.1972, train acc 88.89%, f1 0.8455, precision 0.7761, recall 0.9286, auc 0.8991
epoch 3501, loss 0.2365, train acc 90.64%, f1 0.8667, precision 0.8125, recall 0.9286, auc 0.9121
epoch 4001, loss 0.2054, train acc 90.64%, f1 0.8667, precision 0.8125, recall 0.9286, auc 0.9121
epoch 4501, loss 0.1391, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 5001, loss 0.1639, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 5501, loss 0.2133, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 6001, loss 0.2021, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 6501, loss 0.1727, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 7001, loss 0.2172, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 7501, loss 0.1824, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 8001, loss 0.1117, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 8501, loss 0.1776, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 9001, loss 0.1278, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 9501, loss 0.1038, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 10001, loss 0.0900, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 10501, loss 0.1698, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 11001, loss 0.1771, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 11501, loss 0.1728, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 12001, loss 0.1632, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 12501, loss 0.1036, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 13001, loss 0.0947, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 13501, loss 0.1364, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 14001, loss 0.1224, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 14501, loss 0.1797, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 15001, loss 0.1006, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 15501, loss 0.1677, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 16001, loss 0.1105, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 16501, loss 0.0993, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17001, loss 0.1014, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 17501, loss 0.1142, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18001, loss 0.1245, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18501, loss 0.0845, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19001, loss 0.0625, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.0902, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 20.121957006
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.14
normal_0.5
./test_glass0/model_MLP_20000_0.14/record_1/MLP_20000_0.14_4
./test_glass0/result_MLP_20000_0.14_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.14/record_1/MLP_20000_0.14_5
----------------------



epoch 1, loss 0.6420, train acc 61.05%, f1 0.2472, precision 0.3333, recall 0.1964, auc 0.5034
epoch 501, loss 0.3876, train acc 78.49%, f1 0.7299, precision 0.6173, recall 0.8929, auc 0.8128
epoch 1001, loss 0.2831, train acc 81.40%, f1 0.7576, precision 0.6579, recall 0.8929, auc 0.8344
epoch 1501, loss 0.2880, train acc 81.40%, f1 0.7576, precision 0.6579, recall 0.8929, auc 0.8344
epoch 2001, loss 0.3102, train acc 82.56%, f1 0.7761, precision 0.6667, recall 0.9286, auc 0.8522
epoch 2501, loss 0.3203, train acc 84.30%, f1 0.7939, precision 0.6933, recall 0.9286, auc 0.8651
epoch 3001, loss 0.2878, train acc 86.05%, f1 0.8125, precision 0.7222, recall 0.9286, auc 0.8781
epoch 3501, loss 0.2637, train acc 86.05%, f1 0.8125, precision 0.7222, recall 0.9286, auc 0.8781
epoch 4001, loss 0.2893, train acc 87.21%, f1 0.8281, precision 0.7361, recall 0.9464, auc 0.8913
epoch 4501, loss 0.1862, train acc 88.95%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9089
epoch 5001, loss 0.2140, train acc 90.70%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9218
epoch 5501, loss 0.2884, train acc 91.86%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9304
epoch 6001, loss 0.1614, train acc 91.86%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9258
epoch 6501, loss 0.1826, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 7001, loss 0.1562, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 7501, loss 0.1908, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 8001, loss 0.1823, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 8501, loss 0.1981, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 9001, loss 0.1176, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 9501, loss 0.1713, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 10001, loss 0.2029, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 10501, loss 0.1422, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 11001, loss 0.1301, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 11501, loss 0.1723, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 12001, loss 0.2464, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 12501, loss 0.1856, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 13001, loss 0.2379, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 13501, loss 0.1419, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 14001, loss 0.0950, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 14501, loss 0.1578, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 15001, loss 0.1514, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 15501, loss 0.1255, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 16001, loss 0.0574, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 16501, loss 0.1172, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 17001, loss 0.1866, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 17501, loss 0.2748, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 18001, loss 0.1527, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 18501, loss 0.0699, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 19001, loss 0.2362, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 19501, loss 0.1150, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
running_time is 19.968366883
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.14
normal_0.5
./test_glass0/model_MLP_20000_0.14/record_1/MLP_20000_0.14_5
./test_glass0/result_MLP_20000_0.14_normal_0.5/record_1/
----------------------



the AUC is 0.7321428571428572

the Fscore is 0.6486486486486486

the precision is 0.5217391304347826

the recall is 0.8571428571428571

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.15/record_1/MLP_20000_0.15_1
----------------------



epoch 1, loss 0.6289, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3436, train acc 82.46%, f1 0.7761, precision 0.6667, recall 0.9286, auc 0.8512
epoch 1001, loss 0.3254, train acc 83.04%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8556
epoch 1501, loss 0.3255, train acc 83.63%, f1 0.7879, precision 0.6842, recall 0.9286, auc 0.8599
epoch 2001, loss 0.3271, train acc 84.80%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8686
epoch 2501, loss 0.3408, train acc 84.80%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8686
epoch 3001, loss 0.3498, train acc 85.38%, f1 0.8062, precision 0.7123, recall 0.9286, auc 0.8730
epoch 3501, loss 0.3113, train acc 87.72%, f1 0.8346, precision 0.7465, recall 0.9464, auc 0.8950
epoch 4001, loss 0.3276, train acc 87.72%, f1 0.8346, precision 0.7465, recall 0.9464, auc 0.8950
epoch 4501, loss 0.1741, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 5001, loss 0.1318, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 5501, loss 0.1308, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 6001, loss 0.2283, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 6501, loss 0.1602, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 7001, loss 0.1928, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 7501, loss 0.1873, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 8001, loss 0.1549, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 8501, loss 0.1341, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9001, loss 0.1421, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 9501, loss 0.1465, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10001, loss 0.1385, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10501, loss 0.1095, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11001, loss 0.1186, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11501, loss 0.1354, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12001, loss 0.1618, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12501, loss 0.0765, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13001, loss 0.1258, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13501, loss 0.1222, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14001, loss 0.0723, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.0645, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15001, loss 0.0708, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.0801, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.0726, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.1098, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.0458, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.1468, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.1085, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.0817, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.0997, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.0609, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 20.103374113
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.15
normal_0.5
./test_glass0/model_MLP_20000_0.15/record_1/MLP_20000_0.15_1
./test_glass0/result_MLP_20000_0.15_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.15/record_1/MLP_20000_0.15_2
----------------------



epoch 1, loss 0.6412, train acc 64.91%, f1 0.3023, precision 0.4333, recall 0.2321, auc 0.5422
epoch 501, loss 0.3569, train acc 80.70%, f1 0.7519, precision 0.6494, recall 0.8929, auc 0.8290
epoch 1001, loss 0.2735, train acc 83.63%, f1 0.7879, precision 0.6842, recall 0.9286, auc 0.8599
epoch 1501, loss 0.4111, train acc 87.13%, f1 0.8254, precision 0.7429, recall 0.9286, auc 0.8860
epoch 2001, loss 0.2201, train acc 89.47%, f1 0.8548, precision 0.7794, recall 0.9464, auc 0.9080
epoch 2501, loss 0.1737, train acc 90.64%, f1 0.8689, precision 0.8030, recall 0.9464, auc 0.9167
epoch 3001, loss 0.1838, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 3501, loss 0.2097, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 4001, loss 0.1978, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 4501, loss 0.1658, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 5001, loss 0.2501, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 5501, loss 0.1483, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 6001, loss 0.1412, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 6501, loss 0.1538, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 7001, loss 0.1328, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 7501, loss 0.1368, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 8001, loss 0.1074, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 8501, loss 0.1487, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 9001, loss 0.1379, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 9501, loss 0.0982, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 10001, loss 0.1224, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 10501, loss 0.1440, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 11001, loss 0.1691, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 11501, loss 0.1312, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 12001, loss 0.1304, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 12501, loss 0.1350, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 13001, loss 0.1428, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 13501, loss 0.1035, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14001, loss 0.1275, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14501, loss 0.0885, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 15001, loss 0.0900, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.0695, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.1568, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.1003, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17001, loss 0.1366, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17501, loss 0.1057, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18001, loss 0.0534, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18501, loss 0.1247, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19001, loss 0.1095, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.0616, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 20.151719837
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.15
normal_0.5
./test_glass0/model_MLP_20000_0.15/record_1/MLP_20000_0.15_2
./test_glass0/result_MLP_20000_0.15_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.15/record_1/MLP_20000_0.15_3
----------------------



epoch 1, loss 0.6288, train acc 62.57%, f1 0.2558, precision 0.3667, recall 0.1964, auc 0.5156
epoch 501, loss 0.3359, train acc 80.70%, f1 0.7519, precision 0.6494, recall 0.8929, auc 0.8290
epoch 1001, loss 0.4390, train acc 81.29%, f1 0.7576, precision 0.6579, recall 0.8929, auc 0.8334
epoch 1501, loss 0.3240, train acc 84.80%, f1 0.7903, precision 0.7206, recall 0.8750, auc 0.8549
epoch 2001, loss 0.3847, train acc 87.13%, f1 0.8254, precision 0.7429, recall 0.9286, auc 0.8860
epoch 2501, loss 0.2719, train acc 88.30%, f1 0.8413, precision 0.7571, recall 0.9464, auc 0.8993
epoch 3001, loss 0.4572, train acc 87.72%, f1 0.8346, precision 0.7465, recall 0.9464, auc 0.8950
epoch 3501, loss 0.2632, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 4001, loss 0.2200, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 4501, loss 0.1740, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 5001, loss 0.1257, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 5501, loss 0.1785, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 6001, loss 0.1754, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 6501, loss 0.1593, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 7001, loss 0.1354, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 7501, loss 0.1171, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8001, loss 0.1770, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8501, loss 0.1728, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 9001, loss 0.1693, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9501, loss 0.1306, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 10001, loss 0.1622, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 10501, loss 0.1090, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11001, loss 0.1257, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11501, loss 0.0976, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 12001, loss 0.0909, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12501, loss 0.1237, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 13001, loss 0.1124, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 13501, loss 0.1120, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14001, loss 0.1117, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14501, loss 0.0847, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15001, loss 0.1018, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15501, loss 0.1006, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16001, loss 0.1049, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16501, loss 0.0623, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.0800, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.1287, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.0908, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.0911, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.0548, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.1099, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 20.098377535999997
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.15
normal_0.5
./test_glass0/model_MLP_20000_0.15/record_1/MLP_20000_0.15_3
./test_glass0/result_MLP_20000_0.15_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.15/record_1/MLP_20000_0.15_4
----------------------



epoch 1, loss 0.5792, train acc 71.35%, f1 0.3288, precision 0.7059, recall 0.2143, auc 0.5854
epoch 501, loss 0.3427, train acc 78.36%, f1 0.7299, precision 0.6173, recall 0.8929, auc 0.8116
epoch 1001, loss 0.3600, train acc 81.87%, f1 0.7634, precision 0.6667, recall 0.8929, auc 0.8377
epoch 1501, loss 0.3386, train acc 82.46%, f1 0.7727, precision 0.6711, recall 0.9107, auc 0.8467
epoch 2001, loss 0.2735, train acc 83.63%, f1 0.7812, precision 0.6944, recall 0.8929, auc 0.8508
epoch 2501, loss 0.3493, train acc 85.38%, f1 0.8000, precision 0.7246, recall 0.8929, auc 0.8638
epoch 3001, loss 0.2701, train acc 87.13%, f1 0.8197, precision 0.7576, recall 0.8929, auc 0.8769
epoch 3501, loss 0.2682, train acc 88.89%, f1 0.8455, precision 0.7761, recall 0.9286, auc 0.8991
epoch 4001, loss 0.2720, train acc 89.47%, f1 0.8548, precision 0.7794, recall 0.9464, auc 0.9080
epoch 4501, loss 0.2486, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 5001, loss 0.2631, train acc 90.64%, f1 0.8689, precision 0.8030, recall 0.9464, auc 0.9167
epoch 5501, loss 0.1777, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 6001, loss 0.2567, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 6501, loss 0.1582, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 7001, loss 0.2324, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 7501, loss 0.1524, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 8001, loss 0.1689, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 8501, loss 0.1915, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 9001, loss 0.1804, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 9501, loss 0.1714, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 10001, loss 0.1420, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 10501, loss 0.1252, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 11001, loss 0.1714, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 11501, loss 0.1764, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 12001, loss 0.1102, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 12501, loss 0.1644, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 13001, loss 0.1286, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 13501, loss 0.0871, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 14001, loss 0.1433, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14501, loss 0.1509, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15001, loss 0.1055, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.0967, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.0949, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.1531, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.0762, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.0795, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.1204, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.1656, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.0414, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.0771, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 20.141538341
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.15
normal_0.5
./test_glass0/model_MLP_20000_0.15/record_1/MLP_20000_0.15_4
./test_glass0/result_MLP_20000_0.15_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.15/record_1/MLP_20000_0.15_5
----------------------



epoch 1, loss 0.6165, train acc 67.44%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3755, train acc 80.81%, f1 0.7591, precision 0.6420, recall 0.9286, auc 0.8393
epoch 1001, loss 0.3071, train acc 81.40%, f1 0.7647, precision 0.6500, recall 0.9286, auc 0.8436
epoch 1501, loss 0.3718, train acc 83.14%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8565
epoch 2001, loss 0.2830, train acc 83.72%, f1 0.7879, precision 0.6842, recall 0.9286, auc 0.8608
epoch 2501, loss 0.2132, train acc 87.21%, f1 0.8281, precision 0.7361, recall 0.9464, auc 0.8913
epoch 3001, loss 0.1913, train acc 87.79%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.9002
epoch 3501, loss 0.1814, train acc 87.79%, f1 0.8346, precision 0.7465, recall 0.9464, auc 0.8956
epoch 4001, loss 0.2617, train acc 88.95%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9089
epoch 4501, loss 0.3022, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 5001, loss 0.2381, train acc 91.28%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9307
epoch 5501, loss 0.2309, train acc 91.28%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9307
epoch 6001, loss 0.1863, train acc 92.44%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9393
epoch 6501, loss 0.1674, train acc 91.86%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9304
epoch 7001, loss 0.2485, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 7501, loss 0.2376, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 8001, loss 0.1782, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 8501, loss 0.1285, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 9001, loss 0.2066, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 9501, loss 0.1384, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 10001, loss 0.1885, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 10501, loss 0.1025, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 11001, loss 0.1926, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 11501, loss 0.1646, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 12001, loss 0.1941, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 12501, loss 0.1018, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 13001, loss 0.1082, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 13501, loss 0.1560, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 14001, loss 0.1659, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 14501, loss 0.1910, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 15001, loss 0.1279, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 15501, loss 0.1045, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 16001, loss 0.1151, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 16501, loss 0.1510, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 17001, loss 0.1773, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 17501, loss 0.1552, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 18001, loss 0.1119, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 18501, loss 0.1425, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 19001, loss 0.1243, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 19501, loss 0.0812, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
running_time is 20.079980315
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.15
normal_0.5
./test_glass0/model_MLP_20000_0.15/record_1/MLP_20000_0.15_5
./test_glass0/result_MLP_20000_0.15_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.16/record_1/MLP_20000_0.16_1
----------------------



epoch 1, loss 0.6277, train acc 69.59%, f1 0.2353, precision 0.6667, recall 0.1429, auc 0.5540
epoch 501, loss 0.3600, train acc 81.87%, f1 0.7704, precision 0.6582, recall 0.9286, auc 0.8469
epoch 1001, loss 0.2767, train acc 83.04%, f1 0.7852, precision 0.6709, recall 0.9464, auc 0.8602
epoch 1501, loss 0.3072, train acc 84.80%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8686
epoch 2001, loss 0.2793, train acc 85.38%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8821
epoch 2501, loss 0.2594, train acc 87.13%, f1 0.8281, precision 0.7361, recall 0.9464, auc 0.8906
epoch 3001, loss 0.3071, train acc 89.47%, f1 0.8548, precision 0.7794, recall 0.9464, auc 0.9080
epoch 3501, loss 0.2378, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 4001, loss 0.2135, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 4501, loss 0.1556, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 5001, loss 0.1540, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 5501, loss 0.1461, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 6001, loss 0.2075, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 6501, loss 0.1800, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 7001, loss 0.1774, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 7501, loss 0.1380, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 8001, loss 0.1776, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 8501, loss 0.1289, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9001, loss 0.1454, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9501, loss 0.1146, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10001, loss 0.1801, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10501, loss 0.0858, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11001, loss 0.2343, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11501, loss 0.1820, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12001, loss 0.1152, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12501, loss 0.0838, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13001, loss 0.1366, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13501, loss 0.1601, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14001, loss 0.1363, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.1403, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15001, loss 0.1386, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.1132, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.1068, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.1425, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.1414, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.0720, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.0894, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.1337, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.1728, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.1193, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 20.107382418
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.16
normal_0.5
./test_glass0/model_MLP_20000_0.16/record_1/MLP_20000_0.16_1
./test_glass0/result_MLP_20000_0.16_normal_0.5/record_1/
----------------------



the AUC is 0.46798029556650245

the Fscore is 0.18181818181818182

the precision is 0.25

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.16/record_1/MLP_20000_0.16_2
----------------------



epoch 1, loss 0.6940, train acc 35.09%, f1 0.5022, precision 0.3353, recall 1.0000, auc 0.5174
epoch 501, loss 0.3937, train acc 81.87%, f1 0.7669, precision 0.6623, recall 0.9107, auc 0.8423
epoch 1001, loss 0.4526, train acc 83.04%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8556
epoch 1501, loss 0.4003, train acc 84.80%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8686
epoch 2001, loss 0.2654, train acc 88.30%, f1 0.8437, precision 0.7500, recall 0.9643, auc 0.9039
epoch 2501, loss 0.3002, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 3001, loss 0.1834, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 3501, loss 0.3009, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 4001, loss 0.2247, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 4501, loss 0.2560, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 5001, loss 0.1928, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 5501, loss 0.1765, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 6001, loss 0.1877, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 6501, loss 0.1279, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 7001, loss 0.1368, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 7501, loss 0.1309, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 8001, loss 0.1352, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 8501, loss 0.1246, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 9001, loss 0.1247, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9501, loss 0.1016, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 10001, loss 0.1171, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10501, loss 0.1817, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11001, loss 0.1089, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 11501, loss 0.0964, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12001, loss 0.1365, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 12501, loss 0.1197, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13001, loss 0.1190, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13501, loss 0.0994, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 14001, loss 0.1355, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 14501, loss 0.0972, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 15001, loss 0.0694, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 15501, loss 0.1251, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16001, loss 0.0544, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16501, loss 0.1176, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.1016, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17501, loss 0.0994, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.0758, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.1340, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.1198, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.1178, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
running_time is 20.086102455
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.16
normal_0.5
./test_glass0/model_MLP_20000_0.16/record_1/MLP_20000_0.16_2
./test_glass0/result_MLP_20000_0.16_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.16/record_1/MLP_20000_0.16_3
----------------------



epoch 1, loss 0.6544, train acc 33.92%, f1 0.4978, precision 0.3314, recall 1.0000, auc 0.5087
epoch 501, loss 0.4314, train acc 79.53%, f1 0.7445, precision 0.6296, recall 0.9107, auc 0.8249
epoch 1001, loss 0.3146, train acc 80.70%, f1 0.7519, precision 0.6494, recall 0.8929, auc 0.8290
epoch 1501, loss 0.4153, train acc 83.04%, f1 0.7752, precision 0.6849, recall 0.8929, auc 0.8464
epoch 2001, loss 0.3207, train acc 85.96%, f1 0.8095, precision 0.7286, recall 0.9107, auc 0.8727
epoch 2501, loss 0.3169, train acc 86.55%, f1 0.8189, precision 0.7324, recall 0.9286, auc 0.8817
epoch 3001, loss 0.2864, train acc 88.30%, f1 0.8413, precision 0.7571, recall 0.9464, auc 0.8993
epoch 3501, loss 0.2225, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 4001, loss 0.1600, train acc 90.64%, f1 0.8689, precision 0.8030, recall 0.9464, auc 0.9167
epoch 4501, loss 0.1809, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 5001, loss 0.1383, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 5501, loss 0.1826, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 6001, loss 0.1771, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 6501, loss 0.1051, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 7001, loss 0.0998, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 7501, loss 0.1539, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 8001, loss 0.1282, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 8501, loss 0.1172, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 9001, loss 0.1905, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 9501, loss 0.1467, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10001, loss 0.0925, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 10501, loss 0.1140, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 11001, loss 0.1238, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 11501, loss 0.1075, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12001, loss 0.1177, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 12501, loss 0.2209, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 13001, loss 0.1838, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 13501, loss 0.1244, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 14001, loss 0.1958, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14501, loss 0.1664, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 15001, loss 0.1678, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 15501, loss 0.1293, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 16001, loss 0.1464, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16501, loss 0.0943, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 17001, loss 0.1389, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 17501, loss 0.1155, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 18001, loss 0.1369, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 18501, loss 0.1083, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19001, loss 0.0837, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 19501, loss 0.1260, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
running_time is 20.163526878000003
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.16
normal_0.5
./test_glass0/model_MLP_20000_0.16/record_1/MLP_20000_0.16_3
./test_glass0/result_MLP_20000_0.16_normal_0.5/record_1/
----------------------



the AUC is 0.625615763546798

the Fscore is 0.4210526315789473

the precision is 0.8

the recall is 0.2857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.16/record_1/MLP_20000_0.16_4
----------------------



epoch 1, loss 0.6409, train acc 64.33%, f1 0.3579, precision 0.4359, recall 0.3036, auc 0.5561
epoch 501, loss 0.4241, train acc 77.78%, f1 0.7324, precision 0.6047, recall 0.9286, auc 0.8165
epoch 1001, loss 0.3959, train acc 80.12%, f1 0.7571, precision 0.6310, recall 0.9464, auc 0.8384
epoch 1501, loss 0.4196, train acc 79.53%, f1 0.7482, precision 0.6265, recall 0.9286, auc 0.8295
epoch 2001, loss 0.2918, train acc 80.70%, f1 0.7591, precision 0.6420, recall 0.9286, auc 0.8382
epoch 2501, loss 0.3254, train acc 82.46%, f1 0.7761, precision 0.6667, recall 0.9286, auc 0.8512
epoch 3001, loss 0.2353, train acc 84.21%, f1 0.7939, precision 0.6933, recall 0.9286, auc 0.8643
epoch 3501, loss 0.3004, train acc 85.38%, f1 0.8062, precision 0.7123, recall 0.9286, auc 0.8730
epoch 4001, loss 0.2927, train acc 88.30%, f1 0.8413, precision 0.7571, recall 0.9464, auc 0.8993
epoch 4501, loss 0.2774, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 5001, loss 0.2578, train acc 90.06%, f1 0.8618, precision 0.7910, recall 0.9464, auc 0.9123
epoch 5501, loss 0.2520, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 6001, loss 0.2305, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 6501, loss 0.2802, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 7001, loss 0.2311, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 7501, loss 0.2121, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 8001, loss 0.1712, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 8501, loss 0.2106, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 9001, loss 0.2980, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 9501, loss 0.2251, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 10001, loss 0.2193, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 10501, loss 0.2084, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 11001, loss 0.1446, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 11501, loss 0.2379, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 12001, loss 0.1783, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 12501, loss 0.2501, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 13001, loss 0.1723, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 13501, loss 0.1233, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 14001, loss 0.1243, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 14501, loss 0.1558, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 15001, loss 0.1602, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 15501, loss 0.2148, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 16001, loss 0.1930, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 16501, loss 0.1292, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 17001, loss 0.1677, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 17501, loss 0.1732, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 18001, loss 0.1116, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 18501, loss 0.1233, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19001, loss 0.1188, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 19501, loss 0.1371, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
running_time is 20.227268111999997
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.16
normal_0.5
./test_glass0/model_MLP_20000_0.16/record_1/MLP_20000_0.16_4
./test_glass0/result_MLP_20000_0.16_normal_0.5/record_1/
----------------------



the AUC is 0.5541871921182266

the Fscore is 0.23529411764705882

the precision is 0.6666666666666666

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.16/record_1/MLP_20000_0.16_5
----------------------



epoch 1, loss 0.6146, train acc 66.28%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4914
epoch 501, loss 0.3425, train acc 79.07%, f1 0.7429, precision 0.6190, recall 0.9286, auc 0.8264
epoch 1001, loss 0.3450, train acc 81.98%, f1 0.7669, precision 0.6623, recall 0.9107, auc 0.8433
epoch 1501, loss 0.2838, train acc 81.98%, f1 0.7704, precision 0.6582, recall 0.9286, auc 0.8479
epoch 2001, loss 0.3097, train acc 82.56%, f1 0.7761, precision 0.6667, recall 0.9286, auc 0.8522
epoch 2501, loss 0.2863, train acc 83.14%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8565
epoch 3001, loss 0.3214, train acc 84.88%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8695
epoch 3501, loss 0.2450, train acc 85.47%, f1 0.8092, precision 0.7067, recall 0.9464, auc 0.8784
epoch 4001, loss 0.2158, train acc 88.37%, f1 0.8437, precision 0.7500, recall 0.9643, auc 0.9046
epoch 4501, loss 0.2521, train acc 87.79%, f1 0.8346, precision 0.7465, recall 0.9464, auc 0.8956
epoch 5001, loss 0.2099, train acc 88.95%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9089
epoch 5501, loss 0.1626, train acc 88.95%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9089
epoch 6001, loss 0.2198, train acc 90.12%, f1 0.8595, precision 0.8000, recall 0.9286, auc 0.9083
epoch 6501, loss 0.1905, train acc 90.70%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9218
epoch 7001, loss 0.2117, train acc 91.28%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9261
epoch 7501, loss 0.2276, train acc 91.86%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9304
epoch 8001, loss 0.2129, train acc 91.86%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9304
epoch 8501, loss 0.2143, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 9001, loss 0.1360, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 9501, loss 0.2270, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 10001, loss 0.2084, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 10501, loss 0.1249, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 11001, loss 0.2086, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 11501, loss 0.1680, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 12001, loss 0.1417, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 12501, loss 0.2065, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 13001, loss 0.1853, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 13501, loss 0.1033, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 14001, loss 0.1034, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 14501, loss 0.1616, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 15001, loss 0.1938, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 15501, loss 0.1849, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 16001, loss 0.1758, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 16501, loss 0.1408, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 17001, loss 0.1157, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 17501, loss 0.1212, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 18001, loss 0.1884, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 18501, loss 0.1727, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 19001, loss 0.1244, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 19501, loss 0.1169, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
running_time is 20.119995615999997
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.16
normal_0.5
./test_glass0/model_MLP_20000_0.16/record_1/MLP_20000_0.16_5
./test_glass0/result_MLP_20000_0.16_normal_0.5/record_1/
----------------------



the AUC is 0.6071428571428571

the Fscore is 0.4615384615384615

the precision is 0.5

the recall is 0.42857142857142855

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.17/record_1/MLP_20000_0.17_1
----------------------



epoch 1, loss 0.6554, train acc 41.52%, f1 0.5146, precision 0.3533, recall 0.9464, auc 0.5515
epoch 501, loss 0.3652, train acc 81.87%, f1 0.7737, precision 0.6543, recall 0.9464, auc 0.8515
epoch 1001, loss 0.2716, train acc 81.29%, f1 0.7647, precision 0.6500, recall 0.9286, auc 0.8425
epoch 1501, loss 0.3910, train acc 83.63%, f1 0.7910, precision 0.6795, recall 0.9464, auc 0.8645
epoch 2001, loss 0.2694, train acc 84.21%, f1 0.7939, precision 0.6933, recall 0.9286, auc 0.8643
epoch 2501, loss 0.2740, train acc 84.80%, f1 0.8030, precision 0.6974, recall 0.9464, auc 0.8732
epoch 3001, loss 0.2962, train acc 85.38%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8821
epoch 3501, loss 0.2857, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 4001, loss 0.2644, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 4501, loss 0.2667, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 5001, loss 0.2858, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 5501, loss 0.2507, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 6001, loss 0.2680, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 6501, loss 0.2106, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 7001, loss 0.1374, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 7501, loss 0.1627, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 8001, loss 0.1164, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 8501, loss 0.2011, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9001, loss 0.1485, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9501, loss 0.1116, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10001, loss 0.1792, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10501, loss 0.1471, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11001, loss 0.1279, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11501, loss 0.0934, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12001, loss 0.0989, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12501, loss 0.1052, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13001, loss 0.0852, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13501, loss 0.1072, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14001, loss 0.1112, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.0706, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15001, loss 0.1498, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15501, loss 0.1236, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16001, loss 0.0910, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16501, loss 0.1319, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.0988, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.1157, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.0855, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.0764, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.0685, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19501, loss 0.1091, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 20.054763598
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.17
normal_0.5
./test_glass0/model_MLP_20000_0.17/record_1/MLP_20000_0.17_1
./test_glass0/result_MLP_20000_0.17_normal_0.5/record_1/
----------------------



the AUC is 0.5369458128078817

the Fscore is 0.22222222222222224

the precision is 0.5

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.17/record_1/MLP_20000_0.17_2
----------------------



epoch 1, loss 0.6270, train acc 61.99%, f1 0.3564, precision 0.4000, recall 0.3214, auc 0.5433
epoch 501, loss 0.4148, train acc 81.87%, f1 0.7704, precision 0.6582, recall 0.9286, auc 0.8469
epoch 1001, loss 0.2887, train acc 82.46%, f1 0.7761, precision 0.6667, recall 0.9286, auc 0.8512
epoch 1501, loss 0.2739, train acc 83.63%, f1 0.7910, precision 0.6795, recall 0.9464, auc 0.8645
epoch 2001, loss 0.2599, train acc 87.72%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.8995
epoch 2501, loss 0.3034, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 3001, loss 0.2095, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 3501, loss 0.1687, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 4001, loss 0.2386, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 4501, loss 0.2066, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 5001, loss 0.2073, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 5501, loss 0.2140, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 6001, loss 0.1926, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 6501, loss 0.1666, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 7001, loss 0.1267, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 7501, loss 0.1860, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 8001, loss 0.1337, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8501, loss 0.0937, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9001, loss 0.1573, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9501, loss 0.1587, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10001, loss 0.0949, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10501, loss 0.1732, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 11001, loss 0.1645, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 11501, loss 0.1248, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 12001, loss 0.0853, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 12501, loss 0.0954, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 13001, loss 0.0911, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 13501, loss 0.1376, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14001, loss 0.1056, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14501, loss 0.1495, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15001, loss 0.0907, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15501, loss 0.1653, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 16001, loss 0.1109, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.0796, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 17001, loss 0.0942, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17501, loss 0.0913, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 18001, loss 0.0674, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18501, loss 0.0650, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19001, loss 0.1223, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19501, loss 0.1382, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
running_time is 20.119063319000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.17
normal_0.5
./test_glass0/model_MLP_20000_0.17/record_1/MLP_20000_0.17_2
./test_glass0/result_MLP_20000_0.17_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.17/record_1/MLP_20000_0.17_3
----------------------



epoch 1, loss 0.5846, train acc 64.33%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4783
epoch 501, loss 0.3837, train acc 78.95%, f1 0.7353, precision 0.6250, recall 0.8929, auc 0.8160
epoch 1001, loss 0.4009, train acc 81.87%, f1 0.7634, precision 0.6667, recall 0.8929, auc 0.8377
epoch 1501, loss 0.4445, train acc 83.04%, f1 0.7752, precision 0.6849, recall 0.8929, auc 0.8464
epoch 2001, loss 0.3589, train acc 87.13%, f1 0.8254, precision 0.7429, recall 0.9286, auc 0.8860
epoch 2501, loss 0.2539, train acc 88.30%, f1 0.8387, precision 0.7647, recall 0.9286, auc 0.8947
epoch 3001, loss 0.2730, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 3501, loss 0.2096, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 4001, loss 0.2326, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 4501, loss 0.1038, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 5001, loss 0.1212, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 5501, loss 0.1911, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 6001, loss 0.1992, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 6501, loss 0.1495, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 7001, loss 0.1585, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 7501, loss 0.2420, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 8001, loss 0.1225, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 8501, loss 0.1716, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 9001, loss 0.1998, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 9501, loss 0.2082, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 10001, loss 0.2173, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 10501, loss 0.2071, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 11001, loss 0.1759, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 11501, loss 0.0704, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 12001, loss 0.1184, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 12501, loss 0.1889, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 13001, loss 0.1188, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 13501, loss 0.2030, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 14001, loss 0.1371, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 14501, loss 0.1213, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 15001, loss 0.1344, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 15501, loss 0.0812, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 16001, loss 0.1355, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 16501, loss 0.1294, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 17001, loss 0.1438, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 17501, loss 0.0521, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 18001, loss 0.1450, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 18501, loss 0.1916, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 19001, loss 0.2107, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 19501, loss 0.1351, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
running_time is 20.218389260000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.17
normal_0.5
./test_glass0/model_MLP_20000_0.17/record_1/MLP_20000_0.17_3
./test_glass0/result_MLP_20000_0.17_normal_0.5/record_1/
----------------------



the AUC is 0.625615763546798

the Fscore is 0.4210526315789473

the precision is 0.8

the recall is 0.2857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.17/record_1/MLP_20000_0.17_4
----------------------



epoch 1, loss 0.6129, train acc 63.16%, f1 0.0308, precision 0.1111, recall 0.0179, auc 0.4741
epoch 501, loss 0.3928, train acc 77.19%, f1 0.7310, precision 0.5955, recall 0.9464, auc 0.8167
epoch 1001, loss 0.4295, train acc 80.70%, f1 0.7591, precision 0.6420, recall 0.9286, auc 0.8382
epoch 1501, loss 0.4235, train acc 83.04%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8556
epoch 2001, loss 0.3421, train acc 83.04%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8556
epoch 2501, loss 0.2043, train acc 85.38%, f1 0.8031, precision 0.7183, recall 0.9107, auc 0.8684
epoch 3001, loss 0.2902, train acc 88.30%, f1 0.8387, precision 0.7647, recall 0.9286, auc 0.8947
epoch 3501, loss 0.1995, train acc 89.47%, f1 0.8525, precision 0.7879, recall 0.9286, auc 0.9034
epoch 4001, loss 0.2772, train acc 90.06%, f1 0.8595, precision 0.8000, recall 0.9286, auc 0.9078
epoch 4501, loss 0.2465, train acc 91.23%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9210
epoch 5001, loss 0.3789, train acc 91.23%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9210
epoch 5501, loss 0.1832, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 6001, loss 0.1509, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 6501, loss 0.2069, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 7001, loss 0.1530, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 7501, loss 0.1856, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 8001, loss 0.1494, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 8501, loss 0.2006, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 9001, loss 0.2031, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 9501, loss 0.2037, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 10001, loss 0.2001, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 10501, loss 0.1001, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 11001, loss 0.1588, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 11501, loss 0.2220, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 12001, loss 0.2188, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 12501, loss 0.1208, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 13001, loss 0.1696, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 13501, loss 0.1581, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14001, loss 0.1154, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 14501, loss 0.1104, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15001, loss 0.1588, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15501, loss 0.1692, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16001, loss 0.1395, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 16501, loss 0.1876, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17001, loss 0.1589, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17501, loss 0.1455, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18001, loss 0.1849, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18501, loss 0.1989, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 19001, loss 0.1044, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19501, loss 0.2031, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
running_time is 20.111596786
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.17
normal_0.5
./test_glass0/model_MLP_20000_0.17/record_1/MLP_20000_0.17_4
./test_glass0/result_MLP_20000_0.17_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.17/record_1/MLP_20000_0.17_5
----------------------



epoch 1, loss 0.6554, train acc 39.53%, f1 0.5185, precision 0.3500, recall 1.0000, auc 0.5517
epoch 501, loss 0.3591, train acc 79.07%, f1 0.7429, precision 0.6190, recall 0.9286, auc 0.8264
epoch 1001, loss 0.4840, train acc 80.23%, f1 0.7571, precision 0.6310, recall 0.9464, auc 0.8396
epoch 1501, loss 0.3308, train acc 83.72%, f1 0.7879, precision 0.6842, recall 0.9286, auc 0.8608
epoch 2001, loss 0.2885, train acc 84.30%, f1 0.7970, precision 0.6883, recall 0.9464, auc 0.8698
epoch 2501, loss 0.2680, train acc 85.47%, f1 0.8062, precision 0.7123, recall 0.9286, auc 0.8738
epoch 3001, loss 0.2360, train acc 86.63%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8962
epoch 3501, loss 0.2844, train acc 87.79%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9049
epoch 4001, loss 0.2717, train acc 87.79%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9049
epoch 4501, loss 0.1443, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 5001, loss 0.2281, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 5501, loss 0.2185, train acc 90.70%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9264
epoch 6001, loss 0.2430, train acc 92.44%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9393
epoch 6501, loss 0.2033, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 7001, loss 0.1725, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 7501, loss 0.1895, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 8001, loss 0.1906, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 8501, loss 0.1348, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 9001, loss 0.1602, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 9501, loss 0.1966, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 10001, loss 0.1155, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 10501, loss 0.1852, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 11001, loss 0.1918, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 11501, loss 0.1399, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 12001, loss 0.1021, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 12501, loss 0.1753, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 13001, loss 0.1849, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 13501, loss 0.1018, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 14001, loss 0.0879, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 14501, loss 0.1774, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 15001, loss 0.1638, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 15501, loss 0.1347, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 16001, loss 0.1331, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 16501, loss 0.1519, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 17001, loss 0.1604, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 17501, loss 0.1391, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 18001, loss 0.1738, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 18501, loss 0.1160, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 19001, loss 0.0890, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 19501, loss 0.1515, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
running_time is 19.993141078
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.17
normal_0.5
./test_glass0/model_MLP_20000_0.17/record_1/MLP_20000_0.17_5
./test_glass0/result_MLP_20000_0.17_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.18/record_1/MLP_20000_0.18_1
----------------------



epoch 1, loss 0.6721, train acc 45.61%, f1 0.5463, precision 0.3758, recall 1.0000, auc 0.5957
epoch 501, loss 0.3592, train acc 81.87%, f1 0.7737, precision 0.6543, recall 0.9464, auc 0.8515
epoch 1001, loss 0.4134, train acc 83.63%, f1 0.7910, precision 0.6795, recall 0.9464, auc 0.8645
epoch 1501, loss 0.3023, train acc 84.80%, f1 0.8030, precision 0.6974, recall 0.9464, auc 0.8732
epoch 2001, loss 0.2662, train acc 84.80%, f1 0.8030, precision 0.6974, recall 0.9464, auc 0.8732
epoch 2501, loss 0.3451, train acc 85.96%, f1 0.8154, precision 0.7162, recall 0.9464, auc 0.8819
epoch 3001, loss 0.2231, train acc 86.55%, f1 0.8244, precision 0.7200, recall 0.9643, auc 0.8908
epoch 3501, loss 0.3560, train acc 87.72%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.8995
epoch 4001, loss 0.2042, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 4501, loss 0.3247, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 5001, loss 0.2187, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 5501, loss 0.3175, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 6001, loss 0.1970, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 6501, loss 0.2650, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 7001, loss 0.2353, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 7501, loss 0.2045, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 8001, loss 0.2350, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8501, loss 0.1942, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 9001, loss 0.2192, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9501, loss 0.1794, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10001, loss 0.1894, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10501, loss 0.2495, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11001, loss 0.1347, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 11501, loss 0.1675, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 12001, loss 0.1336, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 12501, loss 0.1056, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 13001, loss 0.1172, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 13501, loss 0.1281, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 14001, loss 0.1607, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 14501, loss 0.1933, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 15001, loss 0.1381, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 15501, loss 0.1882, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 16001, loss 0.1415, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 16501, loss 0.1061, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 17001, loss 0.0864, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 17501, loss 0.2107, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 18001, loss 0.1839, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 18501, loss 0.1326, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19001, loss 0.1122, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19501, loss 0.1534, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
running_time is 20.090820193
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.18
normal_0.5
./test_glass0/model_MLP_20000_0.18/record_1/MLP_20000_0.18_1
./test_glass0/result_MLP_20000_0.18_normal_0.5/record_1/
----------------------



the AUC is 0.6083743842364531

the Fscore is 0.4

the precision is 0.6666666666666666

the recall is 0.2857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.18/record_1/MLP_20000_0.18_2
----------------------



epoch 1, loss 0.6419, train acc 40.35%, f1 0.5234, precision 0.3544, recall 1.0000, auc 0.5565
epoch 501, loss 0.4612, train acc 81.87%, f1 0.7737, precision 0.6543, recall 0.9464, auc 0.8515
epoch 1001, loss 0.3696, train acc 83.63%, f1 0.7879, precision 0.6842, recall 0.9286, auc 0.8599
epoch 1501, loss 0.2405, train acc 84.21%, f1 0.7970, precision 0.6883, recall 0.9464, auc 0.8689
epoch 2001, loss 0.2555, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 2501, loss 0.2284, train acc 88.30%, f1 0.8413, precision 0.7571, recall 0.9464, auc 0.8993
epoch 3001, loss 0.1770, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 3501, loss 0.2321, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 4001, loss 0.2975, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 4501, loss 0.2573, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 5001, loss 0.1904, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 5501, loss 0.2035, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 6001, loss 0.1024, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 6501, loss 0.1703, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 7001, loss 0.0953, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 7501, loss 0.1272, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 8001, loss 0.1693, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 8501, loss 0.1250, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 9001, loss 0.0877, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 9501, loss 0.1437, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 10001, loss 0.1621, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 10501, loss 0.0831, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 11001, loss 0.1932, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 11501, loss 0.1675, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 12001, loss 0.1084, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 12501, loss 0.0599, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 13001, loss 0.0848, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 13501, loss 0.0936, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 14001, loss 0.1853, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 14501, loss 0.1015, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 15001, loss 0.1169, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15501, loss 0.0953, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 16001, loss 0.1036, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 16501, loss 0.1090, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 17001, loss 0.0895, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 17501, loss 0.1400, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18001, loss 0.0799, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.1247, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.1208, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.1225, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
running_time is 19.989654022
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.18
normal_0.5
./test_glass0/model_MLP_20000_0.18/record_1/MLP_20000_0.18_2
./test_glass0/result_MLP_20000_0.18_normal_0.5/record_1/
----------------------



the AUC is 0.5369458128078817

the Fscore is 0.22222222222222224

the precision is 0.5

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.18/record_1/MLP_20000_0.18_3
----------------------



epoch 1, loss 0.5967, train acc 60.82%, f1 0.3232, precision 0.3721, recall 0.2857, auc 0.5255
epoch 501, loss 0.4119, train acc 77.78%, f1 0.7286, precision 0.6071, recall 0.9107, auc 0.8119
epoch 1001, loss 0.3838, train acc 79.53%, f1 0.7445, precision 0.6296, recall 0.9107, auc 0.8249
epoch 1501, loss 0.3931, train acc 80.70%, f1 0.7519, precision 0.6494, recall 0.8929, auc 0.8290
epoch 2001, loss 0.3285, train acc 85.38%, f1 0.8031, precision 0.7183, recall 0.9107, auc 0.8684
epoch 2501, loss 0.2698, train acc 87.13%, f1 0.8254, precision 0.7429, recall 0.9286, auc 0.8860
epoch 3001, loss 0.1512, train acc 87.72%, f1 0.8320, precision 0.7536, recall 0.9286, auc 0.8904
epoch 3501, loss 0.2442, train acc 87.13%, f1 0.8281, precision 0.7361, recall 0.9464, auc 0.8906
epoch 4001, loss 0.2024, train acc 89.47%, f1 0.8548, precision 0.7794, recall 0.9464, auc 0.9080
epoch 4501, loss 0.1926, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 5001, loss 0.1682, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 5501, loss 0.1580, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 6001, loss 0.2150, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 6501, loss 0.1598, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 7001, loss 0.1652, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 7501, loss 0.1622, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 8001, loss 0.1668, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 8501, loss 0.1595, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 9001, loss 0.1323, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 9501, loss 0.2365, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 10001, loss 0.1499, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 10501, loss 0.1390, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 11001, loss 0.1618, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 11501, loss 0.1495, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 12001, loss 0.1327, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 12501, loss 0.0947, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 13001, loss 0.1985, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 13501, loss 0.1143, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 14001, loss 0.1131, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 14501, loss 0.1506, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 15001, loss 0.1587, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 15501, loss 0.1803, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 16001, loss 0.1494, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 16501, loss 0.1042, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 17001, loss 0.1527, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 17501, loss 0.1212, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 18001, loss 0.1697, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18501, loss 0.1233, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 19001, loss 0.1538, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19501, loss 0.1249, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
running_time is 20.188792659
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.18
normal_0.5
./test_glass0/model_MLP_20000_0.18/record_1/MLP_20000_0.18_3
./test_glass0/result_MLP_20000_0.18_normal_0.5/record_1/
----------------------



the AUC is 0.5899014778325122

the Fscore is 0.3333333333333333

the precision is 0.75

the recall is 0.21428571428571427

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.18/record_1/MLP_20000_0.18_4
----------------------



epoch 1, loss 0.6271, train acc 50.88%, f1 0.0667, precision 0.0882, recall 0.0536, auc 0.3920
epoch 501, loss 0.3242, train acc 77.19%, f1 0.7273, precision 0.5977, recall 0.9286, auc 0.8121
epoch 1001, loss 0.3955, train acc 81.29%, f1 0.7681, precision 0.6463, recall 0.9464, auc 0.8471
epoch 1501, loss 0.2977, train acc 83.04%, f1 0.7852, precision 0.6709, recall 0.9464, auc 0.8602
epoch 2001, loss 0.2738, train acc 84.80%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8686
epoch 2501, loss 0.2158, train acc 85.96%, f1 0.8154, precision 0.7162, recall 0.9464, auc 0.8819
epoch 3001, loss 0.1991, train acc 86.55%, f1 0.8217, precision 0.7260, recall 0.9464, auc 0.8863
epoch 3501, loss 0.3402, train acc 87.13%, f1 0.8254, precision 0.7429, recall 0.9286, auc 0.8860
epoch 4001, loss 0.1732, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 4501, loss 0.1947, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 5001, loss 0.2125, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 5501, loss 0.2358, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 6001, loss 0.1929, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 6501, loss 0.2329, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 7001, loss 0.2348, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 7501, loss 0.1279, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 8001, loss 0.2019, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 8501, loss 0.1790, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 9001, loss 0.1339, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 9501, loss 0.1609, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 10001, loss 0.1264, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 10501, loss 0.1397, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 11001, loss 0.1430, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 11501, loss 0.1533, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 12001, loss 0.1341, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 12501, loss 0.1595, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 13001, loss 0.1925, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 13501, loss 0.1408, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 14001, loss 0.1416, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 14501, loss 0.1963, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 15001, loss 0.1595, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 15501, loss 0.1833, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 16001, loss 0.1544, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 16501, loss 0.2096, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 17001, loss 0.1954, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 17501, loss 0.1689, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 18001, loss 0.1630, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 18501, loss 0.1670, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 19001, loss 0.1467, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 19501, loss 0.1067, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
running_time is 20.104392697999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.18
normal_0.5
./test_glass0/model_MLP_20000_0.18/record_1/MLP_20000_0.18_4
./test_glass0/result_MLP_20000_0.18_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.18/record_1/MLP_20000_0.18_5
----------------------



epoch 1, loss 0.6267, train acc 59.30%, f1 0.6154, precision 0.4444, recall 1.0000, auc 0.6983
epoch 501, loss 0.3504, train acc 78.49%, f1 0.7376, precision 0.6118, recall 0.9286, auc 0.8220
epoch 1001, loss 0.3578, train acc 81.40%, f1 0.7647, precision 0.6500, recall 0.9286, auc 0.8436
epoch 1501, loss 0.2826, train acc 81.40%, f1 0.7714, precision 0.6429, recall 0.9643, auc 0.8528
epoch 2001, loss 0.2552, train acc 83.14%, f1 0.7852, precision 0.6709, recall 0.9464, auc 0.8611
epoch 2501, loss 0.2950, train acc 84.88%, f1 0.8060, precision 0.6923, recall 0.9643, auc 0.8787
epoch 3001, loss 0.2200, train acc 84.88%, f1 0.8060, precision 0.6923, recall 0.9643, auc 0.8787
epoch 3501, loss 0.2863, train acc 86.05%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8873
epoch 4001, loss 0.2919, train acc 87.21%, f1 0.8308, precision 0.7297, recall 0.9643, auc 0.8959
epoch 4501, loss 0.2222, train acc 86.63%, f1 0.8244, precision 0.7200, recall 0.9643, auc 0.8916
epoch 5001, loss 0.3222, train acc 85.47%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8830
epoch 5501, loss 0.2381, train acc 87.21%, f1 0.8308, precision 0.7297, recall 0.9643, auc 0.8959
epoch 6001, loss 0.3030, train acc 88.37%, f1 0.8437, precision 0.7500, recall 0.9643, auc 0.9046
epoch 6501, loss 0.1827, train acc 87.79%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9049
epoch 7001, loss 0.2849, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 7501, loss 0.2031, train acc 91.28%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9261
epoch 8001, loss 0.2038, train acc 90.12%, f1 0.8618, precision 0.7910, recall 0.9464, auc 0.9129
epoch 8501, loss 0.2792, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 9001, loss 0.2387, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 9501, loss 0.2007, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 10001, loss 0.2048, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 10501, loss 0.1893, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 11001, loss 0.2277, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 11501, loss 0.2325, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 12001, loss 0.1770, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 12501, loss 0.1635, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 13001, loss 0.1418, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 13501, loss 0.1200, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 14001, loss 0.1867, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 14501, loss 0.1095, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 15001, loss 0.1233, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 15501, loss 0.0857, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 16001, loss 0.1584, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 16501, loss 0.1038, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 17001, loss 0.1016, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 17501, loss 0.1332, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 18001, loss 0.0949, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 18501, loss 0.1394, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 19001, loss 0.1836, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 19501, loss 0.0786, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
running_time is 20.078830365
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.18
normal_0.5
./test_glass0/model_MLP_20000_0.18/record_1/MLP_20000_0.18_5
./test_glass0/result_MLP_20000_0.18_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.19/record_1/MLP_20000_0.19_1
----------------------



epoch 1, loss 0.6592, train acc 34.50%, f1 0.5000, precision 0.3333, recall 1.0000, auc 0.5130
epoch 501, loss 0.3836, train acc 78.36%, f1 0.7376, precision 0.6118, recall 0.9286, auc 0.8208
epoch 1001, loss 0.3475, train acc 80.12%, f1 0.7536, precision 0.6341, recall 0.9286, auc 0.8339
epoch 1501, loss 0.4623, train acc 78.95%, f1 0.7429, precision 0.6190, recall 0.9286, auc 0.8252
epoch 2001, loss 0.2836, train acc 81.29%, f1 0.7647, precision 0.6500, recall 0.9286, auc 0.8425
epoch 2501, loss 0.3614, train acc 82.46%, f1 0.7761, precision 0.6667, recall 0.9286, auc 0.8512
epoch 3001, loss 0.4032, train acc 84.21%, f1 0.7939, precision 0.6933, recall 0.9286, auc 0.8643
epoch 3501, loss 0.2530, train acc 84.80%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8686
epoch 4001, loss 0.2693, train acc 85.38%, f1 0.8062, precision 0.7123, recall 0.9286, auc 0.8730
epoch 4501, loss 0.1711, train acc 88.30%, f1 0.8413, precision 0.7571, recall 0.9464, auc 0.8993
epoch 5001, loss 0.2584, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 5501, loss 0.2134, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 6001, loss 0.1933, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 6501, loss 0.1819, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 7001, loss 0.3109, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 7501, loss 0.1709, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8001, loss 0.1898, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 8501, loss 0.1978, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 9001, loss 0.2240, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 9501, loss 0.2236, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 10001, loss 0.1964, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 10501, loss 0.1024, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 11001, loss 0.1239, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 11501, loss 0.1531, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 12001, loss 0.2046, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 12501, loss 0.2068, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 13001, loss 0.2564, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 13501, loss 0.1583, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 14001, loss 0.1783, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 14501, loss 0.1524, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 15001, loss 0.2083, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 15501, loss 0.0998, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 16001, loss 0.1480, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16501, loss 0.1051, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 17001, loss 0.2139, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 17501, loss 0.1884, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 18001, loss 0.2070, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 18501, loss 0.1713, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19001, loss 0.1141, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19501, loss 0.1018, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
running_time is 20.030677477
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.19
normal_0.5
./test_glass0/model_MLP_20000_0.19/record_1/MLP_20000_0.19_1
./test_glass0/result_MLP_20000_0.19_normal_0.5/record_1/
----------------------



the AUC is 0.5443349753694581

the Fscore is 0.4444444444444444

the precision is 0.36363636363636365

the recall is 0.5714285714285714

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.19/record_1/MLP_20000_0.19_2
----------------------



epoch 1, loss 0.6431, train acc 36.26%, f1 0.5068, precision 0.3394, recall 1.0000, auc 0.5261
epoch 501, loss 0.3982, train acc 78.95%, f1 0.7429, precision 0.6190, recall 0.9286, auc 0.8252
epoch 1001, loss 0.3811, train acc 83.04%, f1 0.7883, precision 0.6667, recall 0.9643, auc 0.8648
epoch 1501, loss 0.3083, train acc 85.38%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8821
epoch 2001, loss 0.2198, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 2501, loss 0.1998, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 3001, loss 0.2409, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 3501, loss 0.2317, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 4001, loss 0.2246, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 4501, loss 0.1823, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 5001, loss 0.1990, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 5501, loss 0.1823, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 6001, loss 0.1474, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 6501, loss 0.1523, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 7001, loss 0.1188, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 7501, loss 0.1769, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 8001, loss 0.1429, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 8501, loss 0.1028, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9001, loss 0.1389, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9501, loss 0.1881, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10001, loss 0.1299, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10501, loss 0.1046, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11001, loss 0.1215, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11501, loss 0.1077, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12001, loss 0.0594, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12501, loss 0.1290, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13001, loss 0.1326, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13501, loss 0.1259, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14001, loss 0.1426, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.1741, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15001, loss 0.1202, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.1052, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.1077, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.1317, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.1166, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17501, loss 0.0903, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.1316, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.0537, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.0971, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.1004, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 20.135295096
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.19
normal_0.5
./test_glass0/model_MLP_20000_0.19/record_1/MLP_20000_0.19_2
./test_glass0/result_MLP_20000_0.19_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.19/record_1/MLP_20000_0.19_3
----------------------



epoch 1, loss 0.6431, train acc 42.69%, f1 0.5333, precision 0.3636, recall 1.0000, auc 0.5739
epoch 501, loss 0.3447, train acc 76.61%, f1 0.7183, precision 0.5930, recall 0.9107, auc 0.8032
epoch 1001, loss 0.4051, train acc 80.12%, f1 0.7500, precision 0.6375, recall 0.9107, auc 0.8293
epoch 1501, loss 0.3374, train acc 80.70%, f1 0.7556, precision 0.6456, recall 0.9107, auc 0.8336
epoch 2001, loss 0.3790, train acc 83.63%, f1 0.7846, precision 0.6892, recall 0.9107, auc 0.8554
epoch 2501, loss 0.2689, train acc 84.80%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8686
epoch 3001, loss 0.2583, train acc 85.96%, f1 0.8154, precision 0.7162, recall 0.9464, auc 0.8819
epoch 3501, loss 0.3147, train acc 87.13%, f1 0.8281, precision 0.7361, recall 0.9464, auc 0.8906
epoch 4001, loss 0.2225, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 4501, loss 0.2074, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 5001, loss 0.2283, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 5501, loss 0.1959, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 6001, loss 0.2221, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 6501, loss 0.1782, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 7001, loss 0.1800, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 7501, loss 0.1460, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 8001, loss 0.1725, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 8501, loss 0.2241, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 9001, loss 0.1411, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 9501, loss 0.1206, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 10001, loss 0.1724, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 10501, loss 0.1878, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 11001, loss 0.1564, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 11501, loss 0.1342, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 12001, loss 0.1967, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 12501, loss 0.1280, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 13001, loss 0.1213, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 13501, loss 0.0818, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 14001, loss 0.1964, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 14501, loss 0.1759, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 15001, loss 0.1003, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 15501, loss 0.1155, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 16001, loss 0.0905, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 16501, loss 0.1352, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 17001, loss 0.1492, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17501, loss 0.1053, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18001, loss 0.1085, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18501, loss 0.0867, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19001, loss 0.1269, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19501, loss 0.0638, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
running_time is 20.095076896
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.19
normal_0.5
./test_glass0/model_MLP_20000_0.19/record_1/MLP_20000_0.19_3
./test_glass0/result_MLP_20000_0.19_normal_0.5/record_1/
----------------------



the AUC is 0.5357142857142857

the Fscore is 0.13333333333333333

the precision is 1.0

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.19/record_1/MLP_20000_0.19_4
----------------------



epoch 1, loss 0.5951, train acc 64.33%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4783
epoch 501, loss 0.4704, train acc 75.44%, f1 0.7200, precision 0.5745, recall 0.9643, auc 0.8082
epoch 1001, loss 0.3331, train acc 80.12%, f1 0.7571, precision 0.6310, recall 0.9464, auc 0.8384
epoch 1501, loss 0.3941, train acc 81.87%, f1 0.7737, precision 0.6543, recall 0.9464, auc 0.8515
epoch 2001, loss 0.2437, train acc 83.04%, f1 0.7852, precision 0.6709, recall 0.9464, auc 0.8602
epoch 2501, loss 0.3505, train acc 85.38%, f1 0.8092, precision 0.7067, recall 0.9464, auc 0.8776
epoch 3001, loss 0.3249, train acc 85.96%, f1 0.8154, precision 0.7162, recall 0.9464, auc 0.8819
epoch 3501, loss 0.3038, train acc 85.96%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8865
epoch 4001, loss 0.2943, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 4501, loss 0.2822, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 5001, loss 0.2748, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 5501, loss 0.2471, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 6001, loss 0.2241, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 6501, loss 0.2262, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 7001, loss 0.2432, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 7501, loss 0.2465, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 8001, loss 0.1504, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 8501, loss 0.2197, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 9001, loss 0.1248, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 9501, loss 0.1562, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 10001, loss 0.1848, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 10501, loss 0.1052, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 11001, loss 0.1886, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 11501, loss 0.1968, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 12001, loss 0.1442, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 12501, loss 0.1298, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 13001, loss 0.1776, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 13501, loss 0.1279, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 14001, loss 0.1704, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 14501, loss 0.1775, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 15001, loss 0.1207, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 15501, loss 0.1523, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 16001, loss 0.1406, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 16501, loss 0.1286, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 17001, loss 0.1514, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17501, loss 0.1033, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 18001, loss 0.1631, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 18501, loss 0.2094, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 19001, loss 0.1545, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19501, loss 0.1805, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
running_time is 20.069746853999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.19
normal_0.5
./test_glass0/model_MLP_20000_0.19/record_1/MLP_20000_0.19_4
./test_glass0/result_MLP_20000_0.19_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.19/record_1/MLP_20000_0.19_5
----------------------



epoch 1, loss 0.5472, train acc 64.53%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4784
epoch 501, loss 0.3951, train acc 77.91%, f1 0.7324, precision 0.6047, recall 0.9286, auc 0.8177
epoch 1001, loss 0.3180, train acc 80.81%, f1 0.7626, precision 0.6386, recall 0.9464, auc 0.8439
epoch 1501, loss 0.2927, train acc 83.14%, f1 0.7883, precision 0.6667, recall 0.9643, auc 0.8658
epoch 2001, loss 0.3464, train acc 84.30%, f1 0.7970, precision 0.6883, recall 0.9464, auc 0.8698
epoch 2501, loss 0.2648, train acc 86.05%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8873
epoch 3001, loss 0.2485, train acc 87.79%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9049
epoch 3501, loss 0.2588, train acc 88.37%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9092
epoch 4001, loss 0.2444, train acc 88.37%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9092
epoch 4501, loss 0.2080, train acc 88.95%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9135
epoch 5001, loss 0.2301, train acc 90.70%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9264
epoch 5501, loss 0.1740, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 6001, loss 0.1657, train acc 92.44%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9393
epoch 6501, loss 0.1545, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 7001, loss 0.1456, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 7501, loss 0.1226, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 8001, loss 0.1623, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 8501, loss 0.1853, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 9001, loss 0.1187, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 9501, loss 0.1930, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 10001, loss 0.2096, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 10501, loss 0.1809, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 11001, loss 0.1542, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 11501, loss 0.1716, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 12001, loss 0.1472, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 12501, loss 0.1619, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 13001, loss 0.1380, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 13501, loss 0.1121, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 14001, loss 0.1279, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 14501, loss 0.1275, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 15001, loss 0.0929, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 15501, loss 0.1516, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 16001, loss 0.1068, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 16501, loss 0.1147, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 17001, loss 0.1390, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 17501, loss 0.1061, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 18001, loss 0.0965, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 18501, loss 0.0918, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 19001, loss 0.0883, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 19501, loss 0.1634, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
running_time is 19.915380746
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.19
normal_0.5
./test_glass0/model_MLP_20000_0.19/record_1/MLP_20000_0.19_5
./test_glass0/result_MLP_20000_0.19_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.2/record_1/MLP_20000_0.2_1
----------------------



epoch 1, loss 0.6614, train acc 54.97%, f1 0.5926, precision 0.4211, recall 1.0000, auc 0.6652
epoch 501, loss 0.3403, train acc 80.70%, f1 0.7591, precision 0.6420, recall 0.9286, auc 0.8382
epoch 1001, loss 0.3383, train acc 81.29%, f1 0.7647, precision 0.6500, recall 0.9286, auc 0.8425
epoch 1501, loss 0.3994, train acc 83.04%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8556
epoch 2001, loss 0.2671, train acc 84.21%, f1 0.7970, precision 0.6883, recall 0.9464, auc 0.8689
epoch 2501, loss 0.3296, train acc 84.80%, f1 0.8030, precision 0.6974, recall 0.9464, auc 0.8732
epoch 3001, loss 0.2585, train acc 85.38%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8821
epoch 3501, loss 0.2970, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 4001, loss 0.2311, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 4501, loss 0.2700, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 5001, loss 0.2346, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 5501, loss 0.2258, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 6001, loss 0.2662, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 6501, loss 0.2065, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 7001, loss 0.1475, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 7501, loss 0.1692, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 8001, loss 0.1696, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 8501, loss 0.1374, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 9001, loss 0.1708, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 9501, loss 0.1010, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 10001, loss 0.1571, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 10501, loss 0.0964, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 11001, loss 0.1814, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 11501, loss 0.1592, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 12001, loss 0.1289, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12501, loss 0.1066, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13001, loss 0.1399, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13501, loss 0.0872, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14001, loss 0.1613, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14501, loss 0.1074, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.1552, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15501, loss 0.1832, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16001, loss 0.0871, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.1471, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17001, loss 0.1247, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17501, loss 0.1649, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18001, loss 0.1570, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.1551, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19001, loss 0.1558, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19501, loss 0.0697, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
running_time is 20.240758473
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.2
normal_0.5
./test_glass0/model_MLP_20000_0.2/record_1/MLP_20000_0.2_1
./test_glass0/result_MLP_20000_0.2_normal_0.5/record_1/
----------------------



the AUC is 0.5738916256157635

the Fscore is 0.36363636363636365

the precision is 0.5

the recall is 0.2857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.2/record_1/MLP_20000_0.2_2
----------------------



epoch 1, loss 0.6110, train acc 61.99%, f1 0.0299, precision 0.0909, recall 0.0179, auc 0.4655
epoch 501, loss 0.4523, train acc 78.95%, f1 0.7465, precision 0.6163, recall 0.9464, auc 0.8297
epoch 1001, loss 0.3692, train acc 84.21%, f1 0.7970, precision 0.6883, recall 0.9464, auc 0.8689
epoch 1501, loss 0.2674, train acc 85.38%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8821
epoch 2001, loss 0.2731, train acc 88.30%, f1 0.8437, precision 0.7500, recall 0.9643, auc 0.9039
epoch 2501, loss 0.3473, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 3001, loss 0.1819, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 3501, loss 0.1800, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 4001, loss 0.1472, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 4501, loss 0.2156, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 5001, loss 0.2234, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 5501, loss 0.1689, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 6001, loss 0.1678, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 6501, loss 0.1539, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 7001, loss 0.1447, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 7501, loss 0.1198, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 8001, loss 0.1573, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 8501, loss 0.0829, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9001, loss 0.1012, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9501, loss 0.1348, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10001, loss 0.1259, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10501, loss 0.1905, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11001, loss 0.1454, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 11501, loss 0.1841, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12001, loss 0.0825, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12501, loss 0.1396, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13001, loss 0.1161, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13501, loss 0.1008, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14001, loss 0.1310, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14501, loss 0.1083, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15001, loss 0.0874, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 15501, loss 0.0900, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 16001, loss 0.0669, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 16501, loss 0.1145, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17001, loss 0.0865, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17501, loss 0.0752, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18001, loss 0.0843, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18501, loss 0.1252, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19001, loss 0.1176, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19501, loss 0.1217, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 20.069207597
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.2
normal_0.5
./test_glass0/model_MLP_20000_0.2/record_1/MLP_20000_0.2_2
./test_glass0/result_MLP_20000_0.2_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.2/record_1/MLP_20000_0.2_3
----------------------



epoch 1, loss 0.5938, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.4203, train acc 73.68%, f1 0.6939, precision 0.5604, recall 0.9107, auc 0.7814
epoch 1001, loss 0.3627, train acc 78.36%, f1 0.7299, precision 0.6173, recall 0.8929, auc 0.8116
epoch 1501, loss 0.2756, train acc 81.29%, f1 0.7576, precision 0.6579, recall 0.8929, auc 0.8334
epoch 2001, loss 0.2747, train acc 84.80%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8686
epoch 2501, loss 0.2024, train acc 85.96%, f1 0.8154, precision 0.7162, recall 0.9464, auc 0.8819
epoch 3001, loss 0.2053, train acc 86.55%, f1 0.8244, precision 0.7200, recall 0.9643, auc 0.8908
epoch 3501, loss 0.3025, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 4001, loss 0.2423, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 4501, loss 0.1956, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 5001, loss 0.1547, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 5501, loss 0.1557, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 6001, loss 0.2478, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 6501, loss 0.1951, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 7001, loss 0.2179, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 7501, loss 0.1903, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 8001, loss 0.1685, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 8501, loss 0.1579, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 9001, loss 0.1867, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 9501, loss 0.2066, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 10001, loss 0.1792, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 10501, loss 0.2482, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 11001, loss 0.1545, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 11501, loss 0.1611, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 12001, loss 0.1955, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 12501, loss 0.1433, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 13001, loss 0.1094, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 13501, loss 0.1747, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 14001, loss 0.1307, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 14501, loss 0.1739, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 15001, loss 0.0997, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15501, loss 0.1543, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 16001, loss 0.1332, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 16501, loss 0.2103, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17001, loss 0.1635, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 17501, loss 0.1554, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 18001, loss 0.2146, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.0868, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19001, loss 0.1057, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19501, loss 0.1149, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
running_time is 20.169195776
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.2
normal_0.5
./test_glass0/model_MLP_20000_0.2/record_1/MLP_20000_0.2_3
./test_glass0/result_MLP_20000_0.2_normal_0.5/record_1/
----------------------



the AUC is 0.5899014778325122

the Fscore is 0.3333333333333333

the precision is 0.75

the recall is 0.21428571428571427

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.2/record_1/MLP_20000_0.2_4
----------------------



epoch 1, loss 0.6787, train acc 43.27%, f1 0.5359, precision 0.3660, recall 1.0000, auc 0.5783
epoch 501, loss 0.3556, train acc 78.36%, f1 0.7448, precision 0.6067, recall 0.9643, auc 0.8300
epoch 1001, loss 0.4247, train acc 79.53%, f1 0.7482, precision 0.6265, recall 0.9286, auc 0.8295
epoch 1501, loss 0.3353, train acc 80.12%, f1 0.7571, precision 0.6310, recall 0.9464, auc 0.8384
epoch 2001, loss 0.3760, train acc 82.46%, f1 0.7794, precision 0.6625, recall 0.9464, auc 0.8558
epoch 2501, loss 0.3709, train acc 85.38%, f1 0.8092, precision 0.7067, recall 0.9464, auc 0.8776
epoch 3001, loss 0.2617, train acc 87.13%, f1 0.8281, precision 0.7361, recall 0.9464, auc 0.8906
epoch 3501, loss 0.2327, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 4001, loss 0.1948, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 4501, loss 0.2065, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 5001, loss 0.1897, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 5501, loss 0.2398, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 6001, loss 0.2067, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 6501, loss 0.2013, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 7001, loss 0.1473, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 7501, loss 0.1566, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 8001, loss 0.1541, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 8501, loss 0.1927, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 9001, loss 0.1431, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 9501, loss 0.2101, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 10001, loss 0.2024, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 10501, loss 0.1350, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 11001, loss 0.1725, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 11501, loss 0.1003, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 12001, loss 0.1783, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12501, loss 0.1804, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13001, loss 0.1820, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13501, loss 0.2326, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14001, loss 0.1261, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14501, loss 0.1344, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15001, loss 0.1706, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15501, loss 0.1465, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16001, loss 0.1603, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16501, loss 0.1437, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 17001, loss 0.1725, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 17501, loss 0.1264, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18001, loss 0.1642, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 18501, loss 0.1732, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19001, loss 0.1167, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19501, loss 0.1517, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
running_time is 20.113137416999997
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.2
normal_0.5
./test_glass0/model_MLP_20000_0.2/record_1/MLP_20000_0.2_4
./test_glass0/result_MLP_20000_0.2_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.2/record_1/MLP_20000_0.2_5
----------------------



epoch 1, loss 0.6450, train acc 38.37%, f1 0.4952, precision 0.3377, recall 0.9286, auc 0.5246
epoch 501, loss 0.3597, train acc 78.49%, f1 0.7448, precision 0.6067, recall 0.9643, auc 0.8313
epoch 1001, loss 0.3048, train acc 81.40%, f1 0.7714, precision 0.6429, recall 0.9643, auc 0.8528
epoch 1501, loss 0.3420, train acc 81.40%, f1 0.7714, precision 0.6429, recall 0.9643, auc 0.8528
epoch 2001, loss 0.2519, train acc 82.56%, f1 0.7794, precision 0.6625, recall 0.9464, auc 0.8568
epoch 2501, loss 0.2757, train acc 85.47%, f1 0.8092, precision 0.7067, recall 0.9464, auc 0.8784
epoch 3001, loss 0.2865, train acc 86.63%, f1 0.8244, precision 0.7200, recall 0.9643, auc 0.8916
epoch 3501, loss 0.2501, train acc 87.79%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.9002
epoch 4001, loss 0.2768, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 4501, loss 0.2533, train acc 90.12%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9221
epoch 5001, loss 0.1484, train acc 90.70%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9264
epoch 5501, loss 0.1750, train acc 91.28%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9307
epoch 6001, loss 0.2459, train acc 91.28%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9307
epoch 6501, loss 0.1720, train acc 92.44%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9393
epoch 7001, loss 0.1986, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 7501, loss 0.1409, train acc 92.44%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9393
epoch 8001, loss 0.1615, train acc 92.44%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9393
epoch 8501, loss 0.2613, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 9001, loss 0.1840, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 9501, loss 0.2127, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 10001, loss 0.2108, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 10501, loss 0.1263, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 11001, loss 0.1590, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 11501, loss 0.1454, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 12001, loss 0.1794, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 12501, loss 0.1702, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 13001, loss 0.0835, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 13501, loss 0.1388, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 14001, loss 0.1221, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 14501, loss 0.1722, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 15001, loss 0.2276, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 15501, loss 0.1549, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 16001, loss 0.2117, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 16501, loss 0.1760, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 17001, loss 0.1687, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 17501, loss 0.1704, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 18001, loss 0.2039, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 18501, loss 0.1317, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 19001, loss 0.1303, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 19501, loss 0.1451, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
running_time is 20.076002463000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.2
normal_0.5
./test_glass0/model_MLP_20000_0.2/record_1/MLP_20000_0.2_5
./test_glass0/result_MLP_20000_0.2_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.21/record_1/MLP_20000_0.21_1
----------------------



epoch 1, loss 0.6647, train acc 36.26%, f1 0.5068, precision 0.3394, recall 1.0000, auc 0.5261
epoch 501, loss 0.3733, train acc 76.02%, f1 0.7248, precision 0.5806, recall 0.9643, auc 0.8126
epoch 1001, loss 0.3192, train acc 81.29%, f1 0.7681, precision 0.6463, recall 0.9464, auc 0.8471
epoch 1501, loss 0.3456, train acc 81.87%, f1 0.7737, precision 0.6543, recall 0.9464, auc 0.8515
epoch 2001, loss 0.3064, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 2501, loss 0.2609, train acc 85.38%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8821
epoch 3001, loss 0.3056, train acc 85.96%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8865
epoch 3501, loss 0.2664, train acc 85.96%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8865
epoch 4001, loss 0.2635, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 4501, loss 0.2826, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 5001, loss 0.2937, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 5501, loss 0.2544, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 6001, loss 0.1632, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 6501, loss 0.2014, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 7001, loss 0.2558, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 7501, loss 0.2316, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 8001, loss 0.1152, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8501, loss 0.2507, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9001, loss 0.2492, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9501, loss 0.3168, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10001, loss 0.1979, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10501, loss 0.3262, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11001, loss 0.1551, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11501, loss 0.2718, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 12001, loss 0.1828, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 12501, loss 0.1939, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 13001, loss 0.2353, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 13501, loss 0.2151, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 14001, loss 0.1868, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 14501, loss 0.2394, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 15001, loss 0.1943, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 15501, loss 0.1525, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 16001, loss 0.2191, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 16501, loss 0.1580, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 17001, loss 0.2408, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 17501, loss 0.2235, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18001, loss 0.2406, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18501, loss 0.1369, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19001, loss 0.1858, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19501, loss 0.2107, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
running_time is 20.085439804
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.21
normal_0.5
./test_glass0/model_MLP_20000_0.21/record_1/MLP_20000_0.21_1
./test_glass0/result_MLP_20000_0.21_normal_0.5/record_1/
----------------------



the AUC is 0.54064039408867

the Fscore is 0.3703703703703704

the precision is 0.38461538461538464

the recall is 0.35714285714285715

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.21/record_1/MLP_20000_0.21_2
----------------------



epoch 1, loss 0.6111, train acc 33.92%, f1 0.4933, precision 0.3293, recall 0.9821, auc 0.5041
epoch 501, loss 0.3571, train acc 79.53%, f1 0.7552, precision 0.6207, recall 0.9643, auc 0.8387
epoch 1001, loss 0.2505, train acc 83.04%, f1 0.7852, precision 0.6709, recall 0.9464, auc 0.8602
epoch 1501, loss 0.3265, train acc 86.55%, f1 0.8244, precision 0.7200, recall 0.9643, auc 0.8908
epoch 2001, loss 0.2199, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 2501, loss 0.2523, train acc 89.47%, f1 0.8548, precision 0.7794, recall 0.9464, auc 0.9080
epoch 3001, loss 0.2516, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 3501, loss 0.1060, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 4001, loss 0.2147, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 4501, loss 0.1794, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 5001, loss 0.1549, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 5501, loss 0.1484, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 6001, loss 0.1299, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 6501, loss 0.1781, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 7001, loss 0.0956, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 7501, loss 0.1299, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 8001, loss 0.1252, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 8501, loss 0.1736, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 9001, loss 0.1248, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 9501, loss 0.0937, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10001, loss 0.1263, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10501, loss 0.1238, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11001, loss 0.1478, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11501, loss 0.1504, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 12001, loss 0.1328, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12501, loss 0.1131, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13001, loss 0.1121, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13501, loss 0.0917, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 14001, loss 0.1335, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14501, loss 0.0946, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.1287, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15501, loss 0.0880, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16001, loss 0.1266, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16501, loss 0.1058, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.1600, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 17501, loss 0.0818, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18001, loss 0.1208, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.1060, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.0851, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.0868, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 20.130687493
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.21
normal_0.5
./test_glass0/model_MLP_20000_0.21/record_1/MLP_20000_0.21_2
./test_glass0/result_MLP_20000_0.21_normal_0.5/record_1/
----------------------



the AUC is 0.518472906403941

the Fscore is 0.125

the precision is 0.5

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.21/record_1/MLP_20000_0.21_3
----------------------



epoch 1, loss 0.5395, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.4660, train acc 74.27%, f1 0.7027, precision 0.5652, recall 0.9286, auc 0.7904
epoch 1001, loss 0.4520, train acc 77.78%, f1 0.7286, precision 0.6071, recall 0.9107, auc 0.8119
epoch 1501, loss 0.3274, train acc 78.95%, f1 0.7429, precision 0.6190, recall 0.9286, auc 0.8252
epoch 2001, loss 0.3358, train acc 81.29%, f1 0.7647, precision 0.6500, recall 0.9286, auc 0.8425
epoch 2501, loss 0.2735, train acc 85.38%, f1 0.8092, precision 0.7067, recall 0.9464, auc 0.8776
epoch 3001, loss 0.2375, train acc 87.13%, f1 0.8308, precision 0.7297, recall 0.9643, auc 0.8952
epoch 3501, loss 0.2596, train acc 87.72%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.8995
epoch 4001, loss 0.1877, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 4501, loss 0.1645, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 5001, loss 0.1802, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 5501, loss 0.1450, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 6001, loss 0.1666, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 6501, loss 0.1650, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 7001, loss 0.1677, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 7501, loss 0.2014, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 8001, loss 0.1417, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 8501, loss 0.1356, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 9001, loss 0.1250, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 9501, loss 0.1633, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10001, loss 0.1718, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10501, loss 0.1363, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11001, loss 0.1883, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11501, loss 0.0953, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 12001, loss 0.1383, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 12501, loss 0.1093, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 13001, loss 0.1257, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13501, loss 0.1423, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14001, loss 0.1415, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14501, loss 0.0990, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 15001, loss 0.1320, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 15501, loss 0.1534, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 16001, loss 0.1159, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 16501, loss 0.1852, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17001, loss 0.1307, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17501, loss 0.0784, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18001, loss 0.1066, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.0926, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.1291, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19501, loss 0.1319, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 20.11766038
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.21
normal_0.5
./test_glass0/model_MLP_20000_0.21/record_1/MLP_20000_0.21_3
./test_glass0/result_MLP_20000_0.21_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.21/record_1/MLP_20000_0.21_4
----------------------



epoch 1, loss 0.5929, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.4527, train acc 76.61%, f1 0.7260, precision 0.5889, recall 0.9464, auc 0.8123
epoch 1001, loss 0.3677, train acc 77.78%, f1 0.7324, precision 0.6047, recall 0.9286, auc 0.8165
epoch 1501, loss 0.3979, train acc 79.53%, f1 0.7482, precision 0.6265, recall 0.9286, auc 0.8295
epoch 2001, loss 0.3832, train acc 83.04%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8556
epoch 2501, loss 0.2869, train acc 84.21%, f1 0.7939, precision 0.6933, recall 0.9286, auc 0.8643
epoch 3001, loss 0.3251, train acc 87.13%, f1 0.8281, precision 0.7361, recall 0.9464, auc 0.8906
epoch 3501, loss 0.3387, train acc 88.30%, f1 0.8413, precision 0.7571, recall 0.9464, auc 0.8993
epoch 4001, loss 0.2858, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 4501, loss 0.2985, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 5001, loss 0.2824, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 5501, loss 0.2036, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 6001, loss 0.3209, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 6501, loss 0.2560, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 7001, loss 0.2013, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 7501, loss 0.2370, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 8001, loss 0.2717, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 8501, loss 0.1924, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 9001, loss 0.2391, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 9501, loss 0.2092, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 10001, loss 0.3040, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 10501, loss 0.3577, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 11001, loss 0.2266, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 11501, loss 0.2360, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 12001, loss 0.1830, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 12501, loss 0.2345, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 13001, loss 0.1359, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 13501, loss 0.1764, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 14001, loss 0.2564, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 14501, loss 0.2008, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 15001, loss 0.1895, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 15501, loss 0.2303, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 16001, loss 0.1902, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 16501, loss 0.2723, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 17001, loss 0.2013, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 17501, loss 0.1822, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 18001, loss 0.2004, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 18501, loss 0.1635, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 19001, loss 0.2139, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 19501, loss 0.2201, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
running_time is 20.021998393
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.21
normal_0.5
./test_glass0/model_MLP_20000_0.21/record_1/MLP_20000_0.21_4
./test_glass0/result_MLP_20000_0.21_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.21/record_1/MLP_20000_0.21_5
----------------------



epoch 1, loss 0.7901, train acc 34.88%, f1 0.5000, precision 0.3333, recall 1.0000, auc 0.5172
epoch 501, loss 0.4900, train acc 76.16%, f1 0.7248, precision 0.5806, recall 0.9643, auc 0.8140
epoch 1001, loss 0.4547, train acc 79.65%, f1 0.7518, precision 0.6235, recall 0.9464, auc 0.8353
epoch 1501, loss 0.4059, train acc 80.81%, f1 0.7660, precision 0.6353, recall 0.9643, auc 0.8485
epoch 2001, loss 0.2494, train acc 83.14%, f1 0.7883, precision 0.6667, recall 0.9643, auc 0.8658
epoch 2501, loss 0.3554, train acc 84.30%, f1 0.7970, precision 0.6883, recall 0.9464, auc 0.8698
epoch 3001, loss 0.3202, train acc 86.05%, f1 0.8154, precision 0.7162, recall 0.9464, auc 0.8827
epoch 3501, loss 0.2646, train acc 87.21%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.9006
epoch 4001, loss 0.2629, train acc 86.63%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8962
epoch 4501, loss 0.3177, train acc 86.63%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8962
epoch 5001, loss 0.2685, train acc 87.79%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9049
epoch 5501, loss 0.2682, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 6001, loss 0.2246, train acc 90.12%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9221
epoch 6501, loss 0.2576, train acc 90.12%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9221
epoch 7001, loss 0.1524, train acc 90.12%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9175
epoch 7501, loss 0.2153, train acc 91.28%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9261
epoch 8001, loss 0.2231, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 8501, loss 0.1735, train acc 91.28%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9353
epoch 9001, loss 0.1687, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 9501, loss 0.1738, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 10001, loss 0.1738, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 10501, loss 0.1648, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 11001, loss 0.1631, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 11501, loss 0.1634, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 12001, loss 0.2346, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 12501, loss 0.1269, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 13001, loss 0.2206, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 13501, loss 0.2488, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 14001, loss 0.1512, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 14501, loss 0.1637, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 15001, loss 0.1415, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 15501, loss 0.1694, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 16001, loss 0.2191, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 16501, loss 0.1791, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 17001, loss 0.1678, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 17501, loss 0.1991, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 18001, loss 0.1464, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 18501, loss 0.1188, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 19001, loss 0.1592, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 19501, loss 0.1644, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
running_time is 20.002438626
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.21
normal_0.5
./test_glass0/model_MLP_20000_0.21/record_1/MLP_20000_0.21_5
./test_glass0/result_MLP_20000_0.21_normal_0.5/record_1/
----------------------



the AUC is 0.6607142857142858

the Fscore is 0.5217391304347826

the precision is 0.6666666666666666

the recall is 0.42857142857142855

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.22/record_1/MLP_20000_0.22_1
----------------------



epoch 1, loss 0.5930, train acc 58.48%, f1 0.2198, precision 0.2857, recall 0.1786, auc 0.4806
epoch 501, loss 0.4303, train acc 77.19%, f1 0.7347, precision 0.5934, recall 0.9643, auc 0.8213
epoch 1001, loss 0.3548, train acc 81.29%, f1 0.7714, precision 0.6429, recall 0.9643, auc 0.8517
epoch 1501, loss 0.3378, train acc 82.46%, f1 0.7826, precision 0.6585, recall 0.9643, auc 0.8604
epoch 2001, loss 0.3100, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 2501, loss 0.3638, train acc 84.80%, f1 0.8030, precision 0.6974, recall 0.9464, auc 0.8732
epoch 3001, loss 0.2411, train acc 84.80%, f1 0.8030, precision 0.6974, recall 0.9464, auc 0.8732
epoch 3501, loss 0.2990, train acc 85.96%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8911
epoch 4001, loss 0.3558, train acc 85.96%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8911
epoch 4501, loss 0.2973, train acc 87.72%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9041
epoch 5001, loss 0.3106, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 5501, loss 0.2541, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 6001, loss 0.3114, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 6501, loss 0.1976, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 7001, loss 0.1492, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 7501, loss 0.1244, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 8001, loss 0.2134, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8501, loss 0.2110, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 9001, loss 0.2208, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9501, loss 0.1171, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10001, loss 0.2053, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10501, loss 0.1196, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 11001, loss 0.1581, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 11501, loss 0.1516, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 12001, loss 0.1405, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 12501, loss 0.2260, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 13001, loss 0.1532, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13501, loss 0.1733, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 14001, loss 0.2419, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14501, loss 0.1192, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15001, loss 0.2178, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15501, loss 0.1690, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16001, loss 0.2017, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16501, loss 0.1257, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17001, loss 0.0823, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17501, loss 0.2113, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18001, loss 0.1341, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18501, loss 0.1603, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19001, loss 0.1715, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19501, loss 0.1789, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
running_time is 20.072966713
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.22
normal_0.5
./test_glass0/model_MLP_20000_0.22/record_1/MLP_20000_0.22_1
./test_glass0/result_MLP_20000_0.22_normal_0.5/record_1/
----------------------



the AUC is 0.5270935960591133

the Fscore is 0.4324324324324324

the precision is 0.34782608695652173

the recall is 0.5714285714285714

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.22/record_1/MLP_20000_0.22_2
----------------------



epoch 1, loss 0.6118, train acc 53.80%, f1 0.4768, precision 0.3789, recall 0.6429, auc 0.5649
epoch 501, loss 0.4026, train acc 76.61%, f1 0.7260, precision 0.5889, recall 0.9464, auc 0.8123
epoch 1001, loss 0.3501, train acc 80.70%, f1 0.7626, precision 0.6386, recall 0.9464, auc 0.8428
epoch 1501, loss 0.2604, train acc 84.80%, f1 0.8088, precision 0.6875, recall 0.9821, auc 0.8824
epoch 2001, loss 0.2665, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 2501, loss 0.2954, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 3001, loss 0.2112, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 3501, loss 0.2319, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 4001, loss 0.1885, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 4501, loss 0.2339, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 5001, loss 0.2939, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 5501, loss 0.1867, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 6001, loss 0.1956, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 6501, loss 0.2439, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 7001, loss 0.1757, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 7501, loss 0.1367, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 8001, loss 0.1568, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 8501, loss 0.1394, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9001, loss 0.1734, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9501, loss 0.1386, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10001, loss 0.1678, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10501, loss 0.1113, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11001, loss 0.1636, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11501, loss 0.1305, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12001, loss 0.1412, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12501, loss 0.1537, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13001, loss 0.1445, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13501, loss 0.1201, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14001, loss 0.1046, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.1438, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15001, loss 0.1354, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15501, loss 0.0846, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16001, loss 0.1435, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16501, loss 0.1186, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.1244, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17501, loss 0.1084, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18001, loss 0.0990, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18501, loss 0.1179, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19001, loss 0.1646, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19501, loss 0.1039, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 20.095987126
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.22
normal_0.5
./test_glass0/model_MLP_20000_0.22/record_1/MLP_20000_0.22_2
./test_glass0/result_MLP_20000_0.22_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.22/record_1/MLP_20000_0.22_3
----------------------



epoch 1, loss 0.6498, train acc 34.50%, f1 0.5000, precision 0.3333, recall 1.0000, auc 0.5130
epoch 501, loss 0.4242, train acc 74.85%, f1 0.7075, precision 0.5714, recall 0.9286, auc 0.7947
epoch 1001, loss 0.3296, train acc 77.19%, f1 0.7234, precision 0.6000, recall 0.9107, auc 0.8075
epoch 1501, loss 0.2801, train acc 80.70%, f1 0.7626, precision 0.6386, recall 0.9464, auc 0.8428
epoch 2001, loss 0.3731, train acc 83.63%, f1 0.7910, precision 0.6795, recall 0.9464, auc 0.8645
epoch 2501, loss 0.2914, train acc 85.38%, f1 0.8092, precision 0.7067, recall 0.9464, auc 0.8776
epoch 3001, loss 0.2360, train acc 87.72%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.8995
epoch 3501, loss 0.2825, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 4001, loss 0.2311, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 4501, loss 0.1320, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 5001, loss 0.1978, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 5501, loss 0.1934, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 6001, loss 0.1732, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 6501, loss 0.1828, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 7001, loss 0.1954, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 7501, loss 0.1921, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 8001, loss 0.1491, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 8501, loss 0.1878, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9001, loss 0.1170, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 9501, loss 0.1577, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10001, loss 0.1485, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10501, loss 0.1686, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11001, loss 0.1711, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 11501, loss 0.1591, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12001, loss 0.1482, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12501, loss 0.1870, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13001, loss 0.1191, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13501, loss 0.1550, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14001, loss 0.1817, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14501, loss 0.1591, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.1150, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15501, loss 0.0780, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.1203, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.0980, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.1185, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.1206, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.1018, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.1173, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.0928, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.1523, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 20.217336093
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.22
normal_0.5
./test_glass0/model_MLP_20000_0.22/record_1/MLP_20000_0.22_3
./test_glass0/result_MLP_20000_0.22_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.22/record_1/MLP_20000_0.22_4
----------------------



epoch 1, loss 0.5741, train acc 67.84%, f1 0.2029, precision 0.5385, recall 0.1250, auc 0.5364
epoch 501, loss 0.3888, train acc 77.19%, f1 0.7347, precision 0.5934, recall 0.9643, auc 0.8213
epoch 1001, loss 0.3538, train acc 81.29%, f1 0.7681, precision 0.6463, recall 0.9464, auc 0.8471
epoch 1501, loss 0.3405, train acc 82.46%, f1 0.7794, precision 0.6625, recall 0.9464, auc 0.8558
epoch 2001, loss 0.3557, train acc 84.21%, f1 0.7970, precision 0.6883, recall 0.9464, auc 0.8689
epoch 2501, loss 0.3302, train acc 85.38%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8821
epoch 3001, loss 0.3062, train acc 85.96%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8911
epoch 3501, loss 0.2491, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 4001, loss 0.2959, train acc 87.72%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.8995
epoch 4501, loss 0.3102, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 5001, loss 0.1807, train acc 88.30%, f1 0.8437, precision 0.7500, recall 0.9643, auc 0.9039
epoch 5501, loss 0.1747, train acc 87.72%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.8995
epoch 6001, loss 0.2446, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 6501, loss 0.2546, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 7001, loss 0.2222, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 7501, loss 0.1763, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 8001, loss 0.2058, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 8501, loss 0.2165, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 9001, loss 0.1665, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 9501, loss 0.1248, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 10001, loss 0.2285, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 10501, loss 0.2220, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 11001, loss 0.1327, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 11501, loss 0.2011, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 12001, loss 0.1345, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 12501, loss 0.1987, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 13001, loss 0.1523, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 13501, loss 0.0986, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 14001, loss 0.1390, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 14501, loss 0.1469, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 15001, loss 0.1226, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 15501, loss 0.1682, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 16001, loss 0.1354, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 16501, loss 0.1870, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 17001, loss 0.1282, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 17501, loss 0.1813, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 18001, loss 0.2564, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 18501, loss 0.1306, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 19001, loss 0.1335, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 19501, loss 0.1423, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
running_time is 20.059844265
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.22
normal_0.5
./test_glass0/model_MLP_20000_0.22/record_1/MLP_20000_0.22_4
./test_glass0/result_MLP_20000_0.22_normal_0.5/record_1/
----------------------



the AUC is 0.5357142857142857

the Fscore is 0.13333333333333333

the precision is 1.0

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.22/record_1/MLP_20000_0.22_5
----------------------



epoch 1, loss 0.6685, train acc 47.09%, f1 0.5517, precision 0.3810, recall 1.0000, auc 0.6078
epoch 501, loss 0.4307, train acc 76.74%, f1 0.7297, precision 0.5870, recall 0.9643, auc 0.8183
epoch 1001, loss 0.3467, train acc 79.07%, f1 0.7465, precision 0.6163, recall 0.9464, auc 0.8310
epoch 1501, loss 0.3062, train acc 80.23%, f1 0.7606, precision 0.6279, recall 0.9643, auc 0.8442
epoch 2001, loss 0.3147, train acc 81.98%, f1 0.7770, precision 0.6506, recall 0.9643, auc 0.8571
epoch 2501, loss 0.3593, train acc 84.30%, f1 0.7970, precision 0.6883, recall 0.9464, auc 0.8698
epoch 3001, loss 0.2898, train acc 85.47%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8830
epoch 3501, loss 0.2147, train acc 86.63%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8962
epoch 4001, loss 0.3261, train acc 87.79%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9049
epoch 4501, loss 0.2023, train acc 87.79%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9049
epoch 5001, loss 0.1712, train acc 88.37%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9092
epoch 5501, loss 0.2341, train acc 90.12%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9267
epoch 6001, loss 0.2053, train acc 91.28%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9353
epoch 6501, loss 0.1573, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 7001, loss 0.1941, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 7501, loss 0.2364, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 8001, loss 0.2630, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 8501, loss 0.2054, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 9001, loss 0.1631, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 9501, loss 0.1069, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 10001, loss 0.1350, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 10501, loss 0.1162, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 11001, loss 0.2175, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 11501, loss 0.1816, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 12001, loss 0.1613, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 12501, loss 0.2020, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 13001, loss 0.1930, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 13501, loss 0.1079, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 14001, loss 0.1628, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 14501, loss 0.1581, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 15001, loss 0.1063, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 15501, loss 0.1726, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 16001, loss 0.1972, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 16501, loss 0.1223, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 17001, loss 0.1354, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 17501, loss 0.2109, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 18001, loss 0.1347, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 18501, loss 0.1303, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 19001, loss 0.1389, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 19501, loss 0.2322, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
running_time is 20.087877964999997
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.22
normal_0.5
./test_glass0/model_MLP_20000_0.22/record_1/MLP_20000_0.22_5
./test_glass0/result_MLP_20000_0.22_normal_0.5/record_1/
----------------------



the AUC is 0.75

the Fscore is 0.6666666666666667

the precision is 0.5789473684210527

the recall is 0.7857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.23/record_1/MLP_20000_0.23_1
----------------------



epoch 1, loss 0.7523, train acc 44.44%, f1 0.5411, precision 0.3709, recall 1.0000, auc 0.5870
epoch 501, loss 0.3350, train acc 78.36%, f1 0.7448, precision 0.6067, recall 0.9643, auc 0.8300
epoch 1001, loss 0.3470, train acc 81.87%, f1 0.7770, precision 0.6506, recall 0.9643, auc 0.8561
epoch 1501, loss 0.3145, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 2001, loss 0.2728, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 2501, loss 0.1998, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 3001, loss 0.2677, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 3501, loss 0.2410, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 4001, loss 0.2353, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 4501, loss 0.2739, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 5001, loss 0.3031, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 5501, loss 0.2602, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 6001, loss 0.2444, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 6501, loss 0.1802, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 7001, loss 0.1877, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 7501, loss 0.2050, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8001, loss 0.2230, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 8501, loss 0.1545, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 9001, loss 0.1774, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9501, loss 0.1473, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 10001, loss 0.1917, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 10501, loss 0.1681, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 11001, loss 0.1961, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 11501, loss 0.1097, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12001, loss 0.1624, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12501, loss 0.1744, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13001, loss 0.1375, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13501, loss 0.1811, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14001, loss 0.1469, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14501, loss 0.0965, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15001, loss 0.1502, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15501, loss 0.1467, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16001, loss 0.1506, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16501, loss 0.1459, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17001, loss 0.1443, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17501, loss 0.1385, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 18001, loss 0.1121, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 18501, loss 0.1797, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 19001, loss 0.1136, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 19501, loss 0.0834, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
running_time is 20.057097971
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.23
normal_0.5
./test_glass0/model_MLP_20000_0.23/record_1/MLP_20000_0.23_1
./test_glass0/result_MLP_20000_0.23_normal_0.5/record_1/
----------------------



the AUC is 0.5036945812807881

the Fscore is 0.2608695652173913

the precision is 0.3333333333333333

the recall is 0.21428571428571427

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.23/record_1/MLP_20000_0.23_2
----------------------



epoch 1, loss 0.6727, train acc 47.95%, f1 0.5528, precision 0.3846, recall 0.9821, auc 0.6085
epoch 501, loss 0.4308, train acc 76.61%, f1 0.7297, precision 0.5870, recall 0.9643, auc 0.8169
epoch 1001, loss 0.3667, train acc 82.46%, f1 0.7857, precision 0.6548, recall 0.9821, auc 0.8650
epoch 1501, loss 0.4352, train acc 83.04%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8693
epoch 2001, loss 0.2840, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 2501, loss 0.3064, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 3001, loss 0.2335, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 3501, loss 0.1630, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 4001, loss 0.2725, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 4501, loss 0.2306, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 5001, loss 0.1960, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 5501, loss 0.2046, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 6001, loss 0.1973, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 6501, loss 0.1613, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 7001, loss 0.1737, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 7501, loss 0.1657, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 8001, loss 0.1074, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 8501, loss 0.1929, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 9001, loss 0.1721, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 9501, loss 0.1807, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10001, loss 0.1580, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10501, loss 0.1146, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11001, loss 0.1440, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11501, loss 0.1443, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12001, loss 0.1650, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12501, loss 0.1798, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13001, loss 0.1401, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13501, loss 0.1130, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14001, loss 0.1175, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14501, loss 0.1586, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 15001, loss 0.1137, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.0943, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16001, loss 0.1183, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.1102, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.1754, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17501, loss 0.1504, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18001, loss 0.1470, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.1234, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.1003, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19501, loss 0.0829, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 20.294907898
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.23
normal_0.5
./test_glass0/model_MLP_20000_0.23/record_1/MLP_20000_0.23_2
./test_glass0/result_MLP_20000_0.23_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.23/record_1/MLP_20000_0.23_3
----------------------



epoch 1, loss 0.7126, train acc 32.75%, f1 0.4934, precision 0.3275, recall 1.0000, auc 0.5000
epoch 501, loss 0.4874, train acc 71.35%, f1 0.6797, precision 0.5361, recall 0.9286, auc 0.7686
epoch 1001, loss 0.2799, train acc 77.19%, f1 0.7273, precision 0.5977, recall 0.9286, auc 0.8121
epoch 1501, loss 0.3095, train acc 78.95%, f1 0.7465, precision 0.6163, recall 0.9464, auc 0.8297
epoch 2001, loss 0.3895, train acc 82.46%, f1 0.7794, precision 0.6625, recall 0.9464, auc 0.8558
epoch 2501, loss 0.2292, train acc 85.38%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8821
epoch 3001, loss 0.2763, train acc 86.55%, f1 0.8244, precision 0.7200, recall 0.9643, auc 0.8908
epoch 3501, loss 0.2308, train acc 87.72%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9041
epoch 4001, loss 0.2174, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 4501, loss 0.1909, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 5001, loss 0.2345, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 5501, loss 0.2688, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 6001, loss 0.1857, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 6501, loss 0.1982, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 7001, loss 0.1964, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 7501, loss 0.1500, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 8001, loss 0.1733, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 8501, loss 0.1214, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 9001, loss 0.1827, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 9501, loss 0.2144, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 10001, loss 0.1551, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 10501, loss 0.1805, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 11001, loss 0.1990, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 11501, loss 0.1546, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 12001, loss 0.1351, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 12501, loss 0.1705, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 13001, loss 0.1498, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 13501, loss 0.1540, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 14001, loss 0.2349, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 14501, loss 0.1532, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 15001, loss 0.1331, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 15501, loss 0.1131, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 16001, loss 0.0946, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 16501, loss 0.1939, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 17001, loss 0.1464, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 17501, loss 0.1446, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 18001, loss 0.1571, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 18501, loss 0.1899, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 19001, loss 0.1174, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 19501, loss 0.0903, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
running_time is 20.107316488
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.23
normal_0.5
./test_glass0/model_MLP_20000_0.23/record_1/MLP_20000_0.23_3
./test_glass0/result_MLP_20000_0.23_normal_0.5/record_1/
----------------------



the AUC is 0.5172413793103448

the Fscore is 0.5

the precision is 0.3333333333333333

the recall is 1.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.23/record_1/MLP_20000_0.23_4
----------------------



epoch 1, loss 0.6329, train acc 46.20%, f1 0.5490, precision 0.3784, recall 1.0000, auc 0.6000
epoch 501, loss 0.4562, train acc 73.68%, f1 0.7059, precision 0.5567, recall 0.9643, auc 0.7952
epoch 1001, loss 0.3693, train acc 78.95%, f1 0.7465, precision 0.6163, recall 0.9464, auc 0.8297
epoch 1501, loss 0.3846, train acc 80.70%, f1 0.7626, precision 0.6386, recall 0.9464, auc 0.8428
epoch 2001, loss 0.3699, train acc 81.29%, f1 0.7681, precision 0.6463, recall 0.9464, auc 0.8471
epoch 2501, loss 0.3401, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 3001, loss 0.2364, train acc 84.80%, f1 0.8060, precision 0.6923, recall 0.9643, auc 0.8778
epoch 3501, loss 0.2463, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 4001, loss 0.2750, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 4501, loss 0.2642, train acc 85.96%, f1 0.8154, precision 0.7162, recall 0.9464, auc 0.8819
epoch 5001, loss 0.2535, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 5501, loss 0.1887, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 6001, loss 0.2395, train acc 87.72%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9041
epoch 6501, loss 0.2722, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 7001, loss 0.2152, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 7501, loss 0.1589, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 8001, loss 0.2797, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 8501, loss 0.1793, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 9001, loss 0.2243, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 9501, loss 0.1439, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 10001, loss 0.2323, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 10501, loss 0.1428, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 11001, loss 0.1200, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 11501, loss 0.1805, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 12001, loss 0.1486, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 12501, loss 0.1868, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 13001, loss 0.1969, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 13501, loss 0.1751, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 14001, loss 0.1861, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 14501, loss 0.1842, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 15001, loss 0.1501, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 15501, loss 0.1988, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 16001, loss 0.1476, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 16501, loss 0.1392, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 17001, loss 0.2205, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 17501, loss 0.2121, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18001, loss 0.1739, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 18501, loss 0.1491, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19001, loss 0.1414, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19501, loss 0.1332, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
running_time is 20.267317955
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.23
normal_0.5
./test_glass0/model_MLP_20000_0.23/record_1/MLP_20000_0.23_4
./test_glass0/result_MLP_20000_0.23_normal_0.5/record_1/
----------------------



the AUC is 0.6637931034482759

the Fscore is 0.5384615384615384

the precision is 0.5833333333333334

the recall is 0.5

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.23/record_1/MLP_20000_0.23_5
----------------------



epoch 1, loss 0.5335, train acc 66.28%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4914
epoch 501, loss 0.3864, train acc 76.74%, f1 0.7297, precision 0.5870, recall 0.9643, auc 0.8183
epoch 1001, loss 0.3440, train acc 80.81%, f1 0.7660, precision 0.6353, recall 0.9643, auc 0.8485
epoch 1501, loss 0.3772, train acc 81.40%, f1 0.7714, precision 0.6429, recall 0.9643, auc 0.8528
epoch 2001, loss 0.3035, train acc 84.30%, f1 0.8029, precision 0.6790, recall 0.9821, auc 0.8790
epoch 2501, loss 0.2860, train acc 85.47%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8830
epoch 3001, loss 0.3044, train acc 86.63%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8962
epoch 3501, loss 0.1647, train acc 87.21%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.9006
epoch 4001, loss 0.2044, train acc 87.79%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9049
epoch 4501, loss 0.2364, train acc 88.37%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9092
epoch 5001, loss 0.2923, train acc 87.79%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9049
epoch 5501, loss 0.2757, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 6001, loss 0.2330, train acc 90.70%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9264
epoch 6501, loss 0.1471, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 7001, loss 0.1344, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 7501, loss 0.1709, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 8001, loss 0.1452, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 8501, loss 0.1739, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 9001, loss 0.1977, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 9501, loss 0.1631, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 10001, loss 0.1967, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 10501, loss 0.1824, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 11001, loss 0.2020, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 11501, loss 0.1252, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 12001, loss 0.1247, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 12501, loss 0.1609, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 13001, loss 0.1224, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 13501, loss 0.1243, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 14001, loss 0.1895, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 14501, loss 0.1467, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 15001, loss 0.0793, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 15501, loss 0.1526, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 16001, loss 0.1250, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 16501, loss 0.1879, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 17001, loss 0.1343, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 17501, loss 0.2185, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 18001, loss 0.1591, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 18501, loss 0.1442, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 19001, loss 0.1289, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 19501, loss 0.1157, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
running_time is 20.043061255
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.23
normal_0.5
./test_glass0/model_MLP_20000_0.23/record_1/MLP_20000_0.23_5
./test_glass0/result_MLP_20000_0.23_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.24/record_1/MLP_20000_0.24_1
----------------------



epoch 1, loss 0.5522, train acc 64.33%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4783
epoch 501, loss 0.3173, train acc 77.19%, f1 0.7347, precision 0.5934, recall 0.9643, auc 0.8213
epoch 1001, loss 0.3340, train acc 80.12%, f1 0.7536, precision 0.6341, recall 0.9286, auc 0.8339
epoch 1501, loss 0.4042, train acc 81.87%, f1 0.7770, precision 0.6506, recall 0.9643, auc 0.8561
epoch 2001, loss 0.3439, train acc 81.29%, f1 0.7714, precision 0.6429, recall 0.9643, auc 0.8517
epoch 2501, loss 0.2909, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 3001, loss 0.3596, train acc 84.80%, f1 0.8060, precision 0.6923, recall 0.9643, auc 0.8778
epoch 3501, loss 0.2867, train acc 85.96%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8911
epoch 4001, loss 0.2545, train acc 85.96%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8911
epoch 4501, loss 0.3558, train acc 85.96%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8911
epoch 5001, loss 0.2589, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 5501, loss 0.3166, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 6001, loss 0.1612, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 6501, loss 0.2598, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 7001, loss 0.2477, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 7501, loss 0.2387, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 8001, loss 0.2220, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 8501, loss 0.2484, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 9001, loss 0.1798, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 9501, loss 0.1482, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 10001, loss 0.2560, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 10501, loss 0.2335, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11001, loss 0.1759, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11501, loss 0.1708, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 12001, loss 0.2134, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 12501, loss 0.1894, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 13001, loss 0.2198, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 13501, loss 0.1894, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 14001, loss 0.2768, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 14501, loss 0.1837, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 15001, loss 0.1288, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 15501, loss 0.2231, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 16001, loss 0.2224, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 16501, loss 0.1487, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 17001, loss 0.2251, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 17501, loss 0.1797, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 18001, loss 0.1514, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 18501, loss 0.2006, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19001, loss 0.0942, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19501, loss 0.1403, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
running_time is 20.188850955
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.24
normal_0.5
./test_glass0/model_MLP_20000_0.24/record_1/MLP_20000_0.24_1
./test_glass0/result_MLP_20000_0.24_normal_0.5/record_1/
----------------------



the AUC is 0.456896551724138

the Fscore is 0.3684210526315789

the precision is 0.2916666666666667

the recall is 0.5

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.24/record_1/MLP_20000_0.24_2
----------------------



epoch 1, loss 0.6985, train acc 44.44%, f1 0.5411, precision 0.3709, recall 1.0000, auc 0.5870
epoch 501, loss 0.3126, train acc 76.61%, f1 0.7260, precision 0.5889, recall 0.9464, auc 0.8123
epoch 1001, loss 0.3306, train acc 83.04%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8693
epoch 1501, loss 0.3073, train acc 84.21%, f1 0.8029, precision 0.6790, recall 0.9821, auc 0.8780
epoch 2001, loss 0.2304, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 2501, loss 0.2704, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 3001, loss 0.2380, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 3501, loss 0.2309, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 4001, loss 0.1753, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 4501, loss 0.2100, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 5001, loss 0.2211, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 5501, loss 0.2014, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 6001, loss 0.2295, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 6501, loss 0.2543, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 7001, loss 0.1523, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 7501, loss 0.1957, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8001, loss 0.1600, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 8501, loss 0.1564, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 9001, loss 0.1887, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 9501, loss 0.0741, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 10001, loss 0.1812, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 10501, loss 0.1585, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11001, loss 0.1295, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 11501, loss 0.1759, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12001, loss 0.1215, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12501, loss 0.1250, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13001, loss 0.1225, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 13501, loss 0.1408, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14001, loss 0.1425, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14501, loss 0.1161, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15001, loss 0.1149, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15501, loss 0.1500, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.1252, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.1199, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17001, loss 0.1151, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.1590, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.1605, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.1260, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.0939, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.1504, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 20.067247062
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.24
normal_0.5
./test_glass0/model_MLP_20000_0.24/record_1/MLP_20000_0.24_2
./test_glass0/result_MLP_20000_0.24_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.24/record_1/MLP_20000_0.24_3
----------------------



epoch 1, loss 0.5731, train acc 67.25%, f1 0.5625, precision 0.5000, recall 0.6429, auc 0.6649
epoch 501, loss 0.3472, train acc 74.85%, f1 0.7075, precision 0.5714, recall 0.9286, auc 0.7947
epoch 1001, loss 0.4020, train acc 77.78%, f1 0.7324, precision 0.6047, recall 0.9286, auc 0.8165
epoch 1501, loss 0.3204, train acc 80.70%, f1 0.7626, precision 0.6386, recall 0.9464, auc 0.8428
epoch 2001, loss 0.3136, train acc 84.80%, f1 0.8088, precision 0.6875, recall 0.9821, auc 0.8824
epoch 2501, loss 0.2535, train acc 85.38%, f1 0.8148, precision 0.6962, recall 0.9821, auc 0.8867
epoch 3001, loss 0.2871, train acc 87.13%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.8998
epoch 3501, loss 0.2257, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 4001, loss 0.1664, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 4501, loss 0.2270, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 5001, loss 0.2202, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 5501, loss 0.1676, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 6001, loss 0.1929, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 6501, loss 0.1809, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 7001, loss 0.1546, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 7501, loss 0.1451, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 8001, loss 0.2343, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 8501, loss 0.1239, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9001, loss 0.1564, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9501, loss 0.1553, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10001, loss 0.1133, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10501, loss 0.1106, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11001, loss 0.1021, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11501, loss 0.1158, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12001, loss 0.1417, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12501, loss 0.1529, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13001, loss 0.1548, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13501, loss 0.1257, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14001, loss 0.0999, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14501, loss 0.1236, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.1044, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.0918, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.1236, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16501, loss 0.1077, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.1200, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17501, loss 0.0977, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18001, loss 0.1172, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18501, loss 0.0782, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19001, loss 0.1253, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19501, loss 0.1026, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 19.985369830000003
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.24
normal_0.5
./test_glass0/model_MLP_20000_0.24/record_1/MLP_20000_0.24_3
./test_glass0/result_MLP_20000_0.24_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.24/record_1/MLP_20000_0.24_4
----------------------



epoch 1, loss 0.5939, train acc 54.39%, f1 0.5895, precision 0.4179, recall 1.0000, auc 0.6609
epoch 501, loss 0.3809, train acc 74.85%, f1 0.7152, precision 0.5684, recall 0.9643, auc 0.8039
epoch 1001, loss 0.3176, train acc 77.78%, f1 0.7361, precision 0.6023, recall 0.9464, auc 0.8210
epoch 1501, loss 0.3019, train acc 80.12%, f1 0.7571, precision 0.6310, recall 0.9464, auc 0.8384
epoch 2001, loss 0.4516, train acc 83.04%, f1 0.7883, precision 0.6667, recall 0.9643, auc 0.8648
epoch 2501, loss 0.3755, train acc 83.04%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8693
epoch 3001, loss 0.2931, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 3501, loss 0.3160, train acc 85.96%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8911
epoch 4001, loss 0.2996, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 4501, loss 0.2849, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 5001, loss 0.3305, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 5501, loss 0.2242, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 6001, loss 0.3378, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 6501, loss 0.1795, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 7001, loss 0.2316, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 7501, loss 0.2195, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 8001, loss 0.2693, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 8501, loss 0.2284, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 9001, loss 0.2339, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 9501, loss 0.2190, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 10001, loss 0.2525, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 10501, loss 0.2744, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 11001, loss 0.2889, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 11501, loss 0.2252, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 12001, loss 0.1918, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 12501, loss 0.1671, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 13001, loss 0.3434, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 13501, loss 0.1571, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 14001, loss 0.2203, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 14501, loss 0.1887, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 15001, loss 0.1458, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 15501, loss 0.2144, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 16001, loss 0.2022, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 16501, loss 0.1818, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 17001, loss 0.1770, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 17501, loss 0.2740, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 18001, loss 0.1646, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 18501, loss 0.2198, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 19001, loss 0.2355, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 19501, loss 0.2557, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
running_time is 20.194653873999997
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.24
normal_0.5
./test_glass0/model_MLP_20000_0.24/record_1/MLP_20000_0.24_4
./test_glass0/result_MLP_20000_0.24_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.24/record_1/MLP_20000_0.24_5
----------------------



epoch 1, loss 0.6149, train acc 33.14%, f1 0.4934, precision 0.3275, recall 1.0000, auc 0.5043
epoch 501, loss 0.3893, train acc 76.16%, f1 0.7248, precision 0.5806, recall 0.9643, auc 0.8140
epoch 1001, loss 0.3447, train acc 79.65%, f1 0.7552, precision 0.6207, recall 0.9643, auc 0.8399
epoch 1501, loss 0.4057, train acc 80.81%, f1 0.7660, precision 0.6353, recall 0.9643, auc 0.8485
epoch 2001, loss 0.3854, train acc 82.56%, f1 0.7794, precision 0.6625, recall 0.9464, auc 0.8568
epoch 2501, loss 0.2880, train acc 82.56%, f1 0.7794, precision 0.6625, recall 0.9464, auc 0.8568
epoch 3001, loss 0.2535, train acc 84.30%, f1 0.8000, precision 0.6835, recall 0.9643, auc 0.8744
epoch 3501, loss 0.2614, train acc 84.88%, f1 0.8060, precision 0.6923, recall 0.9643, auc 0.8787
epoch 4001, loss 0.3453, train acc 84.88%, f1 0.8060, precision 0.6923, recall 0.9643, auc 0.8787
epoch 4501, loss 0.1830, train acc 86.05%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8873
epoch 5001, loss 0.2962, train acc 86.63%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8962
epoch 5501, loss 0.2514, train acc 87.79%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9049
epoch 6001, loss 0.2536, train acc 87.79%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9049
epoch 6501, loss 0.2300, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 7001, loss 0.2398, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 7501, loss 0.1963, train acc 90.70%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9310
epoch 8001, loss 0.1543, train acc 90.70%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9264
epoch 8501, loss 0.2473, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 9001, loss 0.1940, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 9501, loss 0.2623, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 10001, loss 0.1780, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 10501, loss 0.1604, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 11001, loss 0.1869, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 11501, loss 0.2058, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 12001, loss 0.1474, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 12501, loss 0.1132, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 13001, loss 0.1351, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 13501, loss 0.2331, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 14001, loss 0.2018, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 14501, loss 0.1765, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 15001, loss 0.2322, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 15501, loss 0.1771, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 16001, loss 0.2227, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 16501, loss 0.2104, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 17001, loss 0.1290, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 17501, loss 0.1225, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 18001, loss 0.1118, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 18501, loss 0.1251, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 19001, loss 0.1434, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 19501, loss 0.1771, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
running_time is 20.043232302
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.24
normal_0.5
./test_glass0/model_MLP_20000_0.24/record_1/MLP_20000_0.24_5
./test_glass0/result_MLP_20000_0.24_normal_0.5/record_1/
----------------------



the AUC is 0.5535714285714285

the Fscore is 0.23529411764705882

the precision is 0.6666666666666666

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.25/record_1/MLP_20000_0.25_1
----------------------



epoch 1, loss 0.6390, train acc 46.20%, f1 0.5490, precision 0.3784, recall 1.0000, auc 0.6000
epoch 501, loss 0.4395, train acc 76.61%, f1 0.7297, precision 0.5870, recall 0.9643, auc 0.8169
epoch 1001, loss 0.3791, train acc 80.70%, f1 0.7660, precision 0.6353, recall 0.9643, auc 0.8474
epoch 1501, loss 0.3963, train acc 83.04%, f1 0.7883, precision 0.6667, recall 0.9643, auc 0.8648
epoch 2001, loss 0.2731, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 2501, loss 0.3069, train acc 84.80%, f1 0.8060, precision 0.6923, recall 0.9643, auc 0.8778
epoch 3001, loss 0.3759, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 3501, loss 0.2533, train acc 85.96%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8911
epoch 4001, loss 0.3266, train acc 87.13%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.8998
epoch 4501, loss 0.2829, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 5001, loss 0.2022, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 5501, loss 0.2635, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 6001, loss 0.2282, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 6501, loss 0.1734, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 7001, loss 0.2833, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 7501, loss 0.2222, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 8001, loss 0.2635, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 8501, loss 0.2041, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 9001, loss 0.2002, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 9501, loss 0.2429, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 10001, loss 0.1988, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 10501, loss 0.1777, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11001, loss 0.1884, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11501, loss 0.1912, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 12001, loss 0.2086, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 12501, loss 0.1851, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 13001, loss 0.2209, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 13501, loss 0.1636, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 14001, loss 0.2144, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 14501, loss 0.2267, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 15001, loss 0.1614, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 15501, loss 0.1134, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 16001, loss 0.1765, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 16501, loss 0.1637, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 17001, loss 0.1604, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 17501, loss 0.1816, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 18001, loss 0.1833, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 18501, loss 0.1615, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 19001, loss 0.2058, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 19501, loss 0.2310, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
running_time is 20.309168289
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.25
normal_0.5
./test_glass0/model_MLP_20000_0.25/record_1/MLP_20000_0.25_1
./test_glass0/result_MLP_20000_0.25_normal_0.5/record_1/
----------------------



the AUC is 0.5233990147783252

the Fscore is 0.35714285714285715

the precision is 0.35714285714285715

the recall is 0.35714285714285715

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.25/record_1/MLP_20000_0.25_2
----------------------



epoch 1, loss 0.6173, train acc 38.01%, f1 0.4904, precision 0.3355, recall 0.9107, auc 0.5162
epoch 501, loss 0.4542, train acc 77.19%, f1 0.7347, precision 0.5934, recall 0.9643, auc 0.8213
epoch 1001, loss 0.3769, train acc 81.87%, f1 0.7770, precision 0.6506, recall 0.9643, auc 0.8561
epoch 1501, loss 0.2906, train acc 83.63%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8737
epoch 2001, loss 0.2824, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 2501, loss 0.2615, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 3001, loss 0.2232, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 3501, loss 0.2089, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 4001, loss 0.1874, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 4501, loss 0.1822, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 5001, loss 0.1611, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 5501, loss 0.1400, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 6001, loss 0.1202, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 6501, loss 0.1733, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 7001, loss 0.1684, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 7501, loss 0.1383, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 8001, loss 0.1052, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 8501, loss 0.0919, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9001, loss 0.1688, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9501, loss 0.1298, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10001, loss 0.1508, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10501, loss 0.1507, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11001, loss 0.1704, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11501, loss 0.1346, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12001, loss 0.1222, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12501, loss 0.1163, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13001, loss 0.1758, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13501, loss 0.1599, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14001, loss 0.1513, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.1405, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.0984, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15501, loss 0.1585, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16001, loss 0.1601, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16501, loss 0.1276, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17001, loss 0.1527, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17501, loss 0.1292, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18001, loss 0.1419, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18501, loss 0.1010, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19001, loss 0.1639, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19501, loss 0.1662, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 20.101033153
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.25
normal_0.5
./test_glass0/model_MLP_20000_0.25/record_1/MLP_20000_0.25_2
./test_glass0/result_MLP_20000_0.25_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.25/record_1/MLP_20000_0.25_3
----------------------



epoch 1, loss 0.6392, train acc 40.94%, f1 0.5213, precision 0.3548, recall 0.9821, auc 0.5563
epoch 501, loss 0.4300, train acc 72.51%, f1 0.6887, precision 0.5474, recall 0.9286, auc 0.7773
epoch 1001, loss 0.3908, train acc 76.61%, f1 0.7183, precision 0.5930, recall 0.9107, auc 0.8032
epoch 1501, loss 0.3862, train acc 79.53%, f1 0.7518, precision 0.6235, recall 0.9464, auc 0.8341
epoch 2001, loss 0.2820, train acc 79.53%, f1 0.7518, precision 0.6235, recall 0.9464, auc 0.8341
epoch 2501, loss 0.3034, train acc 83.04%, f1 0.7852, precision 0.6709, recall 0.9464, auc 0.8602
epoch 3001, loss 0.3275, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 3501, loss 0.2815, train acc 84.80%, f1 0.8060, precision 0.6923, recall 0.9643, auc 0.8778
epoch 4001, loss 0.2247, train acc 85.38%, f1 0.8148, precision 0.6962, recall 0.9821, auc 0.8867
epoch 4501, loss 0.2830, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 5001, loss 0.1890, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 5501, loss 0.2347, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 6001, loss 0.1977, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 6501, loss 0.2199, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 7001, loss 0.1597, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 7501, loss 0.1749, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 8001, loss 0.1491, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 8501, loss 0.1412, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9001, loss 0.2099, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9501, loss 0.1964, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10001, loss 0.1790, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10501, loss 0.1372, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11001, loss 0.1272, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11501, loss 0.1545, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12001, loss 0.1311, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12501, loss 0.1419, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13001, loss 0.1178, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 13501, loss 0.0925, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14001, loss 0.1051, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 14501, loss 0.0994, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 15001, loss 0.1547, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 15501, loss 0.1297, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 16001, loss 0.1509, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 16501, loss 0.1123, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17001, loss 0.0959, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17501, loss 0.1212, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18001, loss 0.1026, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18501, loss 0.0988, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19001, loss 0.0917, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19501, loss 0.1277, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 20.00536974
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.25
normal_0.5
./test_glass0/model_MLP_20000_0.25/record_1/MLP_20000_0.25_3
./test_glass0/result_MLP_20000_0.25_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.25/record_1/MLP_20000_0.25_4
----------------------



epoch 1, loss 0.5294, train acc 64.91%, f1 0.0625, precision 0.2500, recall 0.0357, auc 0.4918
epoch 501, loss 0.4579, train acc 73.68%, f1 0.7059, precision 0.5567, recall 0.9643, auc 0.7952
epoch 1001, loss 0.3633, train acc 75.44%, f1 0.7162, precision 0.5761, recall 0.9464, auc 0.8036
epoch 1501, loss 0.3327, train acc 78.95%, f1 0.7465, precision 0.6163, recall 0.9464, auc 0.8297
epoch 2001, loss 0.3909, train acc 80.12%, f1 0.7571, precision 0.6310, recall 0.9464, auc 0.8384
epoch 2501, loss 0.3396, train acc 81.87%, f1 0.7770, precision 0.6506, recall 0.9643, auc 0.8561
epoch 3001, loss 0.2495, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 3501, loss 0.2381, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 4001, loss 0.2350, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 4501, loss 0.2184, train acc 85.96%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8865
epoch 5001, loss 0.2276, train acc 87.72%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9041
epoch 5501, loss 0.1785, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 6001, loss 0.2189, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 6501, loss 0.2067, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 7001, loss 0.2100, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 7501, loss 0.2932, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 8001, loss 0.1984, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 8501, loss 0.2421, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 9001, loss 0.1636, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 9501, loss 0.1915, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 10001, loss 0.2747, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 10501, loss 0.2796, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 11001, loss 0.3220, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 11501, loss 0.1872, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 12001, loss 0.1760, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 12501, loss 0.2025, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 13001, loss 0.2293, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 13501, loss 0.2483, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 14001, loss 0.2081, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 14501, loss 0.1767, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 15001, loss 0.1574, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 15501, loss 0.2552, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 16001, loss 0.2432, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 16501, loss 0.1871, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 17001, loss 0.1699, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 17501, loss 0.1811, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18001, loss 0.1896, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18501, loss 0.2058, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19001, loss 0.1711, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19501, loss 0.1404, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
running_time is 20.248496342
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.25
normal_0.5
./test_glass0/model_MLP_20000_0.25/record_1/MLP_20000_0.25_4
./test_glass0/result_MLP_20000_0.25_normal_0.5/record_1/
----------------------



the AUC is 0.6810344827586207

the Fscore is 0.56

the precision is 0.6363636363636364

the recall is 0.5

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.25/record_1/MLP_20000_0.25_5
----------------------



epoch 1, loss 0.6612, train acc 39.53%, f1 0.5185, precision 0.3500, recall 1.0000, auc 0.5517
epoch 501, loss 0.3117, train acc 75.58%, f1 0.7200, precision 0.5745, recall 0.9643, auc 0.8097
epoch 1001, loss 0.2763, train acc 79.07%, f1 0.7500, precision 0.6136, recall 0.9643, auc 0.8356
epoch 1501, loss 0.2983, train acc 79.65%, f1 0.7552, precision 0.6207, recall 0.9643, auc 0.8399
epoch 2001, loss 0.2937, train acc 80.23%, f1 0.7571, precision 0.6310, recall 0.9464, auc 0.8396
epoch 2501, loss 0.2945, train acc 81.40%, f1 0.7681, precision 0.6463, recall 0.9464, auc 0.8482
epoch 3001, loss 0.2900, train acc 83.14%, f1 0.7883, precision 0.6667, recall 0.9643, auc 0.8658
epoch 3501, loss 0.3383, train acc 83.14%, f1 0.7883, precision 0.6667, recall 0.9643, auc 0.8658
epoch 4001, loss 0.2647, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 4501, loss 0.3168, train acc 85.47%, f1 0.8148, precision 0.6962, recall 0.9821, auc 0.8876
epoch 5001, loss 0.3028, train acc 85.47%, f1 0.8148, precision 0.6962, recall 0.9821, auc 0.8876
epoch 5501, loss 0.3124, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 6001, loss 0.2503, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 6501, loss 0.2019, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 7001, loss 0.3067, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 7501, loss 0.2532, train acc 89.53%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9224
epoch 8001, loss 0.2280, train acc 90.70%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9310
epoch 8501, loss 0.2381, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 9001, loss 0.2036, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 9501, loss 0.1958, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 10001, loss 0.3102, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 10501, loss 0.1259, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 11001, loss 0.2357, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 11501, loss 0.2458, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 12001, loss 0.2244, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 12501, loss 0.1645, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 13001, loss 0.1783, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 13501, loss 0.1393, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 14001, loss 0.2027, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 14501, loss 0.1212, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 15001, loss 0.2006, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 15501, loss 0.2190, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 16001, loss 0.1596, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 16501, loss 0.2055, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 17001, loss 0.2470, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 17501, loss 0.1377, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 18001, loss 0.1424, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 18501, loss 0.1629, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 19001, loss 0.1931, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 19501, loss 0.1596, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
running_time is 20.057816914
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.25
normal_0.5
./test_glass0/model_MLP_20000_0.25/record_1/MLP_20000_0.25_5
./test_glass0/result_MLP_20000_0.25_normal_0.5/record_1/
----------------------



the AUC is 0.7857142857142857

the Fscore is 0.7000000000000001

the precision is 0.5384615384615384

the recall is 1.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.26/record_1/MLP_20000_0.26_1
----------------------



epoch 1, loss 0.6202, train acc 32.75%, f1 0.4934, precision 0.3275, recall 1.0000, auc 0.5000
epoch 501, loss 0.4107, train acc 74.27%, f1 0.7105, precision 0.5625, recall 0.9643, auc 0.7995
epoch 1001, loss 0.2875, train acc 77.78%, f1 0.7397, precision 0.6000, recall 0.9643, auc 0.8256
epoch 1501, loss 0.2995, train acc 78.95%, f1 0.7500, precision 0.6136, recall 0.9643, auc 0.8343
epoch 2001, loss 0.3198, train acc 81.87%, f1 0.7770, precision 0.6506, recall 0.9643, auc 0.8561
epoch 2501, loss 0.3571, train acc 80.70%, f1 0.7660, precision 0.6353, recall 0.9643, auc 0.8474
epoch 3001, loss 0.3321, train acc 84.21%, f1 0.8000, precision 0.6835, recall 0.9643, auc 0.8734
epoch 3501, loss 0.3574, train acc 84.21%, f1 0.8000, precision 0.6835, recall 0.9643, auc 0.8734
epoch 4001, loss 0.3740, train acc 85.38%, f1 0.8148, precision 0.6962, recall 0.9821, auc 0.8867
epoch 4501, loss 0.2254, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 5001, loss 0.3317, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 5501, loss 0.2606, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 6001, loss 0.2819, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 6501, loss 0.2570, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 7001, loss 0.2952, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 7501, loss 0.2519, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 8001, loss 0.2794, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 8501, loss 0.2687, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 9001, loss 0.2337, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 9501, loss 0.2965, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 10001, loss 0.2177, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 10501, loss 0.2664, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 11001, loss 0.2173, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 11501, loss 0.2264, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 12001, loss 0.2126, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 12501, loss 0.2463, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 13001, loss 0.1546, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 13501, loss 0.1567, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 14001, loss 0.1922, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 14501, loss 0.2542, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 15001, loss 0.2215, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 15501, loss 0.1201, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 16001, loss 0.2217, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 16501, loss 0.1602, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 17001, loss 0.2683, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 17501, loss 0.1443, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 18001, loss 0.1690, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 18501, loss 0.1846, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 19001, loss 0.1615, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 19501, loss 0.2216, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
running_time is 20.273013159999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.26
normal_0.5
./test_glass0/model_MLP_20000_0.26/record_1/MLP_20000_0.26_1
./test_glass0/result_MLP_20000_0.26_normal_0.5/record_1/
----------------------



the AUC is 0.6896551724137931

the Fscore is 0.6086956521739131

the precision is 0.4375

the recall is 1.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.26/record_1/MLP_20000_0.26_2
----------------------



epoch 1, loss 0.6434, train acc 39.18%, f1 0.5185, precision 0.3500, recall 1.0000, auc 0.5478
epoch 501, loss 0.3824, train acc 74.85%, f1 0.7152, precision 0.5684, recall 0.9643, auc 0.8039
epoch 1001, loss 0.2643, train acc 83.04%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8693
epoch 1501, loss 0.1699, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 2001, loss 0.3382, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 2501, loss 0.2440, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 3001, loss 0.2605, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 3501, loss 0.2406, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 4001, loss 0.1857, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 4501, loss 0.2317, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 5001, loss 0.1799, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 5501, loss 0.1290, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 6001, loss 0.2100, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 6501, loss 0.1513, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 7001, loss 0.2088, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 7501, loss 0.1321, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 8001, loss 0.1271, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 8501, loss 0.1363, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9001, loss 0.1357, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9501, loss 0.1610, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10001, loss 0.1148, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10501, loss 0.1445, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11001, loss 0.1497, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11501, loss 0.1519, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12001, loss 0.1573, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12501, loss 0.1449, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13001, loss 0.1485, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13501, loss 0.1266, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14001, loss 0.1483, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14501, loss 0.1550, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15001, loss 0.0984, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15501, loss 0.1740, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16001, loss 0.1031, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16501, loss 0.1097, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.1428, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.1292, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.1099, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.0801, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.1018, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19501, loss 0.1271, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 20.134559598000003
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.26
normal_0.5
./test_glass0/model_MLP_20000_0.26/record_1/MLP_20000_0.26_2
./test_glass0/result_MLP_20000_0.26_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.26/record_1/MLP_20000_0.26_3
----------------------



epoch 1, loss 0.7124, train acc 32.75%, f1 0.4934, precision 0.3275, recall 1.0000, auc 0.5000
epoch 501, loss 0.3543, train acc 70.76%, f1 0.6795, precision 0.5300, recall 0.9464, auc 0.7689
epoch 1001, loss 0.4663, train acc 73.68%, f1 0.6939, precision 0.5604, recall 0.9107, auc 0.7814
epoch 1501, loss 0.3694, train acc 76.61%, f1 0.7222, precision 0.5909, recall 0.9286, auc 0.8078
epoch 2001, loss 0.3850, train acc 77.78%, f1 0.7361, precision 0.6023, recall 0.9464, auc 0.8210
epoch 2501, loss 0.3638, train acc 79.53%, f1 0.7482, precision 0.6265, recall 0.9286, auc 0.8295
epoch 3001, loss 0.3336, train acc 80.70%, f1 0.7660, precision 0.6353, recall 0.9643, auc 0.8474
epoch 3501, loss 0.2791, train acc 82.46%, f1 0.7826, precision 0.6585, recall 0.9643, auc 0.8604
epoch 4001, loss 0.3480, train acc 82.46%, f1 0.7857, precision 0.6548, recall 0.9821, auc 0.8650
epoch 4501, loss 0.2414, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 5001, loss 0.2441, train acc 84.21%, f1 0.8000, precision 0.6835, recall 0.9643, auc 0.8734
epoch 5501, loss 0.3424, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 6001, loss 0.2841, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 6501, loss 0.2737, train acc 85.96%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8911
epoch 7001, loss 0.2369, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 7501, loss 0.2476, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 8001, loss 0.2292, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 8501, loss 0.2110, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 9001, loss 0.2605, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 9501, loss 0.1573, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 10001, loss 0.1646, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 10501, loss 0.1986, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 11001, loss 0.1897, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 11501, loss 0.1964, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 12001, loss 0.1936, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 12501, loss 0.2002, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 13001, loss 0.2020, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 13501, loss 0.1601, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 14001, loss 0.1986, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 14501, loss 0.2122, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 15001, loss 0.2433, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 15501, loss 0.1924, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 16001, loss 0.2000, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 16501, loss 0.1957, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 17001, loss 0.1458, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 17501, loss 0.2094, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 18001, loss 0.2406, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 18501, loss 0.1425, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 19001, loss 0.1250, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 19501, loss 0.1455, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
running_time is 20.06271096
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.26
normal_0.5
./test_glass0/model_MLP_20000_0.26/record_1/MLP_20000_0.26_3
./test_glass0/result_MLP_20000_0.26_normal_0.5/record_1/
----------------------



the AUC is 0.5862068965517242

the Fscore is 0.5384615384615384

the precision is 0.3684210526315789

the recall is 1.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.26/record_1/MLP_20000_0.26_4
----------------------



epoch 1, loss 0.6666, train acc 36.26%, f1 0.5023, precision 0.3374, recall 0.9821, auc 0.5215
epoch 501, loss 0.3348, train acc 73.10%, f1 0.7013, precision 0.5510, recall 0.9643, auc 0.7908
epoch 1001, loss 0.3779, train acc 77.19%, f1 0.7310, precision 0.5955, recall 0.9464, auc 0.8167
epoch 1501, loss 0.3629, train acc 78.95%, f1 0.7465, precision 0.6163, recall 0.9464, auc 0.8297
epoch 2001, loss 0.4019, train acc 80.70%, f1 0.7626, precision 0.6386, recall 0.9464, auc 0.8428
epoch 2501, loss 0.3080, train acc 83.04%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8693
epoch 3001, loss 0.3836, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 3501, loss 0.2891, train acc 85.96%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8865
epoch 4001, loss 0.3170, train acc 85.96%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8865
epoch 4501, loss 0.2553, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 5001, loss 0.2421, train acc 87.13%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.8998
epoch 5501, loss 0.2458, train acc 87.72%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9041
epoch 6001, loss 0.2883, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 6501, loss 0.2384, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 7001, loss 0.2227, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 7501, loss 0.3573, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 8001, loss 0.2571, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 8501, loss 0.2426, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 9001, loss 0.1977, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 9501, loss 0.1857, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 10001, loss 0.2543, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 10501, loss 0.2670, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 11001, loss 0.1956, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 11501, loss 0.2637, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 12001, loss 0.2422, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 12501, loss 0.2491, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 13001, loss 0.1795, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 13501, loss 0.3131, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 14001, loss 0.2902, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 14501, loss 0.1914, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 15001, loss 0.1861, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 15501, loss 0.2156, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 16001, loss 0.1557, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 16501, loss 0.1598, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 17001, loss 0.1914, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 17501, loss 0.2268, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 18001, loss 0.1604, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 18501, loss 0.1702, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 19001, loss 0.1720, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 19501, loss 0.1946, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
running_time is 19.973224873
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.26
normal_0.5
./test_glass0/model_MLP_20000_0.26/record_1/MLP_20000_0.26_4
./test_glass0/result_MLP_20000_0.26_normal_0.5/record_1/
----------------------



the AUC is 0.5554187192118226

the Fscore is 0.3

the precision is 0.5

the recall is 0.21428571428571427

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.26/record_1/MLP_20000_0.26_5
----------------------



epoch 1, loss 0.5049, train acc 66.86%, f1 0.0952, precision 0.4286, recall 0.0536, auc 0.5095
epoch 501, loss 0.4600, train acc 72.67%, f1 0.6968, precision 0.5455, recall 0.9643, auc 0.7882
epoch 1001, loss 0.3861, train acc 78.49%, f1 0.7448, precision 0.6067, recall 0.9643, auc 0.8313
epoch 1501, loss 0.3870, train acc 80.23%, f1 0.7606, precision 0.6279, recall 0.9643, auc 0.8442
epoch 2001, loss 0.3926, train acc 82.56%, f1 0.7857, precision 0.6548, recall 0.9821, auc 0.8661
epoch 2501, loss 0.3843, train acc 83.14%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8704
epoch 3001, loss 0.2376, train acc 84.30%, f1 0.8029, precision 0.6790, recall 0.9821, auc 0.8790
epoch 3501, loss 0.2661, train acc 84.30%, f1 0.8029, precision 0.6790, recall 0.9821, auc 0.8790
epoch 4001, loss 0.2474, train acc 83.72%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8747
epoch 4501, loss 0.2685, train acc 84.88%, f1 0.8088, precision 0.6875, recall 0.9821, auc 0.8833
epoch 5001, loss 0.2415, train acc 86.05%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8919
epoch 5501, loss 0.3465, train acc 86.05%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8873
epoch 6001, loss 0.1770, train acc 87.79%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9049
epoch 6501, loss 0.2402, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 7001, loss 0.2380, train acc 87.79%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9049
epoch 7501, loss 0.2439, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 8001, loss 0.2478, train acc 91.28%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9353
epoch 8501, loss 0.1908, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 9001, loss 0.2158, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 9501, loss 0.1562, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 10001, loss 0.1481, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 10501, loss 0.1826, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 11001, loss 0.1660, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 11501, loss 0.1539, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 12001, loss 0.1593, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 12501, loss 0.1578, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 13001, loss 0.1824, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 13501, loss 0.2251, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 14001, loss 0.1223, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 14501, loss 0.1395, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 15001, loss 0.2085, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 15501, loss 0.1559, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 16001, loss 0.1294, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 16501, loss 0.0908, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 17001, loss 0.1386, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 17501, loss 0.1276, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 18001, loss 0.1331, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 18501, loss 0.0925, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 19001, loss 0.0878, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 19501, loss 0.1838, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
running_time is 20.156403327
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.26
normal_0.5
./test_glass0/model_MLP_20000_0.26/record_1/MLP_20000_0.26_5
./test_glass0/result_MLP_20000_0.26_normal_0.5/record_1/
----------------------



the AUC is 0.5535714285714285

the Fscore is 0.23529411764705882

the precision is 0.6666666666666666

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.27/record_1/MLP_20000_0.27_1
----------------------



epoch 1, loss 0.5998, train acc 40.35%, f1 0.5234, precision 0.3544, recall 1.0000, auc 0.5565
epoch 501, loss 0.3431, train acc 76.02%, f1 0.7248, precision 0.5806, recall 0.9643, auc 0.8126
epoch 1001, loss 0.3233, train acc 78.95%, f1 0.7500, precision 0.6136, recall 0.9643, auc 0.8343
epoch 1501, loss 0.3432, train acc 81.29%, f1 0.7714, precision 0.6429, recall 0.9643, auc 0.8517
epoch 2001, loss 0.3034, train acc 82.46%, f1 0.7826, precision 0.6585, recall 0.9643, auc 0.8604
epoch 2501, loss 0.3170, train acc 82.46%, f1 0.7826, precision 0.6585, recall 0.9643, auc 0.8604
epoch 3001, loss 0.3636, train acc 84.21%, f1 0.8000, precision 0.6835, recall 0.9643, auc 0.8734
epoch 3501, loss 0.2881, train acc 84.80%, f1 0.8088, precision 0.6875, recall 0.9821, auc 0.8824
epoch 4001, loss 0.3870, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 4501, loss 0.3009, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 5001, loss 0.2927, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 5501, loss 0.2480, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 6001, loss 0.2560, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 6501, loss 0.2538, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 7001, loss 0.2861, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 7501, loss 0.2219, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 8001, loss 0.2478, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 8501, loss 0.1754, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 9001, loss 0.2260, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 9501, loss 0.2229, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 10001, loss 0.2506, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10501, loss 0.1897, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11001, loss 0.1519, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 11501, loss 0.1707, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 12001, loss 0.2124, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 12501, loss 0.2283, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 13001, loss 0.1813, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 13501, loss 0.2451, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 14001, loss 0.2076, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 14501, loss 0.1472, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15001, loss 0.1701, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15501, loss 0.1512, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16001, loss 0.2270, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16501, loss 0.1381, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17001, loss 0.1706, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17501, loss 0.1298, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18001, loss 0.1968, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18501, loss 0.1755, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19001, loss 0.1533, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19501, loss 0.1740, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
running_time is 20.051568977
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.27
normal_0.5
./test_glass0/model_MLP_20000_0.27/record_1/MLP_20000_0.27_1
./test_glass0/result_MLP_20000_0.27_normal_0.5/record_1/
----------------------



the AUC is 0.43103448275862066

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.27/record_1/MLP_20000_0.27_2
----------------------



epoch 1, loss 0.7447, train acc 47.37%, f1 0.5500, precision 0.3819, recall 0.9821, auc 0.6041
epoch 501, loss 0.3413, train acc 74.27%, f1 0.7143, precision 0.5612, recall 0.9821, auc 0.8041
epoch 1001, loss 0.4061, train acc 81.29%, f1 0.7714, precision 0.6429, recall 0.9643, auc 0.8517
epoch 1501, loss 0.3751, train acc 81.29%, f1 0.7746, precision 0.6395, recall 0.9821, auc 0.8563
epoch 2001, loss 0.2465, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 2501, loss 0.2537, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 3001, loss 0.2500, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 3501, loss 0.1941, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 4001, loss 0.2778, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 4501, loss 0.1929, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 5001, loss 0.2128, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 5501, loss 0.1394, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 6001, loss 0.2065, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 6501, loss 0.1784, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 7001, loss 0.1642, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 7501, loss 0.1253, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 8001, loss 0.1872, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 8501, loss 0.1526, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9001, loss 0.1705, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 9501, loss 0.1864, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10001, loss 0.1449, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10501, loss 0.1336, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11001, loss 0.1147, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11501, loss 0.1387, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12001, loss 0.1760, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12501, loss 0.1625, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13001, loss 0.1720, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13501, loss 0.1442, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14001, loss 0.1069, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14501, loss 0.1828, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.1836, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15501, loss 0.1333, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.1412, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.1139, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17001, loss 0.1342, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.1058, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.1522, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.1472, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.1037, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.1595, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 20.156740156999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.27
normal_0.5
./test_glass0/model_MLP_20000_0.27/record_1/MLP_20000_0.27_2
./test_glass0/result_MLP_20000_0.27_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.27/record_1/MLP_20000_0.27_3
----------------------



epoch 1, loss 0.6963, train acc 45.61%, f1 0.5463, precision 0.3758, recall 1.0000, auc 0.5957
epoch 501, loss 0.3649, train acc 71.35%, f1 0.6879, precision 0.5347, recall 0.9643, auc 0.7778
epoch 1001, loss 0.4350, train acc 75.44%, f1 0.7162, precision 0.5761, recall 0.9464, auc 0.8036
epoch 1501, loss 0.3672, train acc 77.19%, f1 0.7273, precision 0.5977, recall 0.9286, auc 0.8121
epoch 2001, loss 0.3000, train acc 77.78%, f1 0.7324, precision 0.6047, recall 0.9286, auc 0.8165
epoch 2501, loss 0.3626, train acc 79.53%, f1 0.7552, precision 0.6207, recall 0.9643, auc 0.8387
epoch 3001, loss 0.2993, train acc 82.46%, f1 0.7857, precision 0.6548, recall 0.9821, auc 0.8650
epoch 3501, loss 0.2221, train acc 82.46%, f1 0.7857, precision 0.6548, recall 0.9821, auc 0.8650
epoch 4001, loss 0.2437, train acc 83.63%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8737
epoch 4501, loss 0.3047, train acc 84.21%, f1 0.8029, precision 0.6790, recall 0.9821, auc 0.8780
epoch 5001, loss 0.2445, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 5501, loss 0.2710, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 6001, loss 0.2014, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 6501, loss 0.2850, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 7001, loss 0.1698, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 7501, loss 0.2573, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 8001, loss 0.2286, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 8501, loss 0.2153, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 9001, loss 0.2314, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 9501, loss 0.2236, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 10001, loss 0.1841, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 10501, loss 0.2331, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 11001, loss 0.1416, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 11501, loss 0.2090, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12001, loss 0.1702, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12501, loss 0.1424, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13001, loss 0.1751, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 13501, loss 0.2334, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14001, loss 0.1547, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14501, loss 0.2504, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15001, loss 0.1506, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15501, loss 0.1338, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16001, loss 0.1877, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16501, loss 0.1572, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17001, loss 0.1522, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17501, loss 0.1863, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 18001, loss 0.1256, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.1611, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19001, loss 0.1691, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19501, loss 0.1966, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
running_time is 20.137773175
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.27
normal_0.5
./test_glass0/model_MLP_20000_0.27/record_1/MLP_20000_0.27_3
./test_glass0/result_MLP_20000_0.27_normal_0.5/record_1/
----------------------



the AUC is 0.6970443349753696

the Fscore is 0.5714285714285714

the precision is 0.8571428571428571

the recall is 0.42857142857142855

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.27/record_1/MLP_20000_0.27_4
----------------------



epoch 1, loss 0.5273, train acc 60.82%, f1 0.1067, precision 0.2105, recall 0.0714, auc 0.4705
epoch 501, loss 0.4718, train acc 71.35%, f1 0.6879, precision 0.5347, recall 0.9643, auc 0.7778
epoch 1001, loss 0.2731, train acc 74.85%, f1 0.7114, precision 0.5699, recall 0.9464, auc 0.7993
epoch 1501, loss 0.3286, train acc 77.78%, f1 0.7361, precision 0.6023, recall 0.9464, auc 0.8210
epoch 2001, loss 0.3695, train acc 78.95%, f1 0.7465, precision 0.6163, recall 0.9464, auc 0.8297
epoch 2501, loss 0.3449, train acc 82.46%, f1 0.7857, precision 0.6548, recall 0.9821, auc 0.8650
epoch 3001, loss 0.2886, train acc 82.46%, f1 0.7857, precision 0.6548, recall 0.9821, auc 0.8650
epoch 3501, loss 0.3308, train acc 85.96%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8911
epoch 4001, loss 0.3101, train acc 84.80%, f1 0.8088, precision 0.6875, recall 0.9821, auc 0.8824
epoch 4501, loss 0.2515, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 5001, loss 0.3367, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 5501, loss 0.2706, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 6001, loss 0.2740, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 6501, loss 0.3014, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 7001, loss 0.1169, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 7501, loss 0.2674, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 8001, loss 0.2495, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 8501, loss 0.2552, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 9001, loss 0.1817, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 9501, loss 0.2698, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 10001, loss 0.2828, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 10501, loss 0.1722, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 11001, loss 0.2976, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 11501, loss 0.2146, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 12001, loss 0.2699, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 12501, loss 0.2496, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 13001, loss 0.2364, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 13501, loss 0.2250, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 14001, loss 0.1804, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 14501, loss 0.2704, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 15001, loss 0.2391, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 15501, loss 0.2566, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 16001, loss 0.2023, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 16501, loss 0.2649, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 17001, loss 0.2646, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 17501, loss 0.2150, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 18001, loss 0.2411, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 18501, loss 0.1816, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 19001, loss 0.2494, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 19501, loss 0.2317, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
running_time is 20.065744864
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.27
normal_0.5
./test_glass0/model_MLP_20000_0.27/record_1/MLP_20000_0.27_4
./test_glass0/result_MLP_20000_0.27_normal_0.5/record_1/
----------------------



the AUC is 0.791871921182266

the Fscore is 0.7027027027027025

the precision is 0.5652173913043478

the recall is 0.9285714285714286

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.27/record_1/MLP_20000_0.27_5
----------------------



epoch 1, loss 0.6481, train acc 37.21%, f1 0.5091, precision 0.3415, recall 1.0000, auc 0.5345
epoch 501, loss 0.5041, train acc 71.51%, f1 0.6957, precision 0.5333, recall 1.0000, auc 0.7888
epoch 1001, loss 0.2902, train acc 79.07%, f1 0.7500, precision 0.6136, recall 0.9643, auc 0.8356
epoch 1501, loss 0.3894, train acc 79.65%, f1 0.7552, precision 0.6207, recall 0.9643, auc 0.8399
epoch 2001, loss 0.3260, train acc 83.72%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8793
epoch 2501, loss 0.2241, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 3001, loss 0.3061, train acc 85.47%, f1 0.8148, precision 0.6962, recall 0.9821, auc 0.8876
epoch 3501, loss 0.2708, train acc 86.63%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8962
epoch 4001, loss 0.3233, train acc 87.21%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.9006
epoch 4501, loss 0.2466, train acc 87.21%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.9006
epoch 5001, loss 0.2500, train acc 87.21%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.9006
epoch 5501, loss 0.2401, train acc 87.79%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9049
epoch 6001, loss 0.1827, train acc 88.37%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9092
epoch 6501, loss 0.3433, train acc 88.37%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9092
epoch 7001, loss 0.2881, train acc 89.53%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9224
epoch 7501, loss 0.3394, train acc 90.70%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9310
epoch 8001, loss 0.1939, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 8501, loss 0.2850, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 9001, loss 0.1978, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 9501, loss 0.2208, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 10001, loss 0.1507, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 10501, loss 0.1838, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 11001, loss 0.2229, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 11501, loss 0.1657, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 12001, loss 0.2279, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 12501, loss 0.1805, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 13001, loss 0.2386, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 13501, loss 0.1928, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 14001, loss 0.1655, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 14501, loss 0.1505, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 15001, loss 0.2391, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 15501, loss 0.2105, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 16001, loss 0.1286, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 16501, loss 0.1494, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 17001, loss 0.2408, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 17501, loss 0.1649, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 18001, loss 0.2169, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 18501, loss 0.1623, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 19001, loss 0.1311, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 19501, loss 0.1118, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
running_time is 20.28099196
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.27
normal_0.5
./test_glass0/model_MLP_20000_0.27/record_1/MLP_20000_0.27_5
./test_glass0/result_MLP_20000_0.27_normal_0.5/record_1/
----------------------



the AUC is 0.48214285714285715

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.28/record_1/MLP_20000_0.28_1
----------------------



epoch 1, loss 0.5775, train acc 48.54%, f1 0.5600, precision 0.3889, recall 1.0000, auc 0.6174
epoch 501, loss 0.4038, train acc 76.61%, f1 0.7297, precision 0.5870, recall 0.9643, auc 0.8169
epoch 1001, loss 0.3128, train acc 80.12%, f1 0.7606, precision 0.6279, recall 0.9643, auc 0.8430
epoch 1501, loss 0.3543, train acc 79.53%, f1 0.7552, precision 0.6207, recall 0.9643, auc 0.8387
epoch 2001, loss 0.3186, train acc 81.87%, f1 0.7770, precision 0.6506, recall 0.9643, auc 0.8561
epoch 2501, loss 0.2590, train acc 84.21%, f1 0.8029, precision 0.6790, recall 0.9821, auc 0.8780
epoch 3001, loss 0.3070, train acc 85.96%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8911
epoch 3501, loss 0.2554, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 4001, loss 0.2599, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 4501, loss 0.2839, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 5001, loss 0.1730, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 5501, loss 0.2615, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 6001, loss 0.1667, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 6501, loss 0.2082, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 7001, loss 0.2071, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 7501, loss 0.1670, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8001, loss 0.2368, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8501, loss 0.2996, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9001, loss 0.2391, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9501, loss 0.1804, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 10001, loss 0.1757, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 10501, loss 0.2417, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 11001, loss 0.1354, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 11501, loss 0.1997, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 12001, loss 0.1788, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 12501, loss 0.1769, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 13001, loss 0.1694, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 13501, loss 0.1544, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 14001, loss 0.2280, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 14501, loss 0.1740, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 15001, loss 0.2648, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 15501, loss 0.1498, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 16001, loss 0.2001, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 16501, loss 0.1571, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 17001, loss 0.1131, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 17501, loss 0.1755, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 18001, loss 0.1380, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 18501, loss 0.2164, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 19001, loss 0.1435, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 19501, loss 0.1697, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
running_time is 20.145319189000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.28
normal_0.5
./test_glass0/model_MLP_20000_0.28/record_1/MLP_20000_0.28_1
./test_glass0/result_MLP_20000_0.28_normal_0.5/record_1/
----------------------



the AUC is 0.46551724137931033

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.28/record_1/MLP_20000_0.28_2
----------------------



epoch 1, loss 0.5268, train acc 67.84%, f1 0.2857, precision 0.5238, recall 0.1964, auc 0.5547
epoch 501, loss 0.4046, train acc 76.02%, f1 0.7285, precision 0.5789, recall 0.9821, auc 0.8172
epoch 1001, loss 0.3200, train acc 80.12%, f1 0.7639, precision 0.6250, recall 0.9821, auc 0.8476
epoch 1501, loss 0.3158, train acc 83.04%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8693
epoch 2001, loss 0.3411, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 2501, loss 0.3789, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 3001, loss 0.2644, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 3501, loss 0.2641, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 4001, loss 0.3000, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 4501, loss 0.2242, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 5001, loss 0.2841, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 5501, loss 0.1742, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 6001, loss 0.2368, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 6501, loss 0.2467, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 7001, loss 0.2122, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 7501, loss 0.2638, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 8001, loss 0.1726, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 8501, loss 0.2242, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9001, loss 0.2167, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9501, loss 0.1561, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10001, loss 0.1757, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10501, loss 0.2426, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 11001, loss 0.1872, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 11501, loss 0.1943, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 12001, loss 0.1421, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 12501, loss 0.1728, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13001, loss 0.1376, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13501, loss 0.1482, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14001, loss 0.1363, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14501, loss 0.1828, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15001, loss 0.1848, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 15501, loss 0.1381, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 16001, loss 0.1699, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 16501, loss 0.1513, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17001, loss 0.1528, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17501, loss 0.1535, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 18001, loss 0.1495, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.2071, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.1896, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.1254, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 20.113954438
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.28
normal_0.5
./test_glass0/model_MLP_20000_0.28/record_1/MLP_20000_0.28_2
./test_glass0/result_MLP_20000_0.28_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.28/record_1/MLP_20000_0.28_3
----------------------



epoch 1, loss 0.6028, train acc 46.78%, f1 0.5517, precision 0.3810, recall 1.0000, auc 0.6043
epoch 501, loss 0.3803, train acc 72.51%, f1 0.6928, precision 0.5464, recall 0.9464, auc 0.7819
epoch 1001, loss 0.3838, train acc 75.44%, f1 0.7162, precision 0.5761, recall 0.9464, auc 0.8036
epoch 1501, loss 0.3552, train acc 77.19%, f1 0.7310, precision 0.5955, recall 0.9464, auc 0.8167
epoch 2001, loss 0.3327, train acc 77.19%, f1 0.7273, precision 0.5977, recall 0.9286, auc 0.8121
epoch 2501, loss 0.3239, train acc 80.70%, f1 0.7626, precision 0.6386, recall 0.9464, auc 0.8428
epoch 3001, loss 0.2257, train acc 83.04%, f1 0.7883, precision 0.6667, recall 0.9643, auc 0.8648
epoch 3501, loss 0.3043, train acc 83.04%, f1 0.7883, precision 0.6667, recall 0.9643, auc 0.8648
epoch 4001, loss 0.2957, train acc 84.21%, f1 0.8029, precision 0.6790, recall 0.9821, auc 0.8780
epoch 4501, loss 0.2881, train acc 84.21%, f1 0.8029, precision 0.6790, recall 0.9821, auc 0.8780
epoch 5001, loss 0.2826, train acc 84.80%, f1 0.8088, precision 0.6875, recall 0.9821, auc 0.8824
epoch 5501, loss 0.2739, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 6001, loss 0.3155, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 6501, loss 0.1674, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 7001, loss 0.2460, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 7501, loss 0.1939, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 8001, loss 0.1766, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8501, loss 0.2378, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 9001, loss 0.1729, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 9501, loss 0.2076, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 10001, loss 0.1891, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 10501, loss 0.1898, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 11001, loss 0.1991, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 11501, loss 0.2108, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 12001, loss 0.1485, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12501, loss 0.1804, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 13001, loss 0.1438, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13501, loss 0.1992, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14001, loss 0.1429, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14501, loss 0.1677, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15001, loss 0.1339, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15501, loss 0.1659, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16001, loss 0.1349, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16501, loss 0.1096, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17001, loss 0.1226, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17501, loss 0.1444, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18001, loss 0.1598, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.1200, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 19001, loss 0.1498, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 19501, loss 0.1311, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 19.969445545
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.28
normal_0.5
./test_glass0/model_MLP_20000_0.28/record_1/MLP_20000_0.28_3
./test_glass0/result_MLP_20000_0.28_normal_0.5/record_1/
----------------------



the AUC is 0.625615763546798

the Fscore is 0.4210526315789473

the precision is 0.8

the recall is 0.2857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.28/record_1/MLP_20000_0.28_4
----------------------



epoch 1, loss 0.6788, train acc 44.44%, f1 0.5411, precision 0.3709, recall 1.0000, auc 0.5870
epoch 501, loss 0.4107, train acc 72.51%, f1 0.6968, precision 0.5455, recall 0.9643, auc 0.7865
epoch 1001, loss 0.3830, train acc 74.85%, f1 0.7152, precision 0.5684, recall 0.9643, auc 0.8039
epoch 1501, loss 0.3868, train acc 77.19%, f1 0.7310, precision 0.5955, recall 0.9464, auc 0.8167
epoch 2001, loss 0.3309, train acc 79.53%, f1 0.7552, precision 0.6207, recall 0.9643, auc 0.8387
epoch 2501, loss 0.2960, train acc 80.12%, f1 0.7639, precision 0.6250, recall 0.9821, auc 0.8476
epoch 3001, loss 0.3374, train acc 83.63%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8737
epoch 3501, loss 0.3502, train acc 83.63%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8737
epoch 4001, loss 0.3144, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 4501, loss 0.2129, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 5001, loss 0.2821, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 5501, loss 0.2787, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 6001, loss 0.1951, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 6501, loss 0.1910, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 7001, loss 0.2621, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 7501, loss 0.2666, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 8001, loss 0.1922, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 8501, loss 0.2665, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 9001, loss 0.3016, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 9501, loss 0.2543, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 10001, loss 0.2571, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 10501, loss 0.2231, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 11001, loss 0.2559, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 11501, loss 0.3079, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 12001, loss 0.1895, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 12501, loss 0.2905, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 13001, loss 0.1916, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 13501, loss 0.1873, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 14001, loss 0.2599, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 14501, loss 0.2342, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 15001, loss 0.2208, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 15501, loss 0.2039, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 16001, loss 0.2751, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 16501, loss 0.1858, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 17001, loss 0.2586, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 17501, loss 0.2436, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 18001, loss 0.2145, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 18501, loss 0.2036, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 19001, loss 0.2211, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 19501, loss 0.2421, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
running_time is 20.119469656
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.28
normal_0.5
./test_glass0/model_MLP_20000_0.28/record_1/MLP_20000_0.28_4
./test_glass0/result_MLP_20000_0.28_normal_0.5/record_1/
----------------------



the AUC is 0.6268472906403941

the Fscore is 0.45454545454545453

the precision is 0.625

the recall is 0.35714285714285715

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.28/record_1/MLP_20000_0.28_5
----------------------



epoch 1, loss 0.7042, train acc 54.65%, f1 0.5806, precision 0.4154, recall 0.9643, auc 0.6546
epoch 501, loss 0.3695, train acc 70.93%, f1 0.6875, precision 0.5288, recall 0.9821, auc 0.7799
epoch 1001, loss 0.3518, train acc 79.07%, f1 0.7500, precision 0.6136, recall 0.9643, auc 0.8356
epoch 1501, loss 0.3106, train acc 80.81%, f1 0.7660, precision 0.6353, recall 0.9643, auc 0.8485
epoch 2001, loss 0.3329, train acc 82.56%, f1 0.7887, precision 0.6512, recall 1.0000, auc 0.8707
epoch 2501, loss 0.2561, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 3001, loss 0.2810, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 3501, loss 0.2906, train acc 84.30%, f1 0.8029, precision 0.6790, recall 0.9821, auc 0.8790
epoch 4001, loss 0.2746, train acc 84.88%, f1 0.8088, precision 0.6875, recall 0.9821, auc 0.8833
epoch 4501, loss 0.4232, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 5001, loss 0.3239, train acc 85.47%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8830
epoch 5501, loss 0.2971, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 6001, loss 0.1999, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 6501, loss 0.3462, train acc 89.53%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9224
epoch 7001, loss 0.2029, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 7501, loss 0.1823, train acc 90.12%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9267
epoch 8001, loss 0.1590, train acc 90.12%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9267
epoch 8501, loss 0.2228, train acc 91.28%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9353
epoch 9001, loss 0.1664, train acc 91.28%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9353
epoch 9501, loss 0.2156, train acc 91.28%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9353
epoch 10001, loss 0.2272, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 10501, loss 0.1563, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 11001, loss 0.1811, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 11501, loss 0.2586, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 12001, loss 0.2371, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 12501, loss 0.1599, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 13001, loss 0.1751, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 13501, loss 0.2518, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 14001, loss 0.2017, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 14501, loss 0.1422, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 15001, loss 0.2119, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 15501, loss 0.1728, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 16001, loss 0.2470, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 16501, loss 0.1567, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 17001, loss 0.1706, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 17501, loss 0.1788, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 18001, loss 0.1886, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 18501, loss 0.1818, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 19001, loss 0.1647, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 19501, loss 0.1910, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
running_time is 20.063567565
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.28
normal_0.5
./test_glass0/model_MLP_20000_0.28/record_1/MLP_20000_0.28_5
./test_glass0/result_MLP_20000_0.28_normal_0.5/record_1/
----------------------



the AUC is 0.48214285714285715

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.29/record_1/MLP_20000_0.29_1
----------------------



epoch 1, loss 0.6066, train acc 56.73%, f1 0.5978, precision 0.4297, recall 0.9821, auc 0.6737
epoch 501, loss 0.3741, train acc 74.85%, f1 0.7190, precision 0.5670, recall 0.9821, auc 0.8085
epoch 1001, loss 0.2893, train acc 78.36%, f1 0.7448, precision 0.6067, recall 0.9643, auc 0.8300
epoch 1501, loss 0.3860, train acc 81.87%, f1 0.7770, precision 0.6506, recall 0.9643, auc 0.8561
epoch 2001, loss 0.3001, train acc 83.04%, f1 0.7883, precision 0.6667, recall 0.9643, auc 0.8648
epoch 2501, loss 0.2433, train acc 85.38%, f1 0.8148, precision 0.6962, recall 0.9821, auc 0.8867
epoch 3001, loss 0.3092, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 3501, loss 0.3245, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 4001, loss 0.2397, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 4501, loss 0.3228, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 5001, loss 0.2978, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 5501, loss 0.2542, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 6001, loss 0.2961, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 6501, loss 0.2499, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 7001, loss 0.2819, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 7501, loss 0.2152, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 8001, loss 0.3086, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 8501, loss 0.2106, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 9001, loss 0.2091, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9501, loss 0.1751, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10001, loss 0.2074, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10501, loss 0.2100, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11001, loss 0.1631, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11501, loss 0.2585, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 12001, loss 0.3014, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 12501, loss 0.1210, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 13001, loss 0.2480, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 13501, loss 0.2217, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 14001, loss 0.2989, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 14501, loss 0.1793, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 15001, loss 0.1949, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 15501, loss 0.1195, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 16001, loss 0.1366, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 16501, loss 0.2200, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 17001, loss 0.1116, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 17501, loss 0.2190, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18001, loss 0.1968, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18501, loss 0.1339, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19001, loss 0.1857, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19501, loss 0.1752, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
running_time is 20.083406644
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.29
normal_0.5
./test_glass0/model_MLP_20000_0.29/record_1/MLP_20000_0.29_1
./test_glass0/result_MLP_20000_0.29_normal_0.5/record_1/
----------------------



the AUC is 0.48522167487684725

the Fscore is 0.19047619047619047

the precision is 0.2857142857142857

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.29/record_1/MLP_20000_0.29_2
----------------------



epoch 1, loss 0.7127, train acc 33.33%, f1 0.4956, precision 0.3294, recall 1.0000, auc 0.5043
epoch 501, loss 0.3793, train acc 71.93%, f1 0.7000, precision 0.5385, recall 1.0000, auc 0.7913
epoch 1001, loss 0.2582, train acc 78.36%, f1 0.7483, precision 0.6044, recall 0.9821, auc 0.8345
epoch 1501, loss 0.3579, train acc 79.53%, f1 0.7619, precision 0.6154, recall 1.0000, auc 0.8478
epoch 2001, loss 0.2490, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 2501, loss 0.4220, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 3001, loss 0.3119, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 3501, loss 0.3237, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 4001, loss 0.3321, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 4501, loss 0.2393, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 5001, loss 0.2742, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 5501, loss 0.2385, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 6001, loss 0.1894, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 6501, loss 0.2544, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 7001, loss 0.2639, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 7501, loss 0.2049, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 8001, loss 0.1658, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 8501, loss 0.2314, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 9001, loss 0.1907, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9501, loss 0.1840, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10001, loss 0.1691, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10501, loss 0.2293, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 11001, loss 0.1552, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 11501, loss 0.1429, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 12001, loss 0.2057, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12501, loss 0.2416, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13001, loss 0.1766, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13501, loss 0.1429, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14001, loss 0.2019, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14501, loss 0.1685, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15001, loss 0.1960, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15501, loss 0.1324, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 16001, loss 0.1898, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.1682, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17001, loss 0.0769, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17501, loss 0.1730, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 18001, loss 0.1470, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.1537, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19001, loss 0.1593, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19501, loss 0.1460, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
running_time is 20.124798313
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.29
normal_0.5
./test_glass0/model_MLP_20000_0.29/record_1/MLP_20000_0.29_2
./test_glass0/result_MLP_20000_0.29_normal_0.5/record_1/
----------------------



the AUC is 0.7019704433497538

the Fscore is 0.6060606060606061

the precision is 0.5263157894736842

the recall is 0.7142857142857143

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.29/record_1/MLP_20000_0.29_3
----------------------



epoch 1, loss 0.6068, train acc 45.03%, f1 0.5437, precision 0.3733, recall 1.0000, auc 0.5913
epoch 501, loss 0.4063, train acc 71.35%, f1 0.6879, precision 0.5347, recall 0.9643, auc 0.7778
epoch 1001, loss 0.4427, train acc 75.44%, f1 0.7200, precision 0.5745, recall 0.9643, auc 0.8082
epoch 1501, loss 0.3772, train acc 77.19%, f1 0.7310, precision 0.5955, recall 0.9464, auc 0.8167
epoch 2001, loss 0.3496, train acc 78.36%, f1 0.7413, precision 0.6092, recall 0.9464, auc 0.8254
epoch 2501, loss 0.4316, train acc 78.36%, f1 0.7483, precision 0.6044, recall 0.9821, auc 0.8345
epoch 3001, loss 0.3729, train acc 83.04%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8693
epoch 3501, loss 0.2502, train acc 83.63%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8737
epoch 4001, loss 0.3562, train acc 84.21%, f1 0.8029, precision 0.6790, recall 0.9821, auc 0.8780
epoch 4501, loss 0.2221, train acc 84.21%, f1 0.8029, precision 0.6790, recall 0.9821, auc 0.8780
epoch 5001, loss 0.3095, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 5501, loss 0.1984, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 6001, loss 0.2286, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 6501, loss 0.2222, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 7001, loss 0.2320, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 7501, loss 0.2368, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 8001, loss 0.2229, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 8501, loss 0.2190, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 9001, loss 0.1753, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9501, loss 0.2150, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 10001, loss 0.1760, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10501, loss 0.1832, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11001, loss 0.2017, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11501, loss 0.1763, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12001, loss 0.1931, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12501, loss 0.1744, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13001, loss 0.1416, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13501, loss 0.1774, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14001, loss 0.1031, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14501, loss 0.1220, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 15001, loss 0.1629, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 15501, loss 0.1476, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16001, loss 0.2026, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.1777, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17001, loss 0.1393, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.1906, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.1103, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.1874, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.1309, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.0957, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 20.036346904
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.29
normal_0.5
./test_glass0/model_MLP_20000_0.29/record_1/MLP_20000_0.29_3
./test_glass0/result_MLP_20000_0.29_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.29/record_1/MLP_20000_0.29_4
----------------------



epoch 1, loss 0.7127, train acc 52.05%, f1 0.5729, precision 0.4044, recall 0.9821, auc 0.6389
epoch 501, loss 0.4145, train acc 69.01%, f1 0.6748, precision 0.5140, recall 0.9821, auc 0.7650
epoch 1001, loss 0.3567, train acc 73.68%, f1 0.7020, precision 0.5579, recall 0.9464, auc 0.7906
epoch 1501, loss 0.3928, train acc 77.19%, f1 0.7310, precision 0.5955, recall 0.9464, auc 0.8167
epoch 2001, loss 0.2885, train acc 79.53%, f1 0.7518, precision 0.6235, recall 0.9464, auc 0.8341
epoch 2501, loss 0.3602, train acc 81.87%, f1 0.7801, precision 0.6471, recall 0.9821, auc 0.8606
epoch 3001, loss 0.2762, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 3501, loss 0.2387, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 4001, loss 0.2887, train acc 85.38%, f1 0.8148, precision 0.6962, recall 0.9821, auc 0.8867
epoch 4501, loss 0.2425, train acc 87.13%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.8998
epoch 5001, loss 0.2790, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 5501, loss 0.2527, train acc 87.72%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9041
epoch 6001, loss 0.2732, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 6501, loss 0.1721, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 7001, loss 0.2459, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 7501, loss 0.2035, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 8001, loss 0.2181, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 8501, loss 0.1359, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 9001, loss 0.2614, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 9501, loss 0.1987, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 10001, loss 0.2062, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 10501, loss 0.2444, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 11001, loss 0.1831, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 11501, loss 0.1772, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12001, loss 0.2066, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12501, loss 0.1572, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13001, loss 0.1204, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13501, loss 0.2594, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14001, loss 0.2183, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14501, loss 0.1896, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15001, loss 0.2091, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15501, loss 0.0945, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16001, loss 0.2072, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16501, loss 0.1662, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 17001, loss 0.2168, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 17501, loss 0.1516, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 18001, loss 0.2044, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 18501, loss 0.1633, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19001, loss 0.1557, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19501, loss 0.1624, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
running_time is 20.222540459999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.29
normal_0.5
./test_glass0/model_MLP_20000_0.29/record_1/MLP_20000_0.29_4
./test_glass0/result_MLP_20000_0.29_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.29/record_1/MLP_20000_0.29_5
----------------------



epoch 1, loss 0.5802, train acc 33.14%, f1 0.4934, precision 0.3275, recall 1.0000, auc 0.5043
epoch 501, loss 0.4031, train acc 70.93%, f1 0.6914, precision 0.5283, recall 1.0000, auc 0.7845
epoch 1001, loss 0.3557, train acc 77.91%, f1 0.7397, precision 0.6000, recall 0.9643, auc 0.8270
epoch 1501, loss 0.3762, train acc 78.49%, f1 0.7448, precision 0.6067, recall 0.9643, auc 0.8313
epoch 2001, loss 0.2943, train acc 80.81%, f1 0.7692, precision 0.6322, recall 0.9821, auc 0.8531
epoch 2501, loss 0.3111, train acc 81.98%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8664
epoch 3001, loss 0.3343, train acc 82.56%, f1 0.7887, precision 0.6512, recall 1.0000, auc 0.8707
epoch 3501, loss 0.3231, train acc 83.14%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8704
epoch 4001, loss 0.3225, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 4501, loss 0.3084, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 5001, loss 0.3154, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 5501, loss 0.3092, train acc 84.30%, f1 0.8029, precision 0.6790, recall 0.9821, auc 0.8790
epoch 6001, loss 0.3039, train acc 84.88%, f1 0.8088, precision 0.6875, recall 0.9821, auc 0.8833
epoch 6501, loss 0.2559, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 7001, loss 0.2508, train acc 84.88%, f1 0.8060, precision 0.6923, recall 0.9643, auc 0.8787
epoch 7501, loss 0.2311, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 8001, loss 0.2351, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 8501, loss 0.2338, train acc 87.21%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.9006
epoch 9001, loss 0.2830, train acc 88.95%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9181
epoch 9501, loss 0.1895, train acc 89.53%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9224
epoch 10001, loss 0.2073, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 10501, loss 0.1724, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 11001, loss 0.1773, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 11501, loss 0.2349, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 12001, loss 0.2790, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 12501, loss 0.2040, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 13001, loss 0.2393, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 13501, loss 0.2210, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 14001, loss 0.2819, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 14501, loss 0.2002, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 15001, loss 0.1771, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 15501, loss 0.1692, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 16001, loss 0.1338, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 16501, loss 0.1493, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 17001, loss 0.1223, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 17501, loss 0.1256, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 18001, loss 0.2412, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 18501, loss 0.2409, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 19001, loss 0.0873, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 19501, loss 0.1584, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
running_time is 20.012600985000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.29
normal_0.5
./test_glass0/model_MLP_20000_0.29/record_1/MLP_20000_0.29_5
./test_glass0/result_MLP_20000_0.29_normal_0.5/record_1/
----------------------



the AUC is 0.6071428571428572

the Fscore is 0.43478260869565216

the precision is 0.5555555555555556

the recall is 0.35714285714285715

Done
