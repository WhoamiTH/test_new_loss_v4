/home/z5102138/anaconda3/envs/py36/bin/python
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.28/record_1/MLP_25000_0.28_1
----------------------



epoch 1, loss 0.6788, train acc 40.35%, f1 0.5234, precision 0.3544, recall 1.0000, auc 0.5565
epoch 501, loss 0.4149, train acc 75.44%, f1 0.7200, precision 0.5745, recall 0.9643, auc 0.8082
epoch 1001, loss 0.3987, train acc 80.12%, f1 0.7606, precision 0.6279, recall 0.9643, auc 0.8430
epoch 1501, loss 0.3266, train acc 82.46%, f1 0.7826, precision 0.6585, recall 0.9643, auc 0.8604
epoch 2001, loss 0.2220, train acc 84.21%, f1 0.8000, precision 0.6835, recall 0.9643, auc 0.8734
epoch 2501, loss 0.3756, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 3001, loss 0.3087, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 3501, loss 0.2402, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 4001, loss 0.2646, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 4501, loss 0.2335, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 5001, loss 0.2741, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 5501, loss 0.2160, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 6001, loss 0.2300, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 6501, loss 0.3071, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 7001, loss 0.1682, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 7501, loss 0.2409, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 8001, loss 0.2240, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8501, loss 0.1920, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9001, loss 0.2036, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 9501, loss 0.2294, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 10001, loss 0.2214, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 10501, loss 0.2396, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 11001, loss 0.1685, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 11501, loss 0.2192, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 12001, loss 0.1629, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 12501, loss 0.1634, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 13001, loss 0.1282, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 13501, loss 0.1472, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 14001, loss 0.1453, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 14501, loss 0.1631, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 15001, loss 0.2249, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 15501, loss 0.1790, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 16001, loss 0.1705, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 16501, loss 0.1780, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 17001, loss 0.1716, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 17501, loss 0.1085, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18001, loss 0.2220, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18501, loss 0.1524, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19001, loss 0.2431, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19501, loss 0.1774, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 20001, loss 0.1711, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 20501, loss 0.1334, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 21001, loss 0.1412, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 21501, loss 0.1717, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 22001, loss 0.2358, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 22501, loss 0.2008, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 23001, loss 0.1758, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 23501, loss 0.1380, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24001, loss 0.1505, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24501, loss 0.1773, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
running_time is 24.109447374
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.28
normal_0.5
./test_glass0/model_MLP_25000_0.28/record_1/MLP_25000_0.28_1
./test_glass0/result_MLP_25000_0.28_normal_0.5/record_1/
----------------------



the AUC is 0.5431034482758621

the Fscore is 0.4242424242424242

the precision is 0.3684210526315789

the recall is 0.5

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.28/record_1/MLP_25000_0.28_2
----------------------



epoch 1, loss 0.6282, train acc 32.75%, f1 0.4934, precision 0.3275, recall 1.0000, auc 0.5000
epoch 501, loss 0.3586, train acc 71.93%, f1 0.6923, precision 0.5400, recall 0.9643, auc 0.7821
epoch 1001, loss 0.3810, train acc 81.29%, f1 0.7714, precision 0.6429, recall 0.9643, auc 0.8517
epoch 1501, loss 0.3620, train acc 81.87%, f1 0.7801, precision 0.6471, recall 0.9821, auc 0.8606
epoch 2001, loss 0.3430, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 2501, loss 0.2296, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 3001, loss 0.3223, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 3501, loss 0.2075, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 4001, loss 0.1872, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 4501, loss 0.2136, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 5001, loss 0.1814, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 5501, loss 0.1925, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 6001, loss 0.1942, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 6501, loss 0.2750, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 7001, loss 0.2190, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 7501, loss 0.1451, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 8001, loss 0.1688, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 8501, loss 0.1195, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9001, loss 0.1906, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 9501, loss 0.1400, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10001, loss 0.1759, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10501, loss 0.1890, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11001, loss 0.1853, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 11501, loss 0.1270, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12001, loss 0.1517, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12501, loss 0.1765, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13001, loss 0.1123, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13501, loss 0.1029, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14001, loss 0.1605, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14501, loss 0.1737, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 15001, loss 0.1414, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15501, loss 0.1580, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16001, loss 0.1272, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.1615, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17001, loss 0.0827, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17501, loss 0.1802, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18001, loss 0.0893, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.1352, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19001, loss 0.1390, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19501, loss 0.1982, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20001, loss 0.1930, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 20501, loss 0.1384, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21001, loss 0.1443, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 21501, loss 0.1130, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22001, loss 0.1367, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22501, loss 0.1194, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23001, loss 0.1316, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23501, loss 0.1575, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24001, loss 0.1365, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24501, loss 0.1152, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 23.552520261999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.28
normal_0.5
./test_glass0/model_MLP_25000_0.28/record_1/MLP_25000_0.28_2
./test_glass0/result_MLP_25000_0.28_normal_0.5/record_1/
----------------------



the AUC is 0.5012315270935961

the Fscore is 0.11764705882352941

the precision is 0.3333333333333333

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.28/record_1/MLP_25000_0.28_3
----------------------



epoch 1, loss 0.6789, train acc 40.35%, f1 0.5234, precision 0.3544, recall 1.0000, auc 0.5565
epoch 501, loss 0.3817, train acc 72.51%, f1 0.6928, precision 0.5464, recall 0.9464, auc 0.7819
epoch 1001, loss 0.4987, train acc 76.61%, f1 0.7260, precision 0.5889, recall 0.9464, auc 0.8123
epoch 1501, loss 0.2617, train acc 77.78%, f1 0.7324, precision 0.6047, recall 0.9286, auc 0.8165
epoch 2001, loss 0.3889, train acc 79.53%, f1 0.7552, precision 0.6207, recall 0.9643, auc 0.8387
epoch 2501, loss 0.2984, train acc 82.46%, f1 0.7794, precision 0.6625, recall 0.9464, auc 0.8558
epoch 3001, loss 0.3660, train acc 84.80%, f1 0.8030, precision 0.6974, recall 0.9464, auc 0.8732
epoch 3501, loss 0.2144, train acc 85.96%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8911
epoch 4001, loss 0.1998, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 4501, loss 0.2361, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 5001, loss 0.2413, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 5501, loss 0.2179, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 6001, loss 0.2146, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 6501, loss 0.1981, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 7001, loss 0.2097, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 7501, loss 0.1643, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 8001, loss 0.2137, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 8501, loss 0.1600, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 9001, loss 0.2013, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 9501, loss 0.1723, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 10001, loss 0.1461, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 10501, loss 0.1792, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 11001, loss 0.2047, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 11501, loss 0.1834, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12001, loss 0.1927, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 12501, loss 0.1303, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13001, loss 0.1462, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13501, loss 0.1948, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14001, loss 0.1510, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14501, loss 0.2049, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15001, loss 0.2125, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15501, loss 0.1459, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16001, loss 0.1506, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16501, loss 0.2056, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17001, loss 0.1664, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17501, loss 0.1448, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18001, loss 0.1355, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18501, loss 0.1748, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19001, loss 0.1786, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19501, loss 0.1991, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 20001, loss 0.1236, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 20501, loss 0.1330, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 21001, loss 0.1565, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 21501, loss 0.1794, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 22001, loss 0.1338, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 22501, loss 0.1407, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 23001, loss 0.1778, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 23501, loss 0.1292, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 24001, loss 0.1399, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 24501, loss 0.0998, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
running_time is 23.488291060999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.28
normal_0.5
./test_glass0/model_MLP_25000_0.28/record_1/MLP_25000_0.28_3
./test_glass0/result_MLP_25000_0.28_normal_0.5/record_1/
----------------------



the AUC is 0.5541871921182266

the Fscore is 0.23529411764705882

the precision is 0.6666666666666666

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.28/record_1/MLP_25000_0.28_4
----------------------



epoch 1, loss 0.6789, train acc 33.92%, f1 0.4978, precision 0.3314, recall 1.0000, auc 0.5087
epoch 501, loss 0.4213, train acc 69.01%, f1 0.6708, precision 0.5143, recall 0.9643, auc 0.7604
epoch 1001, loss 0.3832, train acc 74.27%, f1 0.7105, precision 0.5625, recall 0.9643, auc 0.7995
epoch 1501, loss 0.4064, train acc 75.44%, f1 0.7162, precision 0.5761, recall 0.9464, auc 0.8036
epoch 2001, loss 0.4900, train acc 79.53%, f1 0.7586, precision 0.6180, recall 0.9821, auc 0.8432
epoch 2501, loss 0.3791, train acc 83.04%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8693
epoch 3001, loss 0.3194, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 3501, loss 0.3417, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 4001, loss 0.2938, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 4501, loss 0.3140, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 5001, loss 0.3381, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 5501, loss 0.3244, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 6001, loss 0.2579, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 6501, loss 0.2790, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 7001, loss 0.2628, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 7501, loss 0.2602, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 8001, loss 0.2539, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 8501, loss 0.2402, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 9001, loss 0.3074, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 9501, loss 0.3187, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 10001, loss 0.2174, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 10501, loss 0.2202, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 11001, loss 0.2611, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 11501, loss 0.2640, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 12001, loss 0.1718, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 12501, loss 0.1639, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 13001, loss 0.2943, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 13501, loss 0.2146, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 14001, loss 0.1876, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 14501, loss 0.2967, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 15001, loss 0.1678, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 15501, loss 0.2593, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 16001, loss 0.2034, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 16501, loss 0.2144, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 17001, loss 0.2076, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 17501, loss 0.2037, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 18001, loss 0.2298, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 18501, loss 0.1847, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 19001, loss 0.2411, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 19501, loss 0.2116, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 20001, loss 0.2366, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 20501, loss 0.1807, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 21001, loss 0.1863, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 21501, loss 0.1605, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 22001, loss 0.1649, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 22501, loss 0.2235, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 23001, loss 0.1717, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 23501, loss 0.1574, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 24001, loss 0.2609, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 24501, loss 0.1406, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
running_time is 24.220126652
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.28
normal_0.5
./test_glass0/model_MLP_25000_0.28/record_1/MLP_25000_0.28_4
./test_glass0/result_MLP_25000_0.28_normal_0.5/record_1/
----------------------



the AUC is 0.5751231527093597

the Fscore is 0.4

the precision is 0.45454545454545453

the recall is 0.35714285714285715

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.28/record_1/MLP_25000_0.28_5
----------------------



epoch 1, loss 0.6280, train acc 53.49%, f1 0.5455, precision 0.4000, recall 0.8571, auc 0.6182
epoch 501, loss 0.3679, train acc 70.35%, f1 0.6871, precision 0.5234, recall 1.0000, auc 0.7802
epoch 1001, loss 0.3774, train acc 77.91%, f1 0.7397, precision 0.6000, recall 0.9643, auc 0.8270
epoch 1501, loss 0.3628, train acc 80.81%, f1 0.7660, precision 0.6353, recall 0.9643, auc 0.8485
epoch 2001, loss 0.3711, train acc 80.23%, f1 0.7606, precision 0.6279, recall 0.9643, auc 0.8442
epoch 2501, loss 0.2398, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 3001, loss 0.3332, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 3501, loss 0.2681, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 4001, loss 0.3548, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 4501, loss 0.2719, train acc 85.47%, f1 0.8148, precision 0.6962, recall 0.9821, auc 0.8876
epoch 5001, loss 0.2602, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 5501, loss 0.3059, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 6001, loss 0.2919, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 6501, loss 0.2557, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 7001, loss 0.2629, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 7501, loss 0.2397, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 8001, loss 0.2182, train acc 88.95%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9181
epoch 8501, loss 0.2619, train acc 90.70%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9310
epoch 9001, loss 0.1925, train acc 89.53%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9224
epoch 9501, loss 0.2986, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 10001, loss 0.2153, train acc 91.28%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9307
epoch 10501, loss 0.2723, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 11001, loss 0.2453, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 11501, loss 0.2128, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 12001, loss 0.2318, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 12501, loss 0.2002, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 13001, loss 0.1614, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 13501, loss 0.1607, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 14001, loss 0.1953, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 14501, loss 0.1983, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 15001, loss 0.2243, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 15501, loss 0.2144, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 16001, loss 0.1594, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 16501, loss 0.1514, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 17001, loss 0.2257, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 17501, loss 0.2127, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 18001, loss 0.2201, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 18501, loss 0.1423, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 19001, loss 0.1948, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 19501, loss 0.1374, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 20001, loss 0.1705, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 20501, loss 0.2135, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 21001, loss 0.1478, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 21501, loss 0.2471, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 22001, loss 0.1211, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 22501, loss 0.2574, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 23001, loss 0.2199, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 23501, loss 0.2112, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 24001, loss 0.1218, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 24501, loss 0.1595, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
running_time is 23.536386151
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.28
normal_0.5
./test_glass0/model_MLP_25000_0.28/record_1/MLP_25000_0.28_5
./test_glass0/result_MLP_25000_0.28_normal_0.5/record_1/
----------------------



the AUC is 0.7678571428571428

the Fscore is 0.6829268292682926

the precision is 0.5185185185185185

the recall is 1.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.29/record_1/MLP_25000_0.29_1
----------------------



epoch 1, loss 0.6598, train acc 32.75%, f1 0.4934, precision 0.3275, recall 1.0000, auc 0.5000
epoch 501, loss 0.3500, train acc 76.02%, f1 0.7248, precision 0.5806, recall 0.9643, auc 0.8126
epoch 1001, loss 0.2841, train acc 80.12%, f1 0.7606, precision 0.6279, recall 0.9643, auc 0.8430
epoch 1501, loss 0.2473, train acc 82.46%, f1 0.7826, precision 0.6585, recall 0.9643, auc 0.8604
epoch 2001, loss 0.2376, train acc 81.29%, f1 0.7714, precision 0.6429, recall 0.9643, auc 0.8517
epoch 2501, loss 0.3737, train acc 83.63%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8737
epoch 3001, loss 0.3478, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 3501, loss 0.2068, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 4001, loss 0.2810, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 4501, loss 0.2306, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 5001, loss 0.2790, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 5501, loss 0.3121, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 6001, loss 0.3047, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 6501, loss 0.2786, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 7001, loss 0.2372, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 7501, loss 0.2085, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 8001, loss 0.2315, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 8501, loss 0.2577, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 9001, loss 0.1505, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9501, loss 0.2103, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 10001, loss 0.1663, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10501, loss 0.1798, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11001, loss 0.2052, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11501, loss 0.1420, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 12001, loss 0.1964, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 12501, loss 0.1935, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 13001, loss 0.2219, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 13501, loss 0.2560, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 14001, loss 0.1652, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 14501, loss 0.1787, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 15001, loss 0.2399, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 15501, loss 0.2813, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 16001, loss 0.1793, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 16501, loss 0.2169, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 17001, loss 0.1666, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 17501, loss 0.2089, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 18001, loss 0.2228, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 18501, loss 0.1961, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 19001, loss 0.1666, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 19501, loss 0.2076, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 20001, loss 0.1852, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 20501, loss 0.2074, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 21001, loss 0.1655, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 21501, loss 0.1710, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 22001, loss 0.1771, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 22501, loss 0.2606, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 23001, loss 0.1561, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 23501, loss 0.2628, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 24001, loss 0.2117, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 24501, loss 0.1853, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
running_time is 23.601228098
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.29
normal_0.5
./test_glass0/model_MLP_25000_0.29/record_1/MLP_25000_0.29_1
./test_glass0/result_MLP_25000_0.29_normal_0.5/record_1/
----------------------



the AUC is 0.5443349753694581

the Fscore is 0.4444444444444444

the precision is 0.36363636363636365

the recall is 0.5714285714285714

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.29/record_1/MLP_25000_0.29_2
----------------------



epoch 1, loss 0.6067, train acc 53.22%, f1 0.5833, precision 0.4118, recall 1.0000, auc 0.6522
epoch 501, loss 0.4312, train acc 72.51%, f1 0.6968, precision 0.5455, recall 0.9643, auc 0.7865
epoch 1001, loss 0.3722, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 1501, loss 0.3825, train acc 82.46%, f1 0.7887, precision 0.6512, recall 1.0000, auc 0.8696
epoch 2001, loss 0.2596, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 2501, loss 0.3292, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 3001, loss 0.2478, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 3501, loss 0.1953, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 4001, loss 0.2840, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 4501, loss 0.2234, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 5001, loss 0.1489, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 5501, loss 0.2310, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 6001, loss 0.1954, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 6501, loss 0.1989, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 7001, loss 0.2372, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 7501, loss 0.1674, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 8001, loss 0.1687, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 8501, loss 0.2094, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 9001, loss 0.2248, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 9501, loss 0.1594, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10001, loss 0.1556, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10501, loss 0.1482, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11001, loss 0.2004, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11501, loss 0.1659, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12001, loss 0.1013, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12501, loss 0.1166, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13001, loss 0.1518, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13501, loss 0.2462, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14001, loss 0.1614, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14501, loss 0.1464, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.1709, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15501, loss 0.1644, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16001, loss 0.1136, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.1221, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17001, loss 0.1723, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17501, loss 0.1453, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18001, loss 0.1434, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.0943, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19001, loss 0.1375, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.1387, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20001, loss 0.1603, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20501, loss 0.1930, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21001, loss 0.1555, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21501, loss 0.1796, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22001, loss 0.1051, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22501, loss 0.0837, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23001, loss 0.1633, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23501, loss 0.1334, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24001, loss 0.1136, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24501, loss 0.1080, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 23.363841667
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.29
normal_0.5
./test_glass0/model_MLP_25000_0.29/record_1/MLP_25000_0.29_2
./test_glass0/result_MLP_25000_0.29_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.29/record_1/MLP_25000_0.29_3
----------------------



epoch 1, loss 0.5538, train acc 33.92%, f1 0.4978, precision 0.3314, recall 1.0000, auc 0.5087
epoch 501, loss 0.4853, train acc 71.35%, f1 0.6839, precision 0.5354, recall 0.9464, auc 0.7732
epoch 1001, loss 0.2783, train acc 73.68%, f1 0.7134, precision 0.5545, recall 1.0000, auc 0.8043
epoch 1501, loss 0.3755, train acc 78.36%, f1 0.7413, precision 0.6092, recall 0.9464, auc 0.8254
epoch 2001, loss 0.2350, train acc 79.53%, f1 0.7552, precision 0.6207, recall 0.9643, auc 0.8387
epoch 2501, loss 0.3018, train acc 81.87%, f1 0.7770, precision 0.6506, recall 0.9643, auc 0.8561
epoch 3001, loss 0.3374, train acc 83.04%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8693
epoch 3501, loss 0.3559, train acc 83.63%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8737
epoch 4001, loss 0.2724, train acc 84.80%, f1 0.8088, precision 0.6875, recall 0.9821, auc 0.8824
epoch 4501, loss 0.2555, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 5001, loss 0.2240, train acc 87.13%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.8998
epoch 5501, loss 0.2571, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 6001, loss 0.1724, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 6501, loss 0.2609, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 7001, loss 0.2167, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 7501, loss 0.1648, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8001, loss 0.2093, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 8501, loss 0.2360, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 9001, loss 0.1615, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9501, loss 0.1774, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10001, loss 0.1483, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10501, loss 0.1646, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11001, loss 0.1640, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11501, loss 0.1811, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12001, loss 0.1516, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12501, loss 0.1000, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13001, loss 0.1436, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13501, loss 0.1528, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14001, loss 0.1693, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14501, loss 0.1201, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 15001, loss 0.1384, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15501, loss 0.1590, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16001, loss 0.1146, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.1593, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17001, loss 0.1518, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.1197, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.1383, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.1284, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.1222, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.1543, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20001, loss 0.1388, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20501, loss 0.0859, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21001, loss 0.1303, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21501, loss 0.1446, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22001, loss 0.1931, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 22501, loss 0.1471, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23001, loss 0.1335, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 23501, loss 0.1718, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24001, loss 0.1444, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24501, loss 0.1311, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 23.906658852
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.29
normal_0.5
./test_glass0/model_MLP_25000_0.29/record_1/MLP_25000_0.29_3
./test_glass0/result_MLP_25000_0.29_normal_0.5/record_1/
----------------------



the AUC is 0.6970443349753696

the Fscore is 0.5714285714285714

the precision is 0.8571428571428571

the recall is 0.42857142857142855

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.29/record_1/MLP_25000_0.29_4
----------------------



epoch 1, loss 0.6332, train acc 32.75%, f1 0.4934, precision 0.3275, recall 1.0000, auc 0.5000
epoch 501, loss 0.4107, train acc 69.01%, f1 0.6708, precision 0.5143, recall 0.9643, auc 0.7604
epoch 1001, loss 0.4305, train acc 73.10%, f1 0.6974, precision 0.5521, recall 0.9464, auc 0.7863
epoch 1501, loss 0.4256, train acc 76.61%, f1 0.7260, precision 0.5889, recall 0.9464, auc 0.8123
epoch 2001, loss 0.2249, train acc 78.95%, f1 0.7534, precision 0.6111, recall 0.9821, auc 0.8389
epoch 2501, loss 0.3128, train acc 80.70%, f1 0.7692, precision 0.6322, recall 0.9821, auc 0.8519
epoch 3001, loss 0.2859, train acc 81.87%, f1 0.7801, precision 0.6471, recall 0.9821, auc 0.8606
epoch 3501, loss 0.3399, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 4001, loss 0.3070, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 4501, loss 0.3314, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 5001, loss 0.4098, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 5501, loss 0.2460, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 6001, loss 0.3622, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 6501, loss 0.3163, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 7001, loss 0.3341, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 7501, loss 0.2211, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 8001, loss 0.3395, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 8501, loss 0.2694, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 9001, loss 0.3328, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 9501, loss 0.2779, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 10001, loss 0.2838, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 10501, loss 0.2179, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 11001, loss 0.2314, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 11501, loss 0.2861, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 12001, loss 0.2082, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 12501, loss 0.2217, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 13001, loss 0.1952, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 13501, loss 0.2326, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 14001, loss 0.1980, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 14501, loss 0.1883, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 15001, loss 0.1908, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 15501, loss 0.2502, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 16001, loss 0.1631, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 16501, loss 0.1736, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 17001, loss 0.2382, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 17501, loss 0.2664, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 18001, loss 0.1983, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 18501, loss 0.2081, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 19001, loss 0.3042, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 19501, loss 0.3126, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 20001, loss 0.2713, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 20501, loss 0.2291, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 21001, loss 0.2440, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 21501, loss 0.1561, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 22001, loss 0.3359, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 22501, loss 0.2196, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 23001, loss 0.1655, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 23501, loss 0.1826, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 24001, loss 0.1929, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 24501, loss 0.2447, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
running_time is 23.845481106
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.29
normal_0.5
./test_glass0/model_MLP_25000_0.29/record_1/MLP_25000_0.29_4
./test_glass0/result_MLP_25000_0.29_normal_0.5/record_1/
----------------------



the AUC is 0.8078817733990148

the Fscore is 0.7272727272727273

the precision is 0.631578947368421

the recall is 0.8571428571428571

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.29/record_1/MLP_25000_0.29_5
----------------------



epoch 1, loss 0.8194, train acc 36.05%, f1 0.4271, precision 0.3015, recall 0.7321, auc 0.4566
epoch 501, loss 0.3313, train acc 69.77%, f1 0.6790, precision 0.5189, recall 0.9821, auc 0.7712
epoch 1001, loss 0.3743, train acc 78.49%, f1 0.7448, precision 0.6067, recall 0.9643, auc 0.8313
epoch 1501, loss 0.2531, train acc 81.40%, f1 0.7714, precision 0.6429, recall 0.9643, auc 0.8528
epoch 2001, loss 0.3006, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 2501, loss 0.2844, train acc 83.72%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8793
epoch 3001, loss 0.3325, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 3501, loss 0.2072, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 4001, loss 0.2374, train acc 86.05%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8966
epoch 4501, loss 0.1749, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 5001, loss 0.2119, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 5501, loss 0.2639, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 6001, loss 0.2050, train acc 89.53%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9224
epoch 6501, loss 0.2233, train acc 89.53%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9224
epoch 7001, loss 0.2069, train acc 90.12%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9267
epoch 7501, loss 0.2487, train acc 90.12%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9267
epoch 8001, loss 0.2203, train acc 90.12%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9267
epoch 8501, loss 0.2129, train acc 90.70%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9310
epoch 9001, loss 0.2282, train acc 91.28%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9353
epoch 9501, loss 0.2529, train acc 91.28%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9353
epoch 10001, loss 0.1618, train acc 91.28%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9353
epoch 10501, loss 0.2106, train acc 91.28%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9353
epoch 11001, loss 0.2293, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 11501, loss 0.1720, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 12001, loss 0.2448, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 12501, loss 0.2457, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 13001, loss 0.1933, train acc 91.28%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9353
epoch 13501, loss 0.1145, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 14001, loss 0.2495, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 14501, loss 0.1855, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 15001, loss 0.1490, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 15501, loss 0.1895, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 16001, loss 0.1609, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 16501, loss 0.2298, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 17001, loss 0.1946, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 17501, loss 0.1880, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 18001, loss 0.1940, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 18501, loss 0.1840, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 19001, loss 0.1499, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 19501, loss 0.1731, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 20001, loss 0.2029, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 20501, loss 0.1885, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 21001, loss 0.1063, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 21501, loss 0.1734, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 22001, loss 0.1628, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 22501, loss 0.1859, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 23001, loss 0.1480, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 23501, loss 0.1494, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 24001, loss 0.1370, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 24501, loss 0.1630, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
running_time is 23.657985012
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.29
normal_0.5
./test_glass0/model_MLP_25000_0.29/record_1/MLP_25000_0.29_5
./test_glass0/result_MLP_25000_0.29_normal_0.5/record_1/
----------------------



the AUC is 0.5714285714285714

the Fscore is 0.3157894736842105

the precision is 0.6

the recall is 0.21428571428571427

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.3/record_1/MLP_25000_0.3_1
----------------------



epoch 1, loss 0.6113, train acc 45.03%, f1 0.5437, precision 0.3733, recall 1.0000, auc 0.5913
epoch 501, loss 0.3449, train acc 75.44%, f1 0.7200, precision 0.5745, recall 0.9643, auc 0.8082
epoch 1001, loss 0.2589, train acc 77.19%, f1 0.7347, precision 0.5934, recall 0.9643, auc 0.8213
epoch 1501, loss 0.3633, train acc 81.29%, f1 0.7714, precision 0.6429, recall 0.9643, auc 0.8517
epoch 2001, loss 0.3064, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 2501, loss 0.3036, train acc 84.80%, f1 0.8088, precision 0.6875, recall 0.9821, auc 0.8824
epoch 3001, loss 0.3252, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 3501, loss 0.3356, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 4001, loss 0.3486, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 4501, loss 0.2659, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 5001, loss 0.2930, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 5501, loss 0.3234, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 6001, loss 0.2654, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 6501, loss 0.2454, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 7001, loss 0.3198, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 7501, loss 0.2599, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 8001, loss 0.3041, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 8501, loss 0.1915, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 9001, loss 0.2222, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 9501, loss 0.2338, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10001, loss 0.0714, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10501, loss 0.2116, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11001, loss 0.2096, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11501, loss 0.1613, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 12001, loss 0.1224, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 12501, loss 0.2382, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 13001, loss 0.1289, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 13501, loss 0.2443, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 14001, loss 0.2905, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 14501, loss 0.2032, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 15001, loss 0.1956, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 15501, loss 0.2453, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 16001, loss 0.2297, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 16501, loss 0.1726, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 17001, loss 0.2556, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 17501, loss 0.1521, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18001, loss 0.2001, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18501, loss 0.1487, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19001, loss 0.1273, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19501, loss 0.1555, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 20001, loss 0.1481, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 20501, loss 0.1618, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 21001, loss 0.1992, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 21501, loss 0.1491, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 22001, loss 0.1989, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 22501, loss 0.1716, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 23001, loss 0.1530, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 23501, loss 0.2194, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24001, loss 0.2133, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24501, loss 0.2422, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
running_time is 23.471173403999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.3
normal_0.5
./test_glass0/model_MLP_25000_0.3/record_1/MLP_25000_0.3_1
./test_glass0/result_MLP_25000_0.3_normal_0.5/record_1/
----------------------



the AUC is 0.541871921182266

the Fscore is 0.39999999999999997

the precision is 0.375

the recall is 0.42857142857142855

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.3/record_1/MLP_25000_0.3_2
----------------------



epoch 1, loss 0.5835, train acc 35.09%, f1 0.5022, precision 0.3353, recall 1.0000, auc 0.5174
epoch 501, loss 0.3635, train acc 74.85%, f1 0.7190, precision 0.5670, recall 0.9821, auc 0.8085
epoch 1001, loss 0.3304, train acc 81.29%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8609
epoch 1501, loss 0.3449, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 2001, loss 0.2519, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 2501, loss 0.2510, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 3001, loss 0.2933, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 3501, loss 0.2775, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 4001, loss 0.2110, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 4501, loss 0.1817, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 5001, loss 0.1591, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 5501, loss 0.2202, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 6001, loss 0.1843, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 6501, loss 0.1573, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 7001, loss 0.2348, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 7501, loss 0.1725, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 8001, loss 0.2351, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 8501, loss 0.1598, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 9001, loss 0.1867, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 9501, loss 0.1435, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10001, loss 0.1367, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10501, loss 0.1664, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11001, loss 0.1958, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11501, loss 0.1451, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12001, loss 0.1867, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12501, loss 0.1475, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13001, loss 0.0867, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13501, loss 0.1365, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14001, loss 0.1440, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14501, loss 0.1776, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.1525, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15501, loss 0.1671, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16001, loss 0.1836, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.1744, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17001, loss 0.1787, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.1139, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.1197, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.1538, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.1649, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.1061, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20001, loss 0.1252, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20501, loss 0.1470, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21001, loss 0.1905, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 21501, loss 0.1750, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 22001, loss 0.1243, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 22501, loss 0.1441, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23001, loss 0.1101, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23501, loss 0.1174, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 24001, loss 0.1165, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 24501, loss 0.1271, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 23.373108165
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.3
normal_0.5
./test_glass0/model_MLP_25000_0.3/record_1/MLP_25000_0.3_2
./test_glass0/result_MLP_25000_0.3_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.3/record_1/MLP_25000_0.3_3
----------------------



epoch 1, loss 0.6665, train acc 37.43%, f1 0.5114, precision 0.3436, recall 1.0000, auc 0.5348
epoch 501, loss 0.3268, train acc 72.51%, f1 0.7006, precision 0.5446, recall 0.9821, auc 0.7911
epoch 1001, loss 0.4974, train acc 74.85%, f1 0.7190, precision 0.5670, recall 0.9821, auc 0.8085
epoch 1501, loss 0.4491, train acc 77.19%, f1 0.7347, precision 0.5934, recall 0.9643, auc 0.8213
epoch 2001, loss 0.3490, train acc 78.95%, f1 0.7534, precision 0.6111, recall 0.9821, auc 0.8389
epoch 2501, loss 0.3266, train acc 78.95%, f1 0.7534, precision 0.6111, recall 0.9821, auc 0.8389
epoch 3001, loss 0.2865, train acc 81.29%, f1 0.7746, precision 0.6395, recall 0.9821, auc 0.8563
epoch 3501, loss 0.2940, train acc 82.46%, f1 0.7857, precision 0.6548, recall 0.9821, auc 0.8650
epoch 4001, loss 0.2175, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 4501, loss 0.3472, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 5001, loss 0.2449, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 5501, loss 0.3058, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 6001, loss 0.2618, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 6501, loss 0.2479, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 7001, loss 0.2546, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 7501, loss 0.3332, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 8001, loss 0.2483, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 8501, loss 0.3074, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 9001, loss 0.2873, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 9501, loss 0.2414, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 10001, loss 0.2523, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 10501, loss 0.2471, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 11001, loss 0.2456, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 11501, loss 0.2272, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 12001, loss 0.1916, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 12501, loss 0.1752, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 13001, loss 0.2311, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 13501, loss 0.2628, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 14001, loss 0.2407, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 14501, loss 0.2243, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 15001, loss 0.1820, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 15501, loss 0.2070, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 16001, loss 0.2737, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 16501, loss 0.1954, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 17001, loss 0.1667, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 17501, loss 0.2410, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 18001, loss 0.2690, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 18501, loss 0.2095, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 19001, loss 0.2373, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19501, loss 0.1685, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 20001, loss 0.1716, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 20501, loss 0.2645, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 21001, loss 0.2055, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 21501, loss 0.1733, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 22001, loss 0.1960, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 22501, loss 0.2059, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 23001, loss 0.2029, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 23501, loss 0.2333, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24001, loss 0.1497, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24501, loss 0.1636, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
running_time is 23.901114995
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.3
normal_0.5
./test_glass0/model_MLP_25000_0.3/record_1/MLP_25000_0.3_3
./test_glass0/result_MLP_25000_0.3_normal_0.5/record_1/
----------------------



the AUC is 0.7179802955665024

the Fscore is 0.6206896551724138

the precision is 0.6

the recall is 0.6428571428571429

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.3/record_1/MLP_25000_0.3_4
----------------------



epoch 1, loss 0.7222, train acc 42.11%, f1 0.4817, precision 0.3407, recall 0.8214, auc 0.5238
epoch 501, loss 0.3840, train acc 71.35%, f1 0.6879, precision 0.5347, recall 0.9643, auc 0.7778
epoch 1001, loss 0.3639, train acc 75.44%, f1 0.7200, precision 0.5745, recall 0.9643, auc 0.8082
epoch 1501, loss 0.3043, train acc 77.19%, f1 0.7347, precision 0.5934, recall 0.9643, auc 0.8213
epoch 2001, loss 0.3586, train acc 77.19%, f1 0.7417, precision 0.5895, recall 1.0000, auc 0.8304
epoch 2501, loss 0.3844, train acc 81.29%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8609
epoch 3001, loss 0.3075, train acc 83.04%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8693
epoch 3501, loss 0.2600, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 4001, loss 0.2699, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 4501, loss 0.2808, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 5001, loss 0.2838, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 5501, loss 0.2865, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 6001, loss 0.1693, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 6501, loss 0.2459, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 7001, loss 0.1595, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 7501, loss 0.2714, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 8001, loss 0.2469, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 8501, loss 0.1751, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 9001, loss 0.1929, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9501, loss 0.2059, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10001, loss 0.2124, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10501, loss 0.1780, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 11001, loss 0.2054, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11501, loss 0.2249, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 12001, loss 0.1477, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 12501, loss 0.1911, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 13001, loss 0.1414, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 13501, loss 0.2060, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 14001, loss 0.1412, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 14501, loss 0.2327, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 15001, loss 0.1621, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 15501, loss 0.1989, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 16001, loss 0.1360, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 16501, loss 0.1405, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 17001, loss 0.1509, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 17501, loss 0.2552, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 18001, loss 0.2742, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 18501, loss 0.1883, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19001, loss 0.1941, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19501, loss 0.1922, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 20001, loss 0.1783, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 20501, loss 0.1900, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 21001, loss 0.1464, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 21501, loss 0.1575, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 22001, loss 0.1767, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 22501, loss 0.1993, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 23001, loss 0.1753, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 23501, loss 0.2102, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 24001, loss 0.1855, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 24501, loss 0.1675, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
running_time is 23.507839534
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.3
normal_0.5
./test_glass0/model_MLP_25000_0.3/record_1/MLP_25000_0.3_4
./test_glass0/result_MLP_25000_0.3_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.3/record_1/MLP_25000_0.3_5
----------------------



epoch 1, loss 0.7223, train acc 38.37%, f1 0.5138, precision 0.3457, recall 1.0000, auc 0.5431
epoch 501, loss 0.3851, train acc 69.19%, f1 0.6788, precision 0.5138, recall 1.0000, auc 0.7716
epoch 1001, loss 0.4232, train acc 76.74%, f1 0.7297, precision 0.5870, recall 0.9643, auc 0.8183
epoch 1501, loss 0.3084, train acc 79.65%, f1 0.7552, precision 0.6207, recall 0.9643, auc 0.8399
epoch 2001, loss 0.3313, train acc 81.98%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8664
epoch 2501, loss 0.3369, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 3001, loss 0.2654, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 3501, loss 0.2848, train acc 85.47%, f1 0.8148, precision 0.6962, recall 0.9821, auc 0.8876
epoch 4001, loss 0.2747, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 4501, loss 0.2079, train acc 86.63%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8962
epoch 5001, loss 0.2974, train acc 86.05%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8919
epoch 5501, loss 0.2469, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 6001, loss 0.3227, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 6501, loss 0.2496, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 7001, loss 0.2273, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 7501, loss 0.2343, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 8001, loss 0.2950, train acc 89.53%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9224
epoch 8501, loss 0.2109, train acc 90.12%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9267
epoch 9001, loss 0.1608, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 9501, loss 0.2451, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 10001, loss 0.2232, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 10501, loss 0.2295, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 11001, loss 0.1756, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 11501, loss 0.2545, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 12001, loss 0.2553, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 12501, loss 0.2624, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 13001, loss 0.2131, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 13501, loss 0.1411, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 14001, loss 0.1706, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 14501, loss 0.1992, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 15001, loss 0.1528, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 15501, loss 0.2437, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 16001, loss 0.2230, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 16501, loss 0.2327, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 17001, loss 0.1968, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 17501, loss 0.1587, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 18001, loss 0.1771, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 18501, loss 0.1503, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 19001, loss 0.1694, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 19501, loss 0.1972, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 20001, loss 0.2306, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 20501, loss 0.1387, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 21001, loss 0.1429, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 21501, loss 0.1021, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 22001, loss 0.2370, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 22501, loss 0.1808, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 23001, loss 0.1469, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 23501, loss 0.1799, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 24001, loss 0.2209, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 24501, loss 0.2061, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
running_time is 23.559582889
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.3
normal_0.5
./test_glass0/model_MLP_25000_0.3/record_1/MLP_25000_0.3_5
./test_glass0/result_MLP_25000_0.3_normal_0.5/record_1/
----------------------



the AUC is 0.625

the Fscore is 0.45454545454545453

the precision is 0.625

the recall is 0.35714285714285715

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.31/record_1/MLP_25000_0.31_1
----------------------



epoch 1, loss 0.5877, train acc 36.84%, f1 0.5091, precision 0.3415, recall 1.0000, auc 0.5304
epoch 501, loss 0.3634, train acc 71.93%, f1 0.6962, precision 0.5392, recall 0.9821, auc 0.7867
epoch 1001, loss 0.3451, train acc 78.95%, f1 0.7500, precision 0.6136, recall 0.9643, auc 0.8343
epoch 1501, loss 0.3991, train acc 78.95%, f1 0.7500, precision 0.6136, recall 0.9643, auc 0.8343
epoch 2001, loss 0.2716, train acc 80.70%, f1 0.7692, precision 0.6322, recall 0.9821, auc 0.8519
epoch 2501, loss 0.3861, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 3001, loss 0.2483, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 3501, loss 0.3711, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 4001, loss 0.3447, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 4501, loss 0.2798, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 5001, loss 0.2218, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 5501, loss 0.2823, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 6001, loss 0.2916, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 6501, loss 0.3033, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 7001, loss 0.2072, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 7501, loss 0.2185, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 8001, loss 0.2397, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 8501, loss 0.1907, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 9001, loss 0.3208, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 9501, loss 0.2469, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 10001, loss 0.2800, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 10501, loss 0.2794, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 11001, loss 0.3264, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 11501, loss 0.2762, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 12001, loss 0.2834, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 12501, loss 0.3309, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 13001, loss 0.1818, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 13501, loss 0.2131, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 14001, loss 0.2370, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 14501, loss 0.2363, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 15001, loss 0.2678, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 15501, loss 0.2778, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 16001, loss 0.2687, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 16501, loss 0.2927, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 17001, loss 0.1993, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 17501, loss 0.2155, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 18001, loss 0.1437, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 18501, loss 0.2228, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 19001, loss 0.2630, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 19501, loss 0.1588, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 20001, loss 0.2212, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 20501, loss 0.2055, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 21001, loss 0.1972, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 21501, loss 0.1955, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 22001, loss 0.2045, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 22501, loss 0.1910, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 23001, loss 0.1667, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 23501, loss 0.2207, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 24001, loss 0.2457, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 24501, loss 0.3072, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
running_time is 23.515271226000003
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.31
normal_0.5
./test_glass0/model_MLP_25000_0.31/record_1/MLP_25000_0.31_1
./test_glass0/result_MLP_25000_0.31_normal_0.5/record_1/
----------------------



the AUC is 0.6379310344827587

the Fscore is 0.5714285714285715

the precision is 0.4

the recall is 1.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.31/record_1/MLP_25000_0.31_2
----------------------



epoch 1, loss 0.7327, train acc 35.09%, f1 0.4932, precision 0.3313, recall 0.9643, auc 0.5082
epoch 501, loss 0.4416, train acc 69.59%, f1 0.6790, precision 0.5189, recall 0.9821, auc 0.7693
epoch 1001, loss 0.3680, train acc 78.95%, f1 0.7534, precision 0.6111, recall 0.9821, auc 0.8389
epoch 1501, loss 0.3956, train acc 80.70%, f1 0.7692, precision 0.6322, recall 0.9821, auc 0.8519
epoch 2001, loss 0.3362, train acc 81.29%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8609
epoch 2501, loss 0.3036, train acc 81.87%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8652
epoch 3001, loss 0.3719, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 3501, loss 0.3251, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 4001, loss 0.3624, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 4501, loss 0.2207, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 5001, loss 0.2323, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 5501, loss 0.2397, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 6001, loss 0.2415, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 6501, loss 0.2080, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 7001, loss 0.2866, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 7501, loss 0.1973, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8001, loss 0.2336, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 8501, loss 0.1990, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 9001, loss 0.1688, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 9501, loss 0.2405, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 10001, loss 0.1715, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 10501, loss 0.1781, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 11001, loss 0.2053, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 11501, loss 0.1856, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 12001, loss 0.1826, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 12501, loss 0.1912, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 13001, loss 0.2226, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 13501, loss 0.1534, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 14001, loss 0.2004, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 14501, loss 0.1720, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15001, loss 0.1372, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15501, loss 0.1378, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16001, loss 0.0586, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16501, loss 0.1580, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17001, loss 0.1212, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17501, loss 0.1429, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18001, loss 0.1734, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 18501, loss 0.1539, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 19001, loss 0.1326, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19501, loss 0.1804, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20001, loss 0.1293, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20501, loss 0.1895, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21001, loss 0.1416, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21501, loss 0.1465, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 22001, loss 0.1631, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 22501, loss 0.2289, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 23001, loss 0.1677, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 23501, loss 0.1515, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 24001, loss 0.2278, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 24501, loss 0.1275, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
running_time is 23.895929873
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.31
normal_0.5
./test_glass0/model_MLP_25000_0.31/record_1/MLP_25000_0.31_2
./test_glass0/result_MLP_25000_0.31_normal_0.5/record_1/
----------------------



the AUC is 0.5369458128078817

the Fscore is 0.22222222222222224

the precision is 0.5

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.31/record_1/MLP_25000_0.31_3
----------------------



epoch 1, loss 0.7618, train acc 33.92%, f1 0.4978, precision 0.3314, recall 1.0000, auc 0.5087
epoch 501, loss 0.4362, train acc 72.51%, f1 0.7006, precision 0.5446, recall 0.9821, auc 0.7911
epoch 1001, loss 0.3833, train acc 74.27%, f1 0.7105, precision 0.5625, recall 0.9643, auc 0.7995
epoch 1501, loss 0.4363, train acc 74.85%, f1 0.7190, precision 0.5670, recall 0.9821, auc 0.8085
epoch 2001, loss 0.3414, train acc 78.36%, f1 0.7448, precision 0.6067, recall 0.9643, auc 0.8300
epoch 2501, loss 0.3362, train acc 80.12%, f1 0.7606, precision 0.6279, recall 0.9643, auc 0.8430
epoch 3001, loss 0.2832, train acc 83.04%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8693
epoch 3501, loss 0.2948, train acc 83.04%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8693
epoch 4001, loss 0.2296, train acc 85.38%, f1 0.8148, precision 0.6962, recall 0.9821, auc 0.8867
epoch 4501, loss 0.2342, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 5001, loss 0.2130, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 5501, loss 0.2207, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 6001, loss 0.2118, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 6501, loss 0.1516, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 7001, loss 0.2733, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 7501, loss 0.1852, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 8001, loss 0.2021, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 8501, loss 0.1778, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 9001, loss 0.1533, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 9501, loss 0.2309, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 10001, loss 0.2027, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 10501, loss 0.2067, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 11001, loss 0.1679, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 11501, loss 0.1677, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 12001, loss 0.2521, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 12501, loss 0.1892, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 13001, loss 0.2000, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13501, loss 0.1901, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14001, loss 0.1957, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14501, loss 0.1797, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15001, loss 0.1591, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15501, loss 0.2176, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16001, loss 0.1714, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16501, loss 0.1888, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 17001, loss 0.1578, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 17501, loss 0.1771, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 18001, loss 0.1677, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 18501, loss 0.1596, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19001, loss 0.2515, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19501, loss 0.1149, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 20001, loss 0.1111, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 20501, loss 0.1917, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 21001, loss 0.2153, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 21501, loss 0.1718, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 22001, loss 0.1611, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 22501, loss 0.1231, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 23001, loss 0.1286, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 23501, loss 0.1318, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 24001, loss 0.1935, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 24501, loss 0.1370, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
running_time is 23.461620658
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.31
normal_0.5
./test_glass0/model_MLP_25000_0.31/record_1/MLP_25000_0.31_3
./test_glass0/result_MLP_25000_0.31_normal_0.5/record_1/
----------------------



the AUC is 0.644088669950739

the Fscore is 0.4761904761904762

the precision is 0.7142857142857143

the recall is 0.35714285714285715

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.31/record_1/MLP_25000_0.31_4
----------------------



epoch 1, loss 0.6458, train acc 33.92%, f1 0.4978, precision 0.3314, recall 1.0000, auc 0.5087
epoch 501, loss 0.4571, train acc 66.67%, f1 0.6545, precision 0.4954, recall 0.9643, auc 0.7430
epoch 1001, loss 0.3766, train acc 70.18%, f1 0.6792, precision 0.5243, recall 0.9643, auc 0.7691
epoch 1501, loss 0.3765, train acc 75.44%, f1 0.7200, precision 0.5745, recall 0.9643, auc 0.8082
epoch 2001, loss 0.3534, train acc 76.61%, f1 0.7333, precision 0.5851, recall 0.9821, auc 0.8215
epoch 2501, loss 0.3810, train acc 80.70%, f1 0.7724, precision 0.6292, recall 1.0000, auc 0.8565
epoch 3001, loss 0.3821, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 3501, loss 0.3014, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 4001, loss 0.2571, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 4501, loss 0.3292, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 5001, loss 0.2989, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 5501, loss 0.2320, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 6001, loss 0.2462, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 6501, loss 0.2570, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 7001, loss 0.3064, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 7501, loss 0.2567, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 8001, loss 0.3167, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 8501, loss 0.2806, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 9001, loss 0.2894, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 9501, loss 0.2578, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 10001, loss 0.2425, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 10501, loss 0.3869, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 11001, loss 0.2262, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 11501, loss 0.1364, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 12001, loss 0.2792, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 12501, loss 0.1642, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 13001, loss 0.2443, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 13501, loss 0.2525, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 14001, loss 0.2469, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 14501, loss 0.1961, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 15001, loss 0.2655, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 15501, loss 0.2031, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 16001, loss 0.2120, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 16501, loss 0.3210, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 17001, loss 0.1778, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 17501, loss 0.2019, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 18001, loss 0.2723, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 18501, loss 0.2907, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 19001, loss 0.3050, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 19501, loss 0.2598, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 20001, loss 0.2567, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 20501, loss 0.2273, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 21001, loss 0.2046, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 21501, loss 0.2214, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 22001, loss 0.2428, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 22501, loss 0.2301, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 23001, loss 0.1984, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 23501, loss 0.1521, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 24001, loss 0.1945, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 24501, loss 0.2261, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
running_time is 23.576631099
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.31
normal_0.5
./test_glass0/model_MLP_25000_0.31/record_1/MLP_25000_0.31_4
./test_glass0/result_MLP_25000_0.31_normal_0.5/record_1/
----------------------



the AUC is 0.6169950738916256

the Fscore is 0.5365853658536585

the precision is 0.4074074074074074

the recall is 0.7857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.31/record_1/MLP_25000_0.31_5
----------------------



epoch 1, loss 0.7619, train acc 49.42%, f1 0.5628, precision 0.3916, recall 1.0000, auc 0.6250
epoch 501, loss 0.3434, train acc 69.19%, f1 0.6788, precision 0.5138, recall 1.0000, auc 0.7716
epoch 1001, loss 0.3992, train acc 76.16%, f1 0.7248, precision 0.5806, recall 0.9643, auc 0.8140
epoch 1501, loss 0.5079, train acc 80.23%, f1 0.7639, precision 0.6250, recall 0.9821, auc 0.8488
epoch 2001, loss 0.4147, train acc 80.81%, f1 0.7724, precision 0.6292, recall 1.0000, auc 0.8578
epoch 2501, loss 0.3120, train acc 81.40%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8621
epoch 3001, loss 0.3149, train acc 80.81%, f1 0.7724, precision 0.6292, recall 1.0000, auc 0.8578
epoch 3501, loss 0.2585, train acc 82.56%, f1 0.7857, precision 0.6548, recall 0.9821, auc 0.8661
epoch 4001, loss 0.3213, train acc 81.98%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8664
epoch 4501, loss 0.2938, train acc 82.56%, f1 0.7857, precision 0.6548, recall 0.9821, auc 0.8661
epoch 5001, loss 0.3088, train acc 82.56%, f1 0.7857, precision 0.6548, recall 0.9821, auc 0.8661
epoch 5501, loss 0.3338, train acc 83.14%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8704
epoch 6001, loss 0.2934, train acc 83.72%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8747
epoch 6501, loss 0.3214, train acc 84.88%, f1 0.8088, precision 0.6875, recall 0.9821, auc 0.8833
epoch 7001, loss 0.2846, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 7501, loss 0.2172, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 8001, loss 0.3023, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 8501, loss 0.2128, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 9001, loss 0.2378, train acc 87.21%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.9006
epoch 9501, loss 0.2170, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 10001, loss 0.3103, train acc 88.95%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9181
epoch 10501, loss 0.3191, train acc 88.95%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9181
epoch 11001, loss 0.2751, train acc 89.53%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9224
epoch 11501, loss 0.2425, train acc 89.53%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9224
epoch 12001, loss 0.1803, train acc 90.12%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9267
epoch 12501, loss 0.2086, train acc 90.70%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9310
epoch 13001, loss 0.3224, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 13501, loss 0.2073, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 14001, loss 0.1388, train acc 91.28%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9353
epoch 14501, loss 0.2255, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 15001, loss 0.2054, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 15501, loss 0.2214, train acc 91.28%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9353
epoch 16001, loss 0.2623, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 16501, loss 0.2154, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 17001, loss 0.2393, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 17501, loss 0.2450, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 18001, loss 0.1825, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 18501, loss 0.2001, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 19001, loss 0.2628, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 19501, loss 0.2751, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 20001, loss 0.2211, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 20501, loss 0.1907, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 21001, loss 0.1792, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 21501, loss 0.1966, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 22001, loss 0.1981, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 22501, loss 0.2121, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 23001, loss 0.2532, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 23501, loss 0.1764, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 24001, loss 0.2421, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 24501, loss 0.2372, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
running_time is 23.768461636
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.31
normal_0.5
./test_glass0/model_MLP_25000_0.31/record_1/MLP_25000_0.31_5
./test_glass0/result_MLP_25000_0.31_normal_0.5/record_1/
----------------------



the AUC is 0.5178571428571429

the Fscore is 0.125

the precision is 0.5

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.32/record_1/MLP_25000_0.32_1
----------------------



epoch 1, loss 0.8954, train acc 39.18%, f1 0.5185, precision 0.3500, recall 1.0000, auc 0.5478
epoch 501, loss 0.3500, train acc 73.10%, f1 0.7013, precision 0.5510, recall 0.9643, auc 0.7908
epoch 1001, loss 0.3800, train acc 77.78%, f1 0.7397, precision 0.6000, recall 0.9643, auc 0.8256
epoch 1501, loss 0.3581, train acc 79.53%, f1 0.7552, precision 0.6207, recall 0.9643, auc 0.8387
epoch 2001, loss 0.3468, train acc 81.29%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8609
epoch 2501, loss 0.3197, train acc 83.04%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8693
epoch 3001, loss 0.3016, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 3501, loss 0.2789, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 4001, loss 0.2901, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 4501, loss 0.2814, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 5001, loss 0.3620, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 5501, loss 0.3132, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 6001, loss 0.2639, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 6501, loss 0.2443, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 7001, loss 0.2652, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 7501, loss 0.2633, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 8001, loss 0.2607, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 8501, loss 0.2158, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 9001, loss 0.2479, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 9501, loss 0.2499, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 10001, loss 0.2814, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 10501, loss 0.3126, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 11001, loss 0.2420, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 11501, loss 0.2771, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 12001, loss 0.2826, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 12501, loss 0.2203, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 13001, loss 0.2439, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 13501, loss 0.3039, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 14001, loss 0.3741, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 14501, loss 0.2230, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 15001, loss 0.2497, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 15501, loss 0.2766, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 16001, loss 0.2377, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 16501, loss 0.2503, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 17001, loss 0.2912, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 17501, loss 0.2267, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 18001, loss 0.3378, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 18501, loss 0.2404, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 19001, loss 0.2825, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 19501, loss 0.3075, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 20001, loss 0.2465, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 20501, loss 0.2142, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 21001, loss 0.2226, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 21501, loss 0.2685, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 22001, loss 0.3297, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 22501, loss 0.2533, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 23001, loss 0.2079, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 23501, loss 0.2003, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 24001, loss 0.1761, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 24501, loss 0.3212, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
running_time is 23.568146154
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.32
normal_0.5
./test_glass0/model_MLP_25000_0.32/record_1/MLP_25000_0.32_1
./test_glass0/result_MLP_25000_0.32_normal_0.5/record_1/
----------------------



the AUC is 0.5800492610837438

the Fscore is 0.4864864864864865

the precision is 0.391304347826087

the recall is 0.6428571428571429

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.32/record_1/MLP_25000_0.32_2
----------------------



epoch 1, loss 0.6837, train acc 43.27%, f1 0.5174, precision 0.3586, recall 0.9286, auc 0.5599
epoch 501, loss 0.4408, train acc 70.76%, f1 0.6914, precision 0.5283, recall 1.0000, auc 0.7826
epoch 1001, loss 0.2633, train acc 79.53%, f1 0.7619, precision 0.6154, recall 1.0000, auc 0.8478
epoch 1501, loss 0.2802, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 2001, loss 0.3208, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 2501, loss 0.2858, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 3001, loss 0.3210, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 3501, loss 0.2368, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 4001, loss 0.2385, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 4501, loss 0.2478, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 5001, loss 0.2811, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 5501, loss 0.2119, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 6001, loss 0.2235, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 6501, loss 0.1948, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 7001, loss 0.1959, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 7501, loss 0.2073, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 8001, loss 0.2093, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 8501, loss 0.2215, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 9001, loss 0.1926, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 9501, loss 0.2089, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 10001, loss 0.2319, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 10501, loss 0.2015, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 11001, loss 0.1685, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 11501, loss 0.1199, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12001, loss 0.1715, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12501, loss 0.1576, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13001, loss 0.1683, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13501, loss 0.1859, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14001, loss 0.1805, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14501, loss 0.2214, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 15001, loss 0.1512, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.1122, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.1472, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.1953, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.1130, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.1142, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.1505, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.1986, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.1647, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.1332, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20001, loss 0.1397, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20501, loss 0.1196, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21001, loss 0.1086, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21501, loss 0.2028, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22001, loss 0.2084, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22501, loss 0.1716, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23001, loss 0.1949, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23501, loss 0.1393, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24001, loss 0.1861, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24501, loss 0.1876, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 23.645159916
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.32
normal_0.5
./test_glass0/model_MLP_25000_0.32/record_1/MLP_25000_0.32_2
./test_glass0/result_MLP_25000_0.32_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.32/record_1/MLP_25000_0.32_3
----------------------



epoch 1, loss 0.6837, train acc 32.75%, f1 0.4934, precision 0.3275, recall 1.0000, auc 0.5000
epoch 501, loss 0.3652, train acc 69.59%, f1 0.6829, precision 0.5185, recall 1.0000, auc 0.7739
epoch 1001, loss 0.4288, train acc 73.68%, f1 0.7134, precision 0.5545, recall 1.0000, auc 0.8043
epoch 1501, loss 0.3546, train acc 76.02%, f1 0.7320, precision 0.5773, recall 1.0000, auc 0.8217
epoch 2001, loss 0.4162, train acc 77.19%, f1 0.7383, precision 0.5914, recall 0.9821, auc 0.8259
epoch 2501, loss 0.3293, train acc 77.78%, f1 0.7432, precision 0.5978, recall 0.9821, auc 0.8302
epoch 3001, loss 0.2532, train acc 81.29%, f1 0.7746, precision 0.6395, recall 0.9821, auc 0.8563
epoch 3501, loss 0.2958, train acc 81.87%, f1 0.7801, precision 0.6471, recall 0.9821, auc 0.8606
epoch 4001, loss 0.3085, train acc 83.63%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8737
epoch 4501, loss 0.2811, train acc 83.04%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8693
epoch 5001, loss 0.2900, train acc 84.21%, f1 0.8029, precision 0.6790, recall 0.9821, auc 0.8780
epoch 5501, loss 0.3190, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 6001, loss 0.2028, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 6501, loss 0.2313, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 7001, loss 0.2265, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 7501, loss 0.2611, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 8001, loss 0.2609, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 8501, loss 0.3099, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 9001, loss 0.2113, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 9501, loss 0.1982, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 10001, loss 0.2475, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 10501, loss 0.2863, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 11001, loss 0.2522, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 11501, loss 0.2063, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 12001, loss 0.2143, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 12501, loss 0.2278, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 13001, loss 0.2290, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 13501, loss 0.2476, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 14001, loss 0.1579, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 14501, loss 0.2062, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 15001, loss 0.1434, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 15501, loss 0.2640, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 16001, loss 0.2326, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 16501, loss 0.1801, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 17001, loss 0.1842, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 17501, loss 0.2278, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 18001, loss 0.2267, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18501, loss 0.1376, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 19001, loss 0.1849, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 19501, loss 0.1721, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 20001, loss 0.1631, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 20501, loss 0.1919, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 21001, loss 0.2166, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 21501, loss 0.2210, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 22001, loss 0.1529, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 22501, loss 0.1801, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 23001, loss 0.1350, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 23501, loss 0.1285, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 24001, loss 0.1489, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 24501, loss 0.1672, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
running_time is 23.532608533
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.32
normal_0.5
./test_glass0/model_MLP_25000_0.32/record_1/MLP_25000_0.32_3
./test_glass0/result_MLP_25000_0.32_normal_0.5/record_1/
----------------------



the AUC is 0.7155172413793104

the Fscore is 0.6086956521739131

the precision is 0.7777777777777778

the recall is 0.5

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.32/record_1/MLP_25000_0.32_4
----------------------



epoch 1, loss 0.8655, train acc 32.75%, f1 0.4934, precision 0.3275, recall 1.0000, auc 0.5000
epoch 501, loss 0.4422, train acc 67.84%, f1 0.6707, precision 0.5045, recall 1.0000, auc 0.7609
epoch 1001, loss 0.3668, train acc 69.01%, f1 0.6748, precision 0.5140, recall 0.9821, auc 0.7650
epoch 1501, loss 0.3983, train acc 73.10%, f1 0.7013, precision 0.5510, recall 0.9643, auc 0.7908
epoch 2001, loss 0.3723, train acc 73.68%, f1 0.7020, precision 0.5579, recall 0.9464, auc 0.7906
epoch 2501, loss 0.3888, train acc 75.44%, f1 0.7200, precision 0.5745, recall 0.9643, auc 0.8082
epoch 3001, loss 0.3274, train acc 78.36%, f1 0.7448, precision 0.6067, recall 0.9643, auc 0.8300
epoch 3501, loss 0.3203, train acc 80.12%, f1 0.7639, precision 0.6250, recall 0.9821, auc 0.8476
epoch 4001, loss 0.3106, train acc 79.53%, f1 0.7552, precision 0.6207, recall 0.9643, auc 0.8387
epoch 4501, loss 0.3540, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 5001, loss 0.2779, train acc 81.29%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8609
epoch 5501, loss 0.2163, train acc 80.70%, f1 0.7724, precision 0.6292, recall 1.0000, auc 0.8565
epoch 6001, loss 0.3460, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 6501, loss 0.2220, train acc 84.80%, f1 0.8088, precision 0.6875, recall 0.9821, auc 0.8824
epoch 7001, loss 0.3405, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 7501, loss 0.3596, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 8001, loss 0.2782, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 8501, loss 0.2855, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 9001, loss 0.3215, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 9501, loss 0.2880, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 10001, loss 0.2893, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 10501, loss 0.2797, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 11001, loss 0.3054, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 11501, loss 0.3263, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 12001, loss 0.2536, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 12501, loss 0.2514, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 13001, loss 0.2686, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 13501, loss 0.2766, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 14001, loss 0.2270, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 14501, loss 0.2591, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 15001, loss 0.2936, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 15501, loss 0.2545, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 16001, loss 0.2743, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 16501, loss 0.2737, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 17001, loss 0.2404, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 17501, loss 0.2657, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 18001, loss 0.2383, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 18501, loss 0.2550, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 19001, loss 0.2522, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 19501, loss 0.2863, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 20001, loss 0.2542, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 20501, loss 0.2642, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 21001, loss 0.2494, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 21501, loss 0.2763, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 22001, loss 0.1603, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 22501, loss 0.3434, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 23001, loss 0.2064, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 23501, loss 0.2508, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 24001, loss 0.2917, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 24501, loss 0.2797, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
running_time is 23.464796538
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.32
normal_0.5
./test_glass0/model_MLP_25000_0.32/record_1/MLP_25000_0.32_4
./test_glass0/result_MLP_25000_0.32_normal_0.5/record_1/
----------------------



the AUC is 0.6711822660098523

the Fscore is 0.5909090909090909

the precision is 0.43333333333333335

the recall is 0.9285714285714286

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.32/record_1/MLP_25000_0.32_5
----------------------



epoch 1, loss 0.6533, train acc 43.60%, f1 0.5359, precision 0.3660, recall 1.0000, auc 0.5819
epoch 501, loss 0.3997, train acc 67.44%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.7586
epoch 1001, loss 0.3731, train acc 77.33%, f1 0.7383, precision 0.5914, recall 0.9821, auc 0.8273
epoch 1501, loss 0.2974, train acc 78.49%, f1 0.7483, precision 0.6044, recall 0.9821, auc 0.8359
epoch 2001, loss 0.2728, train acc 80.23%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8534
epoch 2501, loss 0.2969, train acc 81.40%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8621
epoch 3001, loss 0.3068, train acc 81.98%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8664
epoch 3501, loss 0.3645, train acc 81.98%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8664
epoch 4001, loss 0.2474, train acc 81.98%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8664
epoch 4501, loss 0.2772, train acc 81.40%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8621
epoch 5001, loss 0.3430, train acc 82.56%, f1 0.7887, precision 0.6512, recall 1.0000, auc 0.8707
epoch 5501, loss 0.3241, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 6001, loss 0.2948, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 6501, loss 0.2748, train acc 82.56%, f1 0.7857, precision 0.6548, recall 0.9821, auc 0.8661
epoch 7001, loss 0.3257, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 7501, loss 0.2431, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 8001, loss 0.3388, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 8501, loss 0.3108, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 9001, loss 0.3214, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 9501, loss 0.2433, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 10001, loss 0.3193, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 10501, loss 0.2445, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 11001, loss 0.2886, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 11501, loss 0.3099, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 12001, loss 0.3306, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 12501, loss 0.2871, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 13001, loss 0.2478, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 13501, loss 0.2715, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 14001, loss 0.3506, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 14501, loss 0.2803, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 15001, loss 0.3897, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 15501, loss 0.3190, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 16001, loss 0.3067, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 16501, loss 0.2978, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 17001, loss 0.3012, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 17501, loss 0.3379, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 18001, loss 0.3038, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 18501, loss 0.2637, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 19001, loss 0.2968, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 19501, loss 0.2381, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 20001, loss 0.2996, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 20501, loss 0.2794, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 21001, loss 0.3717, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 21501, loss 0.2525, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 22001, loss 0.3692, train acc 86.05%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8966
epoch 22501, loss 0.4194, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 23001, loss 0.3027, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 23501, loss 0.2624, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 24001, loss 0.2612, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 24501, loss 0.2245, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
running_time is 23.561195411
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.32
normal_0.5
./test_glass0/model_MLP_25000_0.32/record_1/MLP_25000_0.32_5
./test_glass0/result_MLP_25000_0.32_normal_0.5/record_1/
----------------------



the AUC is 0.6785714285714286

the Fscore is 0.5806451612903226

the precision is 0.5294117647058824

the recall is 0.6428571428571429

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.33/record_1/MLP_25000_0.33_1
----------------------



epoch 1, loss 0.7889, train acc 32.75%, f1 0.4934, precision 0.3275, recall 1.0000, auc 0.5000
epoch 501, loss 0.3942, train acc 69.59%, f1 0.6829, precision 0.5185, recall 1.0000, auc 0.7739
epoch 1001, loss 0.3523, train acc 75.44%, f1 0.7200, precision 0.5745, recall 0.9643, auc 0.8082
epoch 1501, loss 0.3517, train acc 77.19%, f1 0.7347, precision 0.5934, recall 0.9643, auc 0.8213
epoch 2001, loss 0.3552, train acc 78.95%, f1 0.7568, precision 0.6087, recall 1.0000, auc 0.8435
epoch 2501, loss 0.2552, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 3001, loss 0.3776, train acc 81.87%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8652
epoch 3501, loss 0.2512, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 4001, loss 0.3102, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 4501, loss 0.2871, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 5001, loss 0.3221, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 5501, loss 0.2324, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 6001, loss 0.2452, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 6501, loss 0.3371, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 7001, loss 0.3625, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 7501, loss 0.3012, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 8001, loss 0.2826, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 8501, loss 0.3087, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 9001, loss 0.3673, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 9501, loss 0.2333, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 10001, loss 0.3291, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 10501, loss 0.2094, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 11001, loss 0.2414, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 11501, loss 0.2517, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 12001, loss 0.2584, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 12501, loss 0.2876, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 13001, loss 0.2654, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 13501, loss 0.3520, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 14001, loss 0.2538, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 14501, loss 0.2484, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 15001, loss 0.2472, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 15501, loss 0.2418, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 16001, loss 0.1983, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 16501, loss 0.2543, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 17001, loss 0.3232, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 17501, loss 0.2005, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 18001, loss 0.2659, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 18501, loss 0.2089, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 19001, loss 0.2924, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 19501, loss 0.2914, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 20001, loss 0.2559, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 20501, loss 0.2936, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 21001, loss 0.2559, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 21501, loss 0.2082, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 22001, loss 0.2615, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 22501, loss 0.2506, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 23001, loss 0.2685, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 23501, loss 0.2271, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 24001, loss 0.2266, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 24501, loss 0.3500, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
running_time is 23.596122751
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.33
normal_0.5
./test_glass0/model_MLP_25000_0.33/record_1/MLP_25000_0.33_1
./test_glass0/result_MLP_25000_0.33_normal_0.5/record_1/
----------------------



the AUC is 0.6551724137931034

the Fscore is 0.5833333333333334

the precision is 0.4117647058823529

the recall is 1.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.33/record_1/MLP_25000_0.33_2
----------------------



epoch 1, loss 0.6620, train acc 44.44%, f1 0.5366, precision 0.3691, recall 0.9821, auc 0.5824
epoch 501, loss 0.4369, train acc 73.10%, f1 0.7089, precision 0.5490, recall 1.0000, auc 0.8000
epoch 1001, loss 0.3854, train acc 78.95%, f1 0.7568, precision 0.6087, recall 1.0000, auc 0.8435
epoch 1501, loss 0.2350, train acc 81.29%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8609
epoch 2001, loss 0.2891, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 2501, loss 0.3228, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 3001, loss 0.2841, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 3501, loss 0.2963, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 4001, loss 0.2585, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 4501, loss 0.2259, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 5001, loss 0.2049, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 5501, loss 0.2770, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 6001, loss 0.2288, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 6501, loss 0.2326, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 7001, loss 0.2541, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 7501, loss 0.2510, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 8001, loss 0.2459, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8501, loss 0.1903, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9001, loss 0.1603, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 9501, loss 0.2353, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 10001, loss 0.1960, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 10501, loss 0.1254, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 11001, loss 0.1980, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 11501, loss 0.1132, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 12001, loss 0.1468, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12501, loss 0.1975, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13001, loss 0.2039, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13501, loss 0.1683, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14001, loss 0.2080, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14501, loss 0.1979, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15001, loss 0.1424, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15501, loss 0.1714, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.1635, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.1524, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17001, loss 0.1707, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17501, loss 0.1292, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18001, loss 0.1610, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.1496, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19001, loss 0.1779, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.1601, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20001, loss 0.1987, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20501, loss 0.1575, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21001, loss 0.1654, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21501, loss 0.1753, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22001, loss 0.2249, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22501, loss 0.1165, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23001, loss 0.1762, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23501, loss 0.1412, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24001, loss 0.1322, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24501, loss 0.1409, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 23.226489098
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.33
normal_0.5
./test_glass0/model_MLP_25000_0.33/record_1/MLP_25000_0.33_2
./test_glass0/result_MLP_25000_0.33_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.33/record_1/MLP_25000_0.33_3
----------------------



epoch 1, loss 0.5352, train acc 49.12%, f1 0.5628, precision 0.3916, recall 1.0000, auc 0.6217
epoch 501, loss 0.4212, train acc 70.76%, f1 0.6914, precision 0.5283, recall 1.0000, auc 0.7826
epoch 1001, loss 0.3592, train acc 76.61%, f1 0.7368, precision 0.5833, recall 1.0000, auc 0.8261
epoch 1501, loss 0.3305, train acc 78.36%, f1 0.7517, precision 0.6022, recall 1.0000, auc 0.8391
epoch 2001, loss 0.3484, train acc 80.12%, f1 0.7639, precision 0.6250, recall 0.9821, auc 0.8476
epoch 2501, loss 0.3452, train acc 82.46%, f1 0.7857, precision 0.6548, recall 0.9821, auc 0.8650
epoch 3001, loss 0.2512, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 3501, loss 0.2718, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 4001, loss 0.3317, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 4501, loss 0.2832, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 5001, loss 0.2737, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 5501, loss 0.2551, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 6001, loss 0.2391, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 6501, loss 0.2628, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 7001, loss 0.2329, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 7501, loss 0.2737, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 8001, loss 0.1601, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 8501, loss 0.2861, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 9001, loss 0.1571, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 9501, loss 0.2131, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 10001, loss 0.1252, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 10501, loss 0.2121, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 11001, loss 0.1826, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 11501, loss 0.1759, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 12001, loss 0.2139, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 12501, loss 0.1832, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 13001, loss 0.1731, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13501, loss 0.1790, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 14001, loss 0.2182, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14501, loss 0.1496, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15001, loss 0.1861, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15501, loss 0.1974, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16001, loss 0.1844, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16501, loss 0.1713, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 17001, loss 0.1812, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17501, loss 0.1341, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18001, loss 0.1324, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18501, loss 0.1275, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19001, loss 0.1743, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19501, loss 0.1909, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 20001, loss 0.1486, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 20501, loss 0.1441, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 21001, loss 0.2094, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 21501, loss 0.1679, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 22001, loss 0.1996, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 22501, loss 0.1467, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 23001, loss 0.1636, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 23501, loss 0.1926, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 24001, loss 0.1608, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 24501, loss 0.1250, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
running_time is 23.473282953
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.33
normal_0.5
./test_glass0/model_MLP_25000_0.33/record_1/MLP_25000_0.33_3
./test_glass0/result_MLP_25000_0.33_normal_0.5/record_1/
----------------------



the AUC is 0.5541871921182266

the Fscore is 0.23529411764705882

the precision is 0.6666666666666666

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.33/record_1/MLP_25000_0.33_4
----------------------



epoch 1, loss 0.7572, train acc 49.71%, f1 0.5657, precision 0.3944, recall 1.0000, auc 0.6261
epoch 501, loss 0.4021, train acc 68.42%, f1 0.6747, precision 0.5091, recall 1.0000, auc 0.7652
epoch 1001, loss 0.4331, train acc 74.27%, f1 0.7143, precision 0.5612, recall 0.9821, auc 0.8041
epoch 1501, loss 0.3574, train acc 75.44%, f1 0.7273, precision 0.5714, recall 1.0000, auc 0.8174
epoch 2001, loss 0.3261, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 2501, loss 0.2513, train acc 81.87%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8652
epoch 3001, loss 0.2871, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 3501, loss 0.2737, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 4001, loss 0.2666, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 4501, loss 0.3087, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 5001, loss 0.2633, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 5501, loss 0.2332, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 6001, loss 0.3148, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 6501, loss 0.2327, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 7001, loss 0.3016, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 7501, loss 0.3257, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 8001, loss 0.2384, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 8501, loss 0.2249, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 9001, loss 0.2666, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 9501, loss 0.2809, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 10001, loss 0.2295, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 10501, loss 0.2205, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 11001, loss 0.2112, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 11501, loss 0.2223, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 12001, loss 0.2657, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 12501, loss 0.2189, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 13001, loss 0.3232, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 13501, loss 0.2375, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 14001, loss 0.2552, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 14501, loss 0.2512, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 15001, loss 0.1982, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 15501, loss 0.2576, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 16001, loss 0.1458, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 16501, loss 0.3074, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 17001, loss 0.2812, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 17501, loss 0.1613, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 18001, loss 0.2234, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 18501, loss 0.2438, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 19001, loss 0.1866, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 19501, loss 0.2782, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 20001, loss 0.1268, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 20501, loss 0.2960, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 21001, loss 0.1803, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 21501, loss 0.2351, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 22001, loss 0.2703, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 22501, loss 0.2227, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 23001, loss 0.2236, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 23501, loss 0.1987, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 24001, loss 0.2385, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 24501, loss 0.1566, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
running_time is 23.442109169
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.33
normal_0.5
./test_glass0/model_MLP_25000_0.33/record_1/MLP_25000_0.33_4
./test_glass0/result_MLP_25000_0.33_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.33/record_1/MLP_25000_0.33_5
----------------------



epoch 1, loss 0.6937, train acc 32.56%, f1 0.4912, precision 0.3256, recall 1.0000, auc 0.5000
epoch 501, loss 0.3798, train acc 66.28%, f1 0.6588, precision 0.4912, recall 1.0000, auc 0.7500
epoch 1001, loss 0.4452, train acc 75.00%, f1 0.7152, precision 0.5684, recall 0.9643, auc 0.8054
epoch 1501, loss 0.4259, train acc 77.33%, f1 0.7383, precision 0.5914, recall 0.9821, auc 0.8273
epoch 2001, loss 0.3578, train acc 80.23%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8534
epoch 2501, loss 0.3538, train acc 80.81%, f1 0.7724, precision 0.6292, recall 1.0000, auc 0.8578
epoch 3001, loss 0.3076, train acc 81.40%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8621
epoch 3501, loss 0.3650, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 4001, loss 0.3212, train acc 82.56%, f1 0.7857, precision 0.6548, recall 0.9821, auc 0.8661
epoch 4501, loss 0.2892, train acc 82.56%, f1 0.7857, precision 0.6548, recall 0.9821, auc 0.8661
epoch 5001, loss 0.3297, train acc 82.56%, f1 0.7887, precision 0.6512, recall 1.0000, auc 0.8707
epoch 5501, loss 0.2940, train acc 83.14%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8704
epoch 6001, loss 0.2748, train acc 83.14%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8704
epoch 6501, loss 0.3570, train acc 83.72%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8793
epoch 7001, loss 0.3661, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 7501, loss 0.3588, train acc 83.72%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8747
epoch 8001, loss 0.2602, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 8501, loss 0.3387, train acc 83.72%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8747
epoch 9001, loss 0.2838, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 9501, loss 0.1924, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 10001, loss 0.2913, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 10501, loss 0.3128, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 11001, loss 0.3164, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 11501, loss 0.3831, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 12001, loss 0.2878, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 12501, loss 0.2929, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 13001, loss 0.3297, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 13501, loss 0.2933, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 14001, loss 0.2785, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 14501, loss 0.2099, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 15001, loss 0.2908, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 15501, loss 0.3252, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 16001, loss 0.2524, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 16501, loss 0.3275, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 17001, loss 0.3573, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 17501, loss 0.3174, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 18001, loss 0.3501, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 18501, loss 0.2448, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 19001, loss 0.2431, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 19501, loss 0.2674, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 20001, loss 0.2569, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 20501, loss 0.2752, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 21001, loss 0.3125, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 21501, loss 0.2626, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 22001, loss 0.2883, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 22501, loss 0.1990, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 23001, loss 0.2653, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 23501, loss 0.2802, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 24001, loss 0.3105, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 24501, loss 0.3205, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
running_time is 23.546094185999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.33
normal_0.5
./test_glass0/model_MLP_25000_0.33/record_1/MLP_25000_0.33_5
./test_glass0/result_MLP_25000_0.33_normal_0.5/record_1/
----------------------



the AUC is 0.75

the Fscore is 0.6666666666666666

the precision is 0.5

the recall is 1.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.34/record_1/MLP_25000_0.34_1
----------------------



epoch 1, loss 0.6719, train acc 40.35%, f1 0.5234, precision 0.3544, recall 1.0000, auc 0.5565
epoch 501, loss 0.3918, train acc 71.35%, f1 0.6957, precision 0.5333, recall 1.0000, auc 0.7870
epoch 1001, loss 0.3261, train acc 76.61%, f1 0.7297, precision 0.5870, recall 0.9643, auc 0.8169
epoch 1501, loss 0.3027, train acc 78.95%, f1 0.7534, precision 0.6111, recall 0.9821, auc 0.8389
epoch 2001, loss 0.2972, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 2501, loss 0.3497, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 3001, loss 0.3104, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 3501, loss 0.4100, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 4001, loss 0.3211, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 4501, loss 0.2862, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 5001, loss 0.3040, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 5501, loss 0.2791, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 6001, loss 0.3299, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 6501, loss 0.2640, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 7001, loss 0.3682, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 7501, loss 0.1932, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 8001, loss 0.2769, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 8501, loss 0.2517, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 9001, loss 0.1887, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 9501, loss 0.2371, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 10001, loss 0.2085, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 10501, loss 0.2292, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 11001, loss 0.2205, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 11501, loss 0.2088, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 12001, loss 0.2297, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 12501, loss 0.2175, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 13001, loss 0.2092, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 13501, loss 0.2502, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 14001, loss 0.2124, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 14501, loss 0.1891, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 15001, loss 0.1804, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 15501, loss 0.1780, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 16001, loss 0.2316, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 16501, loss 0.2034, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 17001, loss 0.1468, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 17501, loss 0.2231, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18001, loss 0.2362, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18501, loss 0.1873, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19001, loss 0.2421, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19501, loss 0.2467, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 20001, loss 0.1644, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 20501, loss 0.1556, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 21001, loss 0.1679, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 21501, loss 0.2727, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 22001, loss 0.2766, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 22501, loss 0.2117, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 23001, loss 0.1847, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 23501, loss 0.2445, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24001, loss 0.2326, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24501, loss 0.2227, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
running_time is 23.765413813
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.34
normal_0.5
./test_glass0/model_MLP_25000_0.34/record_1/MLP_25000_0.34_1
./test_glass0/result_MLP_25000_0.34_normal_0.5/record_1/
----------------------



the AUC is 0.560344827586207

the Fscore is 0.43750000000000006

the precision is 0.3888888888888889

the recall is 0.5

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.34/record_1/MLP_25000_0.34_2
----------------------



epoch 1, loss 0.6388, train acc 45.03%, f1 0.5155, precision 0.3623, recall 0.8929, auc 0.5638
epoch 501, loss 0.3653, train acc 71.35%, f1 0.6957, precision 0.5333, recall 1.0000, auc 0.7870
epoch 1001, loss 0.2781, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 1501, loss 0.3374, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 2001, loss 0.3574, train acc 81.87%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8652
epoch 2501, loss 0.2789, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 3001, loss 0.2558, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 3501, loss 0.2358, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 4001, loss 0.2301, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 4501, loss 0.2671, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 5001, loss 0.1505, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 5501, loss 0.2061, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 6001, loss 0.2782, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 6501, loss 0.1735, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 7001, loss 0.1571, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 7501, loss 0.2600, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8001, loss 0.2279, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8501, loss 0.1962, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 9001, loss 0.2349, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 9501, loss 0.1899, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 10001, loss 0.1611, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 10501, loss 0.1962, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 11001, loss 0.1614, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 11501, loss 0.2104, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12001, loss 0.1678, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12501, loss 0.1768, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13001, loss 0.2047, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13501, loss 0.1715, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14001, loss 0.2228, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14501, loss 0.2350, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15001, loss 0.2212, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15501, loss 0.1987, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 16001, loss 0.1960, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.1843, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17001, loss 0.1903, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17501, loss 0.1914, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 18001, loss 0.1233, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.1641, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19001, loss 0.1917, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19501, loss 0.1524, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20001, loss 0.1222, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20501, loss 0.1684, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21001, loss 0.1360, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21501, loss 0.1989, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22001, loss 0.1549, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22501, loss 0.1732, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23001, loss 0.1874, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23501, loss 0.1443, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24001, loss 0.1885, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24501, loss 0.1864, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 23.373873215
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.34
normal_0.5
./test_glass0/model_MLP_25000_0.34/record_1/MLP_25000_0.34_2
./test_glass0/result_MLP_25000_0.34_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.34/record_1/MLP_25000_0.34_3
----------------------



epoch 1, loss 0.8377, train acc 41.52%, f1 0.5283, precision 0.3590, recall 1.0000, auc 0.5652
epoch 501, loss 0.4287, train acc 69.59%, f1 0.6829, precision 0.5185, recall 1.0000, auc 0.7739
epoch 1001, loss 0.3874, train acc 74.27%, f1 0.7179, precision 0.5600, recall 1.0000, auc 0.8087
epoch 1501, loss 0.3049, train acc 77.78%, f1 0.7467, precision 0.5957, recall 1.0000, auc 0.8348
epoch 2001, loss 0.3523, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 2501, loss 0.3162, train acc 84.21%, f1 0.8029, precision 0.6790, recall 0.9821, auc 0.8780
epoch 3001, loss 0.2835, train acc 85.38%, f1 0.8148, precision 0.6962, recall 0.9821, auc 0.8867
epoch 3501, loss 0.2596, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 4001, loss 0.2673, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 4501, loss 0.1697, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 5001, loss 0.2450, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 5501, loss 0.2255, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 6001, loss 0.2430, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 6501, loss 0.2671, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 7001, loss 0.2628, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 7501, loss 0.2586, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 8001, loss 0.1756, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8501, loss 0.2511, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9001, loss 0.2377, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 9501, loss 0.1819, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 10001, loss 0.2270, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10501, loss 0.2139, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11001, loss 0.2296, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 11501, loss 0.2313, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 12001, loss 0.2415, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 12501, loss 0.1487, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 13001, loss 0.2704, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 13501, loss 0.2123, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 14001, loss 0.2063, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 14501, loss 0.1851, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 15001, loss 0.2161, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 15501, loss 0.2233, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 16001, loss 0.2507, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 16501, loss 0.1660, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 17001, loss 0.2583, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 17501, loss 0.2227, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18001, loss 0.1662, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 18501, loss 0.2026, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19001, loss 0.2118, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 19501, loss 0.2441, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 20001, loss 0.1784, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 20501, loss 0.2418, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 21001, loss 0.2242, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 21501, loss 0.1730, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 22001, loss 0.1649, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 22501, loss 0.1790, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 23001, loss 0.2620, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 23501, loss 0.2108, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 24001, loss 0.1387, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 24501, loss 0.1842, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
running_time is 23.561366016
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.34
normal_0.5
./test_glass0/model_MLP_25000_0.34/record_1/MLP_25000_0.34_3
./test_glass0/result_MLP_25000_0.34_normal_0.5/record_1/
----------------------



the AUC is 0.770935960591133

the Fscore is 0.689655172413793

the precision is 0.6666666666666666

the recall is 0.7142857142857143

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.34/record_1/MLP_25000_0.34_4
----------------------



epoch 1, loss 0.6719, train acc 35.09%, f1 0.4977, precision 0.3333, recall 0.9821, auc 0.5128
epoch 501, loss 0.4247, train acc 66.08%, f1 0.6548, precision 0.4911, recall 0.9821, auc 0.7432
epoch 1001, loss 0.4555, train acc 71.93%, f1 0.6923, precision 0.5400, recall 0.9643, auc 0.7821
epoch 1501, loss 0.2818, train acc 74.85%, f1 0.7152, precision 0.5684, recall 0.9643, auc 0.8039
epoch 2001, loss 0.3775, train acc 76.02%, f1 0.7320, precision 0.5773, recall 1.0000, auc 0.8217
epoch 2501, loss 0.3277, train acc 78.36%, f1 0.7517, precision 0.6022, recall 1.0000, auc 0.8391
epoch 3001, loss 0.2938, train acc 79.53%, f1 0.7619, precision 0.6154, recall 1.0000, auc 0.8478
epoch 3501, loss 0.3139, train acc 80.70%, f1 0.7724, precision 0.6292, recall 1.0000, auc 0.8565
epoch 4001, loss 0.3569, train acc 82.46%, f1 0.7857, precision 0.6548, recall 0.9821, auc 0.8650
epoch 4501, loss 0.3232, train acc 83.04%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8693
epoch 5001, loss 0.2536, train acc 82.46%, f1 0.7857, precision 0.6548, recall 0.9821, auc 0.8650
epoch 5501, loss 0.3384, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 6001, loss 0.3335, train acc 83.04%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8693
epoch 6501, loss 0.3296, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 7001, loss 0.2517, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 7501, loss 0.3002, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 8001, loss 0.2747, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 8501, loss 0.2745, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 9001, loss 0.2766, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 9501, loss 0.2984, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 10001, loss 0.2661, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 10501, loss 0.1895, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 11001, loss 0.2508, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 11501, loss 0.2509, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 12001, loss 0.2845, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 12501, loss 0.2468, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 13001, loss 0.2466, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 13501, loss 0.2600, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 14001, loss 0.2095, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 14501, loss 0.2548, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 15001, loss 0.2406, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 15501, loss 0.2134, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 16001, loss 0.2093, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 16501, loss 0.1924, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 17001, loss 0.2867, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 17501, loss 0.2381, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 18001, loss 0.2041, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 18501, loss 0.2806, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 19001, loss 0.2154, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 19501, loss 0.2601, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 20001, loss 0.2370, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 20501, loss 0.2319, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 21001, loss 0.2247, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 21501, loss 0.2028, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 22001, loss 0.2503, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 22501, loss 0.1998, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 23001, loss 0.2126, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 23501, loss 0.2222, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24001, loss 0.2680, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24501, loss 0.2275, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
running_time is 23.663900965
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.34
normal_0.5
./test_glass0/model_MLP_25000_0.34/record_1/MLP_25000_0.34_4
./test_glass0/result_MLP_25000_0.34_normal_0.5/record_1/
----------------------



the AUC is 0.6822660098522167

the Fscore is 0.5714285714285714

the precision is 0.5714285714285714

the recall is 0.5714285714285714

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.34/record_1/MLP_25000_0.34_5
----------------------



epoch 1, loss 0.6720, train acc 37.79%, f1 0.4977, precision 0.3376, recall 0.9464, auc 0.5249
epoch 501, loss 0.3753, train acc 67.44%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.7586
epoch 1001, loss 0.4342, train acc 73.84%, f1 0.7097, precision 0.5556, recall 0.9821, auc 0.8014
epoch 1501, loss 0.4097, train acc 78.49%, f1 0.7517, precision 0.6022, recall 1.0000, auc 0.8405
epoch 2001, loss 0.3808, train acc 78.49%, f1 0.7517, precision 0.6022, recall 1.0000, auc 0.8405
epoch 2501, loss 0.3448, train acc 80.23%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8534
epoch 3001, loss 0.2864, train acc 81.40%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8621
epoch 3501, loss 0.3253, train acc 81.98%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8664
epoch 4001, loss 0.2861, train acc 82.56%, f1 0.7857, precision 0.6548, recall 0.9821, auc 0.8661
epoch 4501, loss 0.2633, train acc 81.98%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8664
epoch 5001, loss 0.3241, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 5501, loss 0.3075, train acc 83.72%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8793
epoch 6001, loss 0.3032, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 6501, loss 0.2544, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 7001, loss 0.3113, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 7501, loss 0.2951, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 8001, loss 0.3178, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 8501, loss 0.2576, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 9001, loss 0.2484, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 9501, loss 0.3369, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 10001, loss 0.3412, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 10501, loss 0.2401, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 11001, loss 0.2753, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 11501, loss 0.2705, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 12001, loss 0.2296, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 12501, loss 0.3047, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 13001, loss 0.2931, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 13501, loss 0.3097, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 14001, loss 0.2661, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 14501, loss 0.3107, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 15001, loss 0.2701, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 15501, loss 0.3071, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 16001, loss 0.3447, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 16501, loss 0.2697, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 17001, loss 0.3648, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 17501, loss 0.3026, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 18001, loss 0.2383, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 18501, loss 0.2039, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 19001, loss 0.2451, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 19501, loss 0.3484, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 20001, loss 0.3153, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 20501, loss 0.3669, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 21001, loss 0.3248, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 21501, loss 0.2991, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 22001, loss 0.2766, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 22501, loss 0.2844, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 23001, loss 0.2468, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 23501, loss 0.2581, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 24001, loss 0.3474, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 24501, loss 0.2623, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
running_time is 23.585864966
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.34
normal_0.5
./test_glass0/model_MLP_25000_0.34/record_1/MLP_25000_0.34_5
./test_glass0/result_MLP_25000_0.34_normal_0.5/record_1/
----------------------



the AUC is 0.7142857142857142

the Fscore is 0.6285714285714286

the precision is 0.5238095238095238

the recall is 0.7857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.35/record_1/MLP_25000_0.35_1
----------------------



epoch 1, loss 0.7868, train acc 36.84%, f1 0.5091, precision 0.3415, recall 1.0000, auc 0.5304
epoch 501, loss 0.4207, train acc 69.59%, f1 0.6829, precision 0.5185, recall 1.0000, auc 0.7739
epoch 1001, loss 0.3598, train acc 75.44%, f1 0.7200, precision 0.5745, recall 0.9643, auc 0.8082
epoch 1501, loss 0.4125, train acc 78.36%, f1 0.7448, precision 0.6067, recall 0.9643, auc 0.8300
epoch 2001, loss 0.2389, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 2501, loss 0.2980, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 3001, loss 0.2477, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 3501, loss 0.3232, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 4001, loss 0.3641, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 4501, loss 0.3430, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 5001, loss 0.3338, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 5501, loss 0.2481, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 6001, loss 0.3150, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 6501, loss 0.3046, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 7001, loss 0.3564, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 7501, loss 0.3149, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 8001, loss 0.1993, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 8501, loss 0.2718, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 9001, loss 0.2061, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 9501, loss 0.3493, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 10001, loss 0.2643, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 10501, loss 0.2259, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 11001, loss 0.2049, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 11501, loss 0.2366, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 12001, loss 0.2207, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 12501, loss 0.3241, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 13001, loss 0.2184, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 13501, loss 0.2575, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 14001, loss 0.2127, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 14501, loss 0.2025, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 15001, loss 0.2525, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 15501, loss 0.2632, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 16001, loss 0.3404, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 16501, loss 0.2749, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 17001, loss 0.2088, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 17501, loss 0.2352, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 18001, loss 0.3116, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 18501, loss 0.2081, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 19001, loss 0.1785, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 19501, loss 0.1945, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 20001, loss 0.2101, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 20501, loss 0.1512, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 21001, loss 0.2436, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 21501, loss 0.2648, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 22001, loss 0.2151, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 22501, loss 0.2297, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 23001, loss 0.2862, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 23501, loss 0.2172, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 24001, loss 0.2405, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 24501, loss 0.2898, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
running_time is 23.450007103
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.35
normal_0.5
./test_glass0/model_MLP_25000_0.35/record_1/MLP_25000_0.35_1
./test_glass0/result_MLP_25000_0.35_normal_0.5/record_1/
----------------------



the AUC is 0.6317733990147784

the Fscore is 0.5294117647058824

the precision is 0.45

the recall is 0.6428571428571429

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.35/record_1/MLP_25000_0.35_2
----------------------



epoch 1, loss 0.8563, train acc 38.60%, f1 0.5161, precision 0.3478, recall 1.0000, auc 0.5435
epoch 501, loss 0.4184, train acc 69.01%, f1 0.6788, precision 0.5138, recall 1.0000, auc 0.7696
epoch 1001, loss 0.4089, train acc 78.95%, f1 0.7568, precision 0.6087, recall 1.0000, auc 0.8435
epoch 1501, loss 0.3315, train acc 81.29%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8609
epoch 2001, loss 0.3324, train acc 82.46%, f1 0.7887, precision 0.6512, recall 1.0000, auc 0.8696
epoch 2501, loss 0.3200, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 3001, loss 0.2620, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 3501, loss 0.2465, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 4001, loss 0.2594, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 4501, loss 0.2675, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 5001, loss 0.2665, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 5501, loss 0.2211, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 6001, loss 0.2416, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 6501, loss 0.1857, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 7001, loss 0.2525, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 7501, loss 0.2480, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 8001, loss 0.2080, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 8501, loss 0.1890, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9001, loss 0.1650, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9501, loss 0.1482, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 10001, loss 0.1768, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 10501, loss 0.1857, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 11001, loss 0.2026, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 11501, loss 0.2322, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 12001, loss 0.1984, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 12501, loss 0.1624, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 13001, loss 0.1574, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13501, loss 0.1908, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 14001, loss 0.1925, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14501, loss 0.1900, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15001, loss 0.1940, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15501, loss 0.1808, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16001, loss 0.1408, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 16501, loss 0.1514, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17001, loss 0.1630, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17501, loss 0.1815, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 18001, loss 0.1812, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.1316, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 19001, loss 0.1751, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 19501, loss 0.1403, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20001, loss 0.1687, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20501, loss 0.2157, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 21001, loss 0.1764, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21501, loss 0.1723, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 22001, loss 0.1605, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 22501, loss 0.2226, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 23001, loss 0.1382, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 23501, loss 0.1709, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 24001, loss 0.1869, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 24501, loss 0.1424, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
running_time is 23.387035732
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.35
normal_0.5
./test_glass0/model_MLP_25000_0.35/record_1/MLP_25000_0.35_2
./test_glass0/result_MLP_25000_0.35_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.35/record_1/MLP_25000_0.35_3
----------------------



epoch 1, loss 0.5788, train acc 39.18%, f1 0.5140, precision 0.3481, recall 0.9821, auc 0.5432
epoch 501, loss 0.4911, train acc 68.42%, f1 0.6747, precision 0.5091, recall 1.0000, auc 0.7652
epoch 1001, loss 0.4943, train acc 73.10%, f1 0.7089, precision 0.5490, recall 1.0000, auc 0.8000
epoch 1501, loss 0.3474, train acc 74.27%, f1 0.7179, precision 0.5600, recall 1.0000, auc 0.8087
epoch 2001, loss 0.4369, train acc 75.44%, f1 0.7237, precision 0.5729, recall 0.9821, auc 0.8128
epoch 2501, loss 0.3671, train acc 73.10%, f1 0.7089, precision 0.5490, recall 1.0000, auc 0.8000
epoch 3001, loss 0.3797, train acc 73.68%, f1 0.7097, precision 0.5556, recall 0.9821, auc 0.7998
epoch 3501, loss 0.2887, train acc 73.68%, f1 0.7097, precision 0.5556, recall 0.9821, auc 0.7998
epoch 4001, loss 0.3738, train acc 74.85%, f1 0.7190, precision 0.5670, recall 0.9821, auc 0.8085
epoch 4501, loss 0.3103, train acc 74.27%, f1 0.7143, precision 0.5612, recall 0.9821, auc 0.8041
epoch 5001, loss 0.4041, train acc 78.95%, f1 0.7568, precision 0.6087, recall 1.0000, auc 0.8435
epoch 5501, loss 0.3102, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 6001, loss 0.3214, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 6501, loss 0.2181, train acc 80.70%, f1 0.7724, precision 0.6292, recall 1.0000, auc 0.8565
epoch 7001, loss 0.3275, train acc 81.29%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8609
epoch 7501, loss 0.2938, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 8001, loss 0.2437, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 8501, loss 0.2470, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 9001, loss 0.2292, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 9501, loss 0.3016, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 10001, loss 0.2685, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 10501, loss 0.1927, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 11001, loss 0.2040, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 11501, loss 0.1939, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 12001, loss 0.2482, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 12501, loss 0.2743, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 13001, loss 0.2344, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 13501, loss 0.1830, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 14001, loss 0.1963, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 14501, loss 0.2122, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 15001, loss 0.1972, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 15501, loss 0.2298, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 16001, loss 0.2013, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 16501, loss 0.2294, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 17001, loss 0.1829, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 17501, loss 0.2700, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 18001, loss 0.1832, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 18501, loss 0.2081, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 19001, loss 0.2242, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 19501, loss 0.2385, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 20001, loss 0.2577, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 20501, loss 0.2351, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 21001, loss 0.2328, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 21501, loss 0.2694, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 22001, loss 0.1859, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 22501, loss 0.2226, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 23001, loss 0.2217, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 23501, loss 0.1805, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24001, loss 0.2161, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24501, loss 0.1356, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
running_time is 23.420445196
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.35
normal_0.5
./test_glass0/model_MLP_25000_0.35/record_1/MLP_25000_0.35_3
./test_glass0/result_MLP_25000_0.35_normal_0.5/record_1/
----------------------



the AUC is 0.7216748768472907

the Fscore is 0.631578947368421

the precision is 0.5

the recall is 0.8571428571428571

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.35/record_1/MLP_25000_0.35_4
----------------------



epoch 1, loss 0.8563, train acc 32.75%, f1 0.4934, precision 0.3275, recall 1.0000, auc 0.5000
epoch 501, loss 0.4217, train acc 65.50%, f1 0.6550, precision 0.4870, recall 1.0000, auc 0.7435
epoch 1001, loss 0.3932, train acc 68.42%, f1 0.6707, precision 0.5093, recall 0.9821, auc 0.7606
epoch 1501, loss 0.4184, train acc 69.59%, f1 0.6790, precision 0.5189, recall 0.9821, auc 0.7693
epoch 2001, loss 0.4207, train acc 73.10%, f1 0.7089, precision 0.5490, recall 1.0000, auc 0.8000
epoch 2501, loss 0.3813, train acc 74.27%, f1 0.7143, precision 0.5612, recall 0.9821, auc 0.8041
epoch 3001, loss 0.2817, train acc 75.44%, f1 0.7273, precision 0.5714, recall 1.0000, auc 0.8174
epoch 3501, loss 0.3341, train acc 76.61%, f1 0.7368, precision 0.5833, recall 1.0000, auc 0.8261
epoch 4001, loss 0.2969, train acc 76.61%, f1 0.7368, precision 0.5833, recall 1.0000, auc 0.8261
epoch 4501, loss 0.3602, train acc 78.95%, f1 0.7568, precision 0.6087, recall 1.0000, auc 0.8435
epoch 5001, loss 0.3227, train acc 78.95%, f1 0.7568, precision 0.6087, recall 1.0000, auc 0.8435
epoch 5501, loss 0.3352, train acc 80.70%, f1 0.7724, precision 0.6292, recall 1.0000, auc 0.8565
epoch 6001, loss 0.3692, train acc 81.29%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8609
epoch 6501, loss 0.3132, train acc 81.29%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8609
epoch 7001, loss 0.2891, train acc 82.46%, f1 0.7887, precision 0.6512, recall 1.0000, auc 0.8696
epoch 7501, loss 0.3227, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 8001, loss 0.2734, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 8501, loss 0.2617, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 9001, loss 0.3419, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 9501, loss 0.2923, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 10001, loss 0.3224, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 10501, loss 0.2663, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 11001, loss 0.2530, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 11501, loss 0.2670, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 12001, loss 0.3314, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 12501, loss 0.2791, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 13001, loss 0.3238, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 13501, loss 0.1910, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 14001, loss 0.2622, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 14501, loss 0.1954, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 15001, loss 0.2585, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 15501, loss 0.3142, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 16001, loss 0.2585, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 16501, loss 0.2398, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 17001, loss 0.2629, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 17501, loss 0.3126, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 18001, loss 0.2443, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 18501, loss 0.3004, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 19001, loss 0.2262, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 19501, loss 0.2239, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 20001, loss 0.2849, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 20501, loss 0.2742, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 21001, loss 0.2657, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 21501, loss 0.1845, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 22001, loss 0.2340, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 22501, loss 0.3025, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 23001, loss 0.2943, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 23501, loss 0.3136, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 24001, loss 0.2563, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 24501, loss 0.2736, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
running_time is 23.822691408
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.35
normal_0.5
./test_glass0/model_MLP_25000_0.35/record_1/MLP_25000_0.35_4
./test_glass0/result_MLP_25000_0.35_normal_0.5/record_1/
----------------------



the AUC is 0.5849753694581281

the Fscore is 0.5306122448979592

the precision is 0.37142857142857144

the recall is 0.9285714285714286

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.35/record_1/MLP_25000_0.35_5
----------------------



epoch 1, loss 0.7522, train acc 45.93%, f1 0.5463, precision 0.3758, recall 1.0000, auc 0.5991
epoch 501, loss 0.4059, train acc 67.44%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.7586
epoch 1001, loss 0.3854, train acc 72.09%, f1 0.6962, precision 0.5392, recall 0.9821, auc 0.7885
epoch 1501, loss 0.3745, train acc 78.49%, f1 0.7517, precision 0.6022, recall 1.0000, auc 0.8405
epoch 2001, loss 0.3194, train acc 79.07%, f1 0.7568, precision 0.6087, recall 1.0000, auc 0.8448
epoch 2501, loss 0.3028, train acc 80.23%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8534
epoch 3001, loss 0.3406, train acc 81.40%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8621
epoch 3501, loss 0.3659, train acc 81.40%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8621
epoch 4001, loss 0.3160, train acc 81.98%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8664
epoch 4501, loss 0.3317, train acc 82.56%, f1 0.7887, precision 0.6512, recall 1.0000, auc 0.8707
epoch 5001, loss 0.3411, train acc 82.56%, f1 0.7887, precision 0.6512, recall 1.0000, auc 0.8707
epoch 5501, loss 0.2833, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 6001, loss 0.3836, train acc 83.72%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8793
epoch 6501, loss 0.2366, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 7001, loss 0.3514, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 7501, loss 0.3062, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 8001, loss 0.3445, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 8501, loss 0.2731, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 9001, loss 0.2713, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 9501, loss 0.3201, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 10001, loss 0.3419, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 10501, loss 0.2742, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 11001, loss 0.3423, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 11501, loss 0.2952, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 12001, loss 0.3275, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 12501, loss 0.2298, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 13001, loss 0.3147, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 13501, loss 0.3651, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 14001, loss 0.3296, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 14501, loss 0.2897, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 15001, loss 0.3271, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 15501, loss 0.2603, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 16001, loss 0.3262, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 16501, loss 0.3636, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 17001, loss 0.2824, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 17501, loss 0.2992, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 18001, loss 0.3046, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 18501, loss 0.3024, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 19001, loss 0.2994, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 19501, loss 0.3898, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 20001, loss 0.2750, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 20501, loss 0.3014, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 21001, loss 0.3353, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 21501, loss 0.3266, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 22001, loss 0.2157, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 22501, loss 0.2774, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 23001, loss 0.3504, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 23501, loss 0.3456, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 24001, loss 0.3624, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 24501, loss 0.3054, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
running_time is 23.273234564
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.35
normal_0.5
./test_glass0/model_MLP_25000_0.35/record_1/MLP_25000_0.35_5
./test_glass0/result_MLP_25000_0.35_normal_0.5/record_1/
----------------------



the AUC is 0.75

the Fscore is 0.6666666666666666

the precision is 0.52

the recall is 0.9285714285714286

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.36/record_1/MLP_25000_0.36_1
----------------------



epoch 1, loss 0.9859, train acc 46.78%, f1 0.5517, precision 0.3810, recall 1.0000, auc 0.6043
epoch 501, loss 0.4213, train acc 69.01%, f1 0.6788, precision 0.5138, recall 1.0000, auc 0.7696
epoch 1001, loss 0.4082, train acc 76.61%, f1 0.7368, precision 0.5833, recall 1.0000, auc 0.8261
epoch 1501, loss 0.4274, train acc 79.53%, f1 0.7619, precision 0.6154, recall 1.0000, auc 0.8478
epoch 2001, loss 0.3993, train acc 80.70%, f1 0.7724, precision 0.6292, recall 1.0000, auc 0.8565
epoch 2501, loss 0.2257, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 3001, loss 0.3247, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 3501, loss 0.2945, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 4001, loss 0.3396, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 4501, loss 0.3542, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 5001, loss 0.2130, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 5501, loss 0.3376, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 6001, loss 0.3390, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 6501, loss 0.2774, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 7001, loss 0.3684, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 7501, loss 0.2731, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 8001, loss 0.2034, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 8501, loss 0.2800, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 9001, loss 0.1801, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 9501, loss 0.2078, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 10001, loss 0.2889, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 10501, loss 0.2686, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 11001, loss 0.2105, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11501, loss 0.1840, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 12001, loss 0.2223, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 12501, loss 0.2411, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 13001, loss 0.2497, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 13501, loss 0.2647, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 14001, loss 0.2270, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 14501, loss 0.2629, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 15001, loss 0.1748, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 15501, loss 0.2084, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 16001, loss 0.2504, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 16501, loss 0.1958, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 17001, loss 0.1562, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 17501, loss 0.2022, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 18001, loss 0.1657, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 18501, loss 0.2651, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 19001, loss 0.2247, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 19501, loss 0.2458, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 20001, loss 0.1831, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 20501, loss 0.2364, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 21001, loss 0.2078, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 21501, loss 0.2414, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 22001, loss 0.2031, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 22501, loss 0.2266, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 23001, loss 0.2282, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 23501, loss 0.1447, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24001, loss 0.2371, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24501, loss 0.2154, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
running_time is 23.638987051
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.36
normal_0.5
./test_glass0/model_MLP_25000_0.36/record_1/MLP_25000_0.36_1
./test_glass0/result_MLP_25000_0.36_normal_0.5/record_1/
----------------------



the AUC is 0.4334975369458128

the Fscore is 0.16666666666666666

the precision is 0.2

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.36/record_1/MLP_25000_0.36_2
----------------------



epoch 1, loss 0.6230, train acc 32.75%, f1 0.4934, precision 0.3275, recall 1.0000, auc 0.5000
epoch 501, loss 0.3977, train acc 67.84%, f1 0.6707, precision 0.5045, recall 1.0000, auc 0.7609
epoch 1001, loss 0.3054, train acc 74.27%, f1 0.7179, precision 0.5600, recall 1.0000, auc 0.8087
epoch 1501, loss 0.2656, train acc 77.78%, f1 0.7467, precision 0.5957, recall 1.0000, auc 0.8348
epoch 2001, loss 0.3427, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 2501, loss 0.3030, train acc 81.87%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8652
epoch 3001, loss 0.2935, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 3501, loss 0.2708, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 4001, loss 0.1992, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 4501, loss 0.2825, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 5001, loss 0.2374, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 5501, loss 0.2929, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 6001, loss 0.1946, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 6501, loss 0.2385, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 7001, loss 0.2167, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 7501, loss 0.2467, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 8001, loss 0.2172, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 8501, loss 0.2826, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 9001, loss 0.2485, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 9501, loss 0.2146, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 10001, loss 0.2203, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 10501, loss 0.2245, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 11001, loss 0.2278, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 11501, loss 0.1663, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 12001, loss 0.1545, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 12501, loss 0.1886, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13001, loss 0.1800, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13501, loss 0.2189, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14001, loss 0.1763, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14501, loss 0.2051, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15001, loss 0.2188, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15501, loss 0.2158, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16001, loss 0.2043, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16501, loss 0.2039, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17001, loss 0.2580, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17501, loss 0.1675, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 18001, loss 0.2415, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18501, loss 0.1980, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 19001, loss 0.1857, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 19501, loss 0.1844, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 20001, loss 0.2002, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 20501, loss 0.2164, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 21001, loss 0.2108, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 21501, loss 0.1566, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 22001, loss 0.2538, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 22501, loss 0.2279, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 23001, loss 0.1551, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 23501, loss 0.1999, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 24001, loss 0.2052, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 24501, loss 0.1706, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
running_time is 24.095268836
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.36
normal_0.5
./test_glass0/model_MLP_25000_0.36/record_1/MLP_25000_0.36_2
./test_glass0/result_MLP_25000_0.36_normal_0.5/record_1/
----------------------



the AUC is 0.645320197044335

the Fscore is 0.5

the precision is 0.6

the recall is 0.42857142857142855

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.36/record_1/MLP_25000_0.36_3
----------------------



epoch 1, loss 0.8768, train acc 32.75%, f1 0.4934, precision 0.3275, recall 1.0000, auc 0.5000
epoch 501, loss 0.3997, train acc 67.84%, f1 0.6707, precision 0.5045, recall 1.0000, auc 0.7609
epoch 1001, loss 0.3887, train acc 71.93%, f1 0.7000, precision 0.5385, recall 1.0000, auc 0.7913
epoch 1501, loss 0.4196, train acc 73.68%, f1 0.7097, precision 0.5556, recall 0.9821, auc 0.7998
epoch 2001, loss 0.3245, train acc 71.93%, f1 0.7000, precision 0.5385, recall 1.0000, auc 0.7913
epoch 2501, loss 0.3128, train acc 73.10%, f1 0.7051, precision 0.5500, recall 0.9821, auc 0.7954
epoch 3001, loss 0.4940, train acc 73.10%, f1 0.7051, precision 0.5500, recall 0.9821, auc 0.7954
epoch 3501, loss 0.3912, train acc 73.68%, f1 0.7134, precision 0.5545, recall 1.0000, auc 0.8043
epoch 4001, loss 0.4121, train acc 74.85%, f1 0.7152, precision 0.5684, recall 0.9643, auc 0.8039
epoch 4501, loss 0.3289, train acc 75.44%, f1 0.7237, precision 0.5729, recall 0.9821, auc 0.8128
epoch 5001, loss 0.3516, train acc 73.68%, f1 0.7134, precision 0.5545, recall 1.0000, auc 0.8043
epoch 5501, loss 0.4580, train acc 75.44%, f1 0.7273, precision 0.5714, recall 1.0000, auc 0.8174
epoch 6001, loss 0.3840, train acc 73.68%, f1 0.7134, precision 0.5545, recall 1.0000, auc 0.8043
epoch 6501, loss 0.3755, train acc 74.27%, f1 0.7143, precision 0.5612, recall 0.9821, auc 0.8041
epoch 7001, loss 0.3305, train acc 75.44%, f1 0.7273, precision 0.5714, recall 1.0000, auc 0.8174
epoch 7501, loss 0.3830, train acc 75.44%, f1 0.7200, precision 0.5745, recall 0.9643, auc 0.8082
epoch 8001, loss 0.4134, train acc 74.27%, f1 0.7179, precision 0.5600, recall 1.0000, auc 0.8087
epoch 8501, loss 0.3840, train acc 74.27%, f1 0.7143, precision 0.5612, recall 0.9821, auc 0.8041
epoch 9001, loss 0.3097, train acc 77.78%, f1 0.7467, precision 0.5957, recall 1.0000, auc 0.8348
epoch 9501, loss 0.3412, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 10001, loss 0.2838, train acc 80.70%, f1 0.7724, precision 0.6292, recall 1.0000, auc 0.8565
epoch 10501, loss 0.2806, train acc 81.29%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8609
epoch 11001, loss 0.2836, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 11501, loss 0.2843, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 12001, loss 0.2597, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 12501, loss 0.3094, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 13001, loss 0.2720, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 13501, loss 0.3107, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 14001, loss 0.3169, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 14501, loss 0.3049, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 15001, loss 0.2558, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 15501, loss 0.2573, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 16001, loss 0.2772, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 16501, loss 0.2934, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 17001, loss 0.2489, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 17501, loss 0.2726, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 18001, loss 0.2612, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 18501, loss 0.2465, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 19001, loss 0.2868, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 19501, loss 0.2525, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 20001, loss 0.2714, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 20501, loss 0.1907, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 21001, loss 0.2920, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 21501, loss 0.3188, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 22001, loss 0.2128, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 22501, loss 0.2093, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 23001, loss 0.2366, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 23501, loss 0.2590, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 24001, loss 0.1984, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 24501, loss 0.3042, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
running_time is 23.31622401
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.36
normal_0.5
./test_glass0/model_MLP_25000_0.36/record_1/MLP_25000_0.36_3
./test_glass0/result_MLP_25000_0.36_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.49122807017543857

the precision is 0.32558139534883723

the recall is 1.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.36/record_1/MLP_25000_0.36_4
----------------------



epoch 1, loss 0.7681, train acc 33.33%, f1 0.4956, precision 0.3294, recall 1.0000, auc 0.5043
epoch 501, loss 0.4151, train acc 64.33%, f1 0.6474, precision 0.4786, recall 1.0000, auc 0.7348
epoch 1001, loss 0.3650, train acc 69.59%, f1 0.6790, precision 0.5189, recall 0.9821, auc 0.7693
epoch 1501, loss 0.3605, train acc 73.10%, f1 0.7051, precision 0.5500, recall 0.9821, auc 0.7954
epoch 2001, loss 0.3435, train acc 74.85%, f1 0.7226, precision 0.5657, recall 1.0000, auc 0.8130
epoch 2501, loss 0.4551, train acc 76.02%, f1 0.7320, precision 0.5773, recall 1.0000, auc 0.8217
epoch 3001, loss 0.3774, train acc 77.78%, f1 0.7432, precision 0.5978, recall 0.9821, auc 0.8302
epoch 3501, loss 0.3365, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 4001, loss 0.3355, train acc 81.87%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8652
epoch 4501, loss 0.3270, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 5001, loss 0.3427, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 5501, loss 0.2986, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 6001, loss 0.2558, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 6501, loss 0.2992, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 7001, loss 0.3394, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 7501, loss 0.3254, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 8001, loss 0.2862, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 8501, loss 0.2684, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 9001, loss 0.2473, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 9501, loss 0.2700, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 10001, loss 0.1806, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 10501, loss 0.3179, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 11001, loss 0.2347, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 11501, loss 0.2510, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 12001, loss 0.2680, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 12501, loss 0.2483, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 13001, loss 0.2788, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 13501, loss 0.3215, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 14001, loss 0.2766, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 14501, loss 0.1801, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 15001, loss 0.2351, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 15501, loss 0.1864, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 16001, loss 0.2890, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 16501, loss 0.2640, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 17001, loss 0.2919, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 17501, loss 0.2968, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 18001, loss 0.2332, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 18501, loss 0.3116, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 19001, loss 0.2329, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 19501, loss 0.2193, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 20001, loss 0.2440, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 20501, loss 0.2673, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 21001, loss 0.2065, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 21501, loss 0.2567, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 22001, loss 0.2065, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 22501, loss 0.2344, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 23001, loss 0.2404, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 23501, loss 0.2122, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 24001, loss 0.2123, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 24501, loss 0.1965, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
running_time is 23.413396823
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.36
normal_0.5
./test_glass0/model_MLP_25000_0.36/record_1/MLP_25000_0.36_4
./test_glass0/result_MLP_25000_0.36_normal_0.5/record_1/
----------------------



the AUC is 0.7056650246305418

the Fscore is 0.6190476190476191

the precision is 0.4642857142857143

the recall is 0.9285714285714286

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.36/record_1/MLP_25000_0.36_5
----------------------



epoch 1, loss 0.6229, train acc 50.00%, f1 0.5426, precision 0.3864, recall 0.9107, auc 0.6062
epoch 501, loss 0.4267, train acc 68.02%, f1 0.6707, precision 0.5045, recall 1.0000, auc 0.7629
epoch 1001, loss 0.3504, train acc 75.58%, f1 0.7273, precision 0.5714, recall 1.0000, auc 0.8190
epoch 1501, loss 0.3384, train acc 78.49%, f1 0.7517, precision 0.6022, recall 1.0000, auc 0.8405
epoch 2001, loss 0.3407, train acc 80.23%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8534
epoch 2501, loss 0.3287, train acc 81.40%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8621
epoch 3001, loss 0.3348, train acc 81.98%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8664
epoch 3501, loss 0.2521, train acc 81.98%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8664
epoch 4001, loss 0.3223, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 4501, loss 0.3437, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 5001, loss 0.2957, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 5501, loss 0.3436, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 6001, loss 0.3027, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 6501, loss 0.2883, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 7001, loss 0.2859, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 7501, loss 0.2998, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 8001, loss 0.2896, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 8501, loss 0.3465, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 9001, loss 0.3214, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 9501, loss 0.2193, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 10001, loss 0.2824, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 10501, loss 0.3355, train acc 86.05%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8966
epoch 11001, loss 0.2903, train acc 86.05%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8966
epoch 11501, loss 0.2623, train acc 86.05%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8966
epoch 12001, loss 0.3512, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 12501, loss 0.2959, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 13001, loss 0.3466, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 13501, loss 0.2880, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 14001, loss 0.2112, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 14501, loss 0.2842, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 15001, loss 0.2854, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 15501, loss 0.3004, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 16001, loss 0.2796, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 16501, loss 0.2441, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 17001, loss 0.2796, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 17501, loss 0.2805, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 18001, loss 0.2988, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 18501, loss 0.2425, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 19001, loss 0.2778, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 19501, loss 0.2205, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 20001, loss 0.2087, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 20501, loss 0.2628, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 21001, loss 0.2798, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 21501, loss 0.2190, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 22001, loss 0.2682, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 22501, loss 0.2516, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 23001, loss 0.2770, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 23501, loss 0.2245, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 24001, loss 0.2742, train acc 88.95%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9181
epoch 24501, loss 0.2452, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
running_time is 23.482040326
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.36
normal_0.5
./test_glass0/model_MLP_25000_0.36/record_1/MLP_25000_0.36_5
./test_glass0/result_MLP_25000_0.36_normal_0.5/record_1/
----------------------



the AUC is 0.48214285714285715

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.37/record_1/MLP_25000_0.37_1
----------------------



epoch 1, loss 0.7473, train acc 32.75%, f1 0.4934, precision 0.3275, recall 1.0000, auc 0.5000
epoch 501, loss 0.3873, train acc 67.25%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.7565
epoch 1001, loss 0.3333, train acc 76.02%, f1 0.7248, precision 0.5806, recall 0.9643, auc 0.8126
epoch 1501, loss 0.3475, train acc 77.78%, f1 0.7432, precision 0.5978, recall 0.9821, auc 0.8302
epoch 2001, loss 0.3373, train acc 78.36%, f1 0.7517, precision 0.6022, recall 1.0000, auc 0.8391
epoch 2501, loss 0.2930, train acc 78.95%, f1 0.7568, precision 0.6087, recall 1.0000, auc 0.8435
epoch 3001, loss 0.2451, train acc 81.29%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8609
epoch 3501, loss 0.3486, train acc 82.46%, f1 0.7887, precision 0.6512, recall 1.0000, auc 0.8696
epoch 4001, loss 0.2567, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 4501, loss 0.3186, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 5001, loss 0.3517, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 5501, loss 0.3153, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 6001, loss 0.2956, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 6501, loss 0.3822, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 7001, loss 0.3300, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 7501, loss 0.2865, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 8001, loss 0.2520, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 8501, loss 0.3545, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 9001, loss 0.3313, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 9501, loss 0.3552, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 10001, loss 0.2969, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 10501, loss 0.3402, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 11001, loss 0.2213, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 11501, loss 0.2995, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 12001, loss 0.2687, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 12501, loss 0.2951, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 13001, loss 0.3076, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 13501, loss 0.3212, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 14001, loss 0.2812, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 14501, loss 0.2459, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 15001, loss 0.2495, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 15501, loss 0.2467, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 16001, loss 0.2846, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 16501, loss 0.2748, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 17001, loss 0.2848, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 17501, loss 0.2291, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 18001, loss 0.2924, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 18501, loss 0.2644, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 19001, loss 0.3119, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 19501, loss 0.1990, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 20001, loss 0.2832, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 20501, loss 0.3093, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 21001, loss 0.2021, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 21501, loss 0.2861, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 22001, loss 0.2622, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 22501, loss 0.2136, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 23001, loss 0.2638, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 23501, loss 0.2713, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 24001, loss 0.3551, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 24501, loss 0.2175, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
running_time is 23.377673735
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.37
normal_0.5
./test_glass0/model_MLP_25000_0.37/record_1/MLP_25000_0.37_1
./test_glass0/result_MLP_25000_0.37_normal_0.5/record_1/
----------------------



the AUC is 0.6896551724137931

the Fscore is 0.6086956521739131

the precision is 0.4375

the recall is 1.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.37/record_1/MLP_25000_0.37_2
----------------------



epoch 1, loss 0.8609, train acc 47.95%, f1 0.5572, precision 0.3862, recall 1.0000, auc 0.6130
epoch 501, loss 0.4913, train acc 68.42%, f1 0.6747, precision 0.5091, recall 1.0000, auc 0.7652
epoch 1001, loss 0.4139, train acc 73.10%, f1 0.7089, precision 0.5490, recall 1.0000, auc 0.8000
epoch 1501, loss 0.3569, train acc 78.95%, f1 0.7568, precision 0.6087, recall 1.0000, auc 0.8435
epoch 2001, loss 0.4282, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 2501, loss 0.3374, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 3001, loss 0.3459, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 3501, loss 0.3318, train acc 80.70%, f1 0.7724, precision 0.6292, recall 1.0000, auc 0.8565
epoch 4001, loss 0.3491, train acc 80.70%, f1 0.7724, precision 0.6292, recall 1.0000, auc 0.8565
epoch 4501, loss 0.3265, train acc 80.70%, f1 0.7724, precision 0.6292, recall 1.0000, auc 0.8565
epoch 5001, loss 0.3772, train acc 81.87%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8652
epoch 5501, loss 0.3090, train acc 81.87%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8652
epoch 6001, loss 0.3705, train acc 82.46%, f1 0.7887, precision 0.6512, recall 1.0000, auc 0.8696
epoch 6501, loss 0.3566, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 7001, loss 0.3697, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 7501, loss 0.2827, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 8001, loss 0.3812, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 8501, loss 0.3425, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 9001, loss 0.3812, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 9501, loss 0.2926, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 10001, loss 0.2818, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 10501, loss 0.2729, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 11001, loss 0.2953, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 11501, loss 0.2093, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 12001, loss 0.2766, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 12501, loss 0.2083, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 13001, loss 0.2024, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 13501, loss 0.1501, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 14001, loss 0.1863, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 14501, loss 0.2008, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 15001, loss 0.2316, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 15501, loss 0.2463, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 16001, loss 0.2214, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 16501, loss 0.1818, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 17001, loss 0.1869, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 17501, loss 0.2083, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 18001, loss 0.1697, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18501, loss 0.2375, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19001, loss 0.1968, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 19501, loss 0.2205, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 20001, loss 0.1897, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 20501, loss 0.2130, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21001, loss 0.2088, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21501, loss 0.2010, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 22001, loss 0.1739, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 22501, loss 0.1667, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 23001, loss 0.1590, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 23501, loss 0.1782, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 24001, loss 0.1731, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 24501, loss 0.1689, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
running_time is 23.827957195
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.37
normal_0.5
./test_glass0/model_MLP_25000_0.37/record_1/MLP_25000_0.37_2
./test_glass0/result_MLP_25000_0.37_normal_0.5/record_1/
----------------------



the AUC is 0.7179802955665024

the Fscore is 0.6206896551724138

the precision is 0.6

the recall is 0.6428571428571429

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.37/record_1/MLP_25000_0.37_3
----------------------



epoch 1, loss 1.0138, train acc 46.78%, f1 0.5517, precision 0.3810, recall 1.0000, auc 0.6043
epoch 501, loss 0.3770, train acc 69.01%, f1 0.6788, precision 0.5138, recall 1.0000, auc 0.7696
epoch 1001, loss 0.3410, train acc 74.27%, f1 0.7179, precision 0.5600, recall 1.0000, auc 0.8087
epoch 1501, loss 0.3574, train acc 74.27%, f1 0.7179, precision 0.5600, recall 1.0000, auc 0.8087
epoch 2001, loss 0.4355, train acc 77.78%, f1 0.7467, precision 0.5957, recall 1.0000, auc 0.8348
epoch 2501, loss 0.3646, train acc 80.12%, f1 0.7639, precision 0.6250, recall 0.9821, auc 0.8476
epoch 3001, loss 0.3366, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 3501, loss 0.3447, train acc 82.46%, f1 0.7887, precision 0.6512, recall 1.0000, auc 0.8696
epoch 4001, loss 0.3697, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 4501, loss 0.2843, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 5001, loss 0.2512, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 5501, loss 0.2087, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 6001, loss 0.2491, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 6501, loss 0.2632, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 7001, loss 0.2074, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 7501, loss 0.2434, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 8001, loss 0.2563, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 8501, loss 0.2046, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 9001, loss 0.2119, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 9501, loss 0.2453, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 10001, loss 0.2645, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10501, loss 0.2870, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11001, loss 0.2366, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11501, loss 0.2105, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 12001, loss 0.2276, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 12501, loss 0.2298, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 13001, loss 0.2019, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 13501, loss 0.2864, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 14001, loss 0.1938, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 14501, loss 0.2397, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 15001, loss 0.2092, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15501, loss 0.2199, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16001, loss 0.2029, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16501, loss 0.2027, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17001, loss 0.1606, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17501, loss 0.2182, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18001, loss 0.1974, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18501, loss 0.1816, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19001, loss 0.2427, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19501, loss 0.2442, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 20001, loss 0.1887, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 20501, loss 0.2257, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 21001, loss 0.2465, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 21501, loss 0.1650, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 22001, loss 0.1701, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 22501, loss 0.2056, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 23001, loss 0.2179, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 23501, loss 0.1781, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 24001, loss 0.1806, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 24501, loss 0.2319, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
running_time is 23.314531051
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.37
normal_0.5
./test_glass0/model_MLP_25000_0.37/record_1/MLP_25000_0.37_3
./test_glass0/result_MLP_25000_0.37_normal_0.5/record_1/
----------------------



the AUC is 0.625615763546798

the Fscore is 0.4210526315789473

the precision is 0.8

the recall is 0.2857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.37/record_1/MLP_25000_0.37_4
----------------------



epoch 1, loss 0.8236, train acc 43.27%, f1 0.5359, precision 0.3660, recall 1.0000, auc 0.5783
epoch 501, loss 0.3743, train acc 67.25%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.7565
epoch 1001, loss 0.2995, train acc 73.10%, f1 0.7089, precision 0.5490, recall 1.0000, auc 0.8000
epoch 1501, loss 0.4946, train acc 74.85%, f1 0.7226, precision 0.5657, recall 1.0000, auc 0.8130
epoch 2001, loss 0.2861, train acc 77.78%, f1 0.7467, precision 0.5957, recall 1.0000, auc 0.8348
epoch 2501, loss 0.2984, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 3001, loss 0.2392, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 3501, loss 0.2943, train acc 81.87%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8652
epoch 4001, loss 0.3211, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 4501, loss 0.2453, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 5001, loss 0.3418, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 5501, loss 0.2727, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 6001, loss 0.2937, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 6501, loss 0.2918, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 7001, loss 0.2551, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 7501, loss 0.2569, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 8001, loss 0.2573, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 8501, loss 0.2008, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 9001, loss 0.3118, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 9501, loss 0.2702, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 10001, loss 0.3078, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 10501, loss 0.3019, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 11001, loss 0.2533, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 11501, loss 0.3261, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 12001, loss 0.2865, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 12501, loss 0.2880, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 13001, loss 0.2951, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 13501, loss 0.2064, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 14001, loss 0.2745, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 14501, loss 0.2200, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 15001, loss 0.2236, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 15501, loss 0.2504, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 16001, loss 0.3140, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 16501, loss 0.2996, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 17001, loss 0.2013, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 17501, loss 0.2389, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 18001, loss 0.2732, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 18501, loss 0.2390, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 19001, loss 0.3464, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 19501, loss 0.2559, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 20001, loss 0.2741, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 20501, loss 0.2689, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 21001, loss 0.2117, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 21501, loss 0.2681, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 22001, loss 0.2298, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 22501, loss 0.1755, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 23001, loss 0.2833, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 23501, loss 0.1879, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 24001, loss 0.2433, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 24501, loss 0.2208, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
running_time is 23.331244007
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.37
normal_0.5
./test_glass0/model_MLP_25000_0.37/record_1/MLP_25000_0.37_4
./test_glass0/result_MLP_25000_0.37_normal_0.5/record_1/
----------------------



the AUC is 0.5738916256157635

the Fscore is 0.36363636363636365

the precision is 0.5

the recall is 0.2857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.37/record_1/MLP_25000_0.37_5
----------------------



epoch 1, loss 0.9763, train acc 48.26%, f1 0.5436, precision 0.3813, recall 0.9464, auc 0.6025
epoch 501, loss 0.3861, train acc 67.44%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.7586
epoch 1001, loss 0.4121, train acc 75.58%, f1 0.7273, precision 0.5714, recall 1.0000, auc 0.8190
epoch 1501, loss 0.3816, train acc 77.33%, f1 0.7417, precision 0.5895, recall 1.0000, auc 0.8319
epoch 2001, loss 0.3282, train acc 79.65%, f1 0.7619, precision 0.6154, recall 1.0000, auc 0.8491
epoch 2501, loss 0.3443, train acc 80.81%, f1 0.7724, precision 0.6292, recall 1.0000, auc 0.8578
epoch 3001, loss 0.3366, train acc 80.81%, f1 0.7724, precision 0.6292, recall 1.0000, auc 0.8578
epoch 3501, loss 0.2825, train acc 81.40%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8621
epoch 4001, loss 0.2767, train acc 82.56%, f1 0.7887, precision 0.6512, recall 1.0000, auc 0.8707
epoch 4501, loss 0.2790, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 5001, loss 0.3109, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 5501, loss 0.3210, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 6001, loss 0.3473, train acc 83.72%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8793
epoch 6501, loss 0.3631, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 7001, loss 0.3125, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 7501, loss 0.2954, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 8001, loss 0.3202, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 8501, loss 0.2525, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 9001, loss 0.2870, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 9501, loss 0.2370, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 10001, loss 0.2302, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 10501, loss 0.3105, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 11001, loss 0.3381, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 11501, loss 0.2222, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 12001, loss 0.3019, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 12501, loss 0.2763, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 13001, loss 0.2972, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 13501, loss 0.2634, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 14001, loss 0.2494, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 14501, loss 0.2077, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 15001, loss 0.3283, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 15501, loss 0.2480, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 16001, loss 0.2420, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 16501, loss 0.2579, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 17001, loss 0.2622, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 17501, loss 0.2580, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 18001, loss 0.2694, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 18501, loss 0.3073, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 19001, loss 0.2575, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 19501, loss 0.2780, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 20001, loss 0.2739, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 20501, loss 0.2959, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 21001, loss 0.2688, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 21501, loss 0.3220, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 22001, loss 0.3081, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 22501, loss 0.3488, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 23001, loss 0.2542, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 23501, loss 0.2487, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 24001, loss 0.2799, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 24501, loss 0.2771, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
running_time is 23.528002486000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.37
normal_0.5
./test_glass0/model_MLP_25000_0.37/record_1/MLP_25000_0.37_5
./test_glass0/result_MLP_25000_0.37_normal_0.5/record_1/
----------------------



the AUC is 0.6428571428571429

the Fscore is 0.4761904761904762

the precision is 0.7142857142857143

the recall is 0.35714285714285715

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.38/record_1/MLP_25000_0.38_1
----------------------



epoch 1, loss 0.5662, train acc 34.50%, f1 0.5000, precision 0.3333, recall 1.0000, auc 0.5130
epoch 501, loss 0.4053, train acc 68.42%, f1 0.6747, precision 0.5091, recall 1.0000, auc 0.7652
epoch 1001, loss 0.3625, train acc 76.61%, f1 0.7368, precision 0.5833, recall 1.0000, auc 0.8261
epoch 1501, loss 0.3564, train acc 77.19%, f1 0.7417, precision 0.5895, recall 1.0000, auc 0.8304
epoch 2001, loss 0.4052, train acc 81.87%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8652
epoch 2501, loss 0.3459, train acc 82.46%, f1 0.7887, precision 0.6512, recall 1.0000, auc 0.8696
epoch 3001, loss 0.2578, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 3501, loss 0.2878, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 4001, loss 0.2830, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 4501, loss 0.3232, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 5001, loss 0.3223, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 5501, loss 0.3730, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 6001, loss 0.2503, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 6501, loss 0.3239, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 7001, loss 0.3131, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 7501, loss 0.2523, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 8001, loss 0.3096, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 8501, loss 0.2527, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 9001, loss 0.2145, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 9501, loss 0.3139, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 10001, loss 0.2733, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 10501, loss 0.2324, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 11001, loss 0.2331, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 11501, loss 0.2545, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 12001, loss 0.2711, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 12501, loss 0.2306, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 13001, loss 0.2499, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 13501, loss 0.2033, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 14001, loss 0.2741, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 14501, loss 0.2380, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 15001, loss 0.1951, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 15501, loss 0.2504, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 16001, loss 0.2507, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 16501, loss 0.2538, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 17001, loss 0.2069, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 17501, loss 0.2611, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 18001, loss 0.3013, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 18501, loss 0.2326, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 19001, loss 0.2717, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 19501, loss 0.2142, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 20001, loss 0.2799, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 20501, loss 0.2519, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 21001, loss 0.2511, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 21501, loss 0.2402, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 22001, loss 0.2406, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 22501, loss 0.2040, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 23001, loss 0.2219, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 23501, loss 0.1730, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 24001, loss 0.2018, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 24501, loss 0.1924, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
running_time is 23.544812982
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.38
normal_0.5
./test_glass0/model_MLP_25000_0.38/record_1/MLP_25000_0.38_1
./test_glass0/result_MLP_25000_0.38_normal_0.5/record_1/
----------------------



the AUC is 0.522167487684729

the Fscore is 0.32

the precision is 0.36363636363636365

the recall is 0.2857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.38/record_1/MLP_25000_0.38_2
----------------------



epoch 1, loss 0.7653, train acc 44.44%, f1 0.5411, precision 0.3709, recall 1.0000, auc 0.5870
epoch 501, loss 0.3828, train acc 67.84%, f1 0.6707, precision 0.5045, recall 1.0000, auc 0.7609
epoch 1001, loss 0.3963, train acc 76.61%, f1 0.7368, precision 0.5833, recall 1.0000, auc 0.8261
epoch 1501, loss 0.3263, train acc 79.53%, f1 0.7619, precision 0.6154, recall 1.0000, auc 0.8478
epoch 2001, loss 0.3062, train acc 81.87%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8652
epoch 2501, loss 0.3484, train acc 82.46%, f1 0.7887, precision 0.6512, recall 1.0000, auc 0.8696
epoch 3001, loss 0.2724, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 3501, loss 0.3011, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 4001, loss 0.2780, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 4501, loss 0.2803, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 5001, loss 0.2894, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 5501, loss 0.2930, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 6001, loss 0.2295, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 6501, loss 0.2444, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 7001, loss 0.3185, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 7501, loss 0.1894, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 8001, loss 0.2520, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8501, loss 0.1767, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 9001, loss 0.2112, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 9501, loss 0.2326, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 10001, loss 0.1794, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 10501, loss 0.2026, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 11001, loss 0.2420, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 11501, loss 0.1716, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 12001, loss 0.1843, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 12501, loss 0.1784, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 13001, loss 0.1985, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 13501, loss 0.2025, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 14001, loss 0.2224, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 14501, loss 0.2239, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 15001, loss 0.2096, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 15501, loss 0.2565, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 16001, loss 0.2322, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 16501, loss 0.1658, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 17001, loss 0.2043, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 17501, loss 0.1622, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 18001, loss 0.1659, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 18501, loss 0.2440, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19001, loss 0.1617, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19501, loss 0.2414, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 20001, loss 0.1856, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 20501, loss 0.1969, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 21001, loss 0.2324, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 21501, loss 0.1727, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 22001, loss 0.2024, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 22501, loss 0.2121, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 23001, loss 0.2397, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 23501, loss 0.2021, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 24001, loss 0.1832, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 24501, loss 0.2245, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
running_time is 23.709268626
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.38
normal_0.5
./test_glass0/model_MLP_25000_0.38/record_1/MLP_25000_0.38_2
./test_glass0/result_MLP_25000_0.38_normal_0.5/record_1/
----------------------



the AUC is 0.4839901477832512

the Fscore is 0.11111111111111112

the precision is 0.25

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.38/record_1/MLP_25000_0.38_3
----------------------



epoch 1, loss 0.7655, train acc 35.09%, f1 0.5022, precision 0.3353, recall 1.0000, auc 0.5174
epoch 501, loss 0.4448, train acc 67.84%, f1 0.6707, precision 0.5045, recall 1.0000, auc 0.7609
epoch 1001, loss 0.4274, train acc 72.51%, f1 0.7044, precision 0.5437, recall 1.0000, auc 0.7957
epoch 1501, loss 0.3225, train acc 73.68%, f1 0.7134, precision 0.5545, recall 1.0000, auc 0.8043
epoch 2001, loss 0.3115, train acc 76.61%, f1 0.7368, precision 0.5833, recall 1.0000, auc 0.8261
epoch 2501, loss 0.3775, train acc 76.61%, f1 0.7368, precision 0.5833, recall 1.0000, auc 0.8261
epoch 3001, loss 0.3211, train acc 77.78%, f1 0.7467, precision 0.5957, recall 1.0000, auc 0.8348
epoch 3501, loss 0.3333, train acc 80.12%, f1 0.7639, precision 0.6250, recall 0.9821, auc 0.8476
epoch 4001, loss 0.3819, train acc 78.36%, f1 0.7517, precision 0.6022, recall 1.0000, auc 0.8391
epoch 4501, loss 0.3267, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 5001, loss 0.3434, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 5501, loss 0.2856, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 6001, loss 0.3499, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 6501, loss 0.2993, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 7001, loss 0.2616, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 7501, loss 0.2569, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 8001, loss 0.2196, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 8501, loss 0.2436, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 9001, loss 0.2716, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 9501, loss 0.2952, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 10001, loss 0.3061, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 10501, loss 0.2660, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 11001, loss 0.2822, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 11501, loss 0.2608, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 12001, loss 0.2355, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 12501, loss 0.2025, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 13001, loss 0.2676, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 13501, loss 0.2165, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 14001, loss 0.2608, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 14501, loss 0.3322, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 15001, loss 0.1864, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 15501, loss 0.2358, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 16001, loss 0.2805, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 16501, loss 0.1749, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 17001, loss 0.2286, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 17501, loss 0.2195, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 18001, loss 0.2181, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 18501, loss 0.2248, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 19001, loss 0.3014, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 19501, loss 0.2528, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 20001, loss 0.2132, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 20501, loss 0.2801, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 21001, loss 0.1911, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 21501, loss 0.2230, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 22001, loss 0.2608, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 22501, loss 0.2025, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 23001, loss 0.2972, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 23501, loss 0.2898, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 24001, loss 0.1922, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 24501, loss 0.2320, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
running_time is 23.471036069
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.38
normal_0.5
./test_glass0/model_MLP_25000_0.38/record_1/MLP_25000_0.38_3
./test_glass0/result_MLP_25000_0.38_normal_0.5/record_1/
----------------------



the AUC is 0.625615763546798

the Fscore is 0.4210526315789473

the precision is 0.8

the recall is 0.2857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.38/record_1/MLP_25000_0.38_4
----------------------



epoch 1, loss 0.8842, train acc 36.26%, f1 0.5068, precision 0.3394, recall 1.0000, auc 0.5261
epoch 501, loss 0.3542, train acc 65.50%, f1 0.6550, precision 0.4870, recall 1.0000, auc 0.7435
epoch 1001, loss 0.3715, train acc 70.18%, f1 0.6871, precision 0.5234, recall 1.0000, auc 0.7783
epoch 1501, loss 0.3261, train acc 71.35%, f1 0.6957, precision 0.5333, recall 1.0000, auc 0.7870
epoch 2001, loss 0.3499, train acc 74.27%, f1 0.7179, precision 0.5600, recall 1.0000, auc 0.8087
epoch 2501, loss 0.3558, train acc 74.85%, f1 0.7226, precision 0.5657, recall 1.0000, auc 0.8130
epoch 3001, loss 0.3357, train acc 76.02%, f1 0.7320, precision 0.5773, recall 1.0000, auc 0.8217
epoch 3501, loss 0.3348, train acc 76.02%, f1 0.7320, precision 0.5773, recall 1.0000, auc 0.8217
epoch 4001, loss 0.3997, train acc 78.36%, f1 0.7517, precision 0.6022, recall 1.0000, auc 0.8391
epoch 4501, loss 0.3042, train acc 78.36%, f1 0.7517, precision 0.6022, recall 1.0000, auc 0.8391
epoch 5001, loss 0.3096, train acc 81.87%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8652
epoch 5501, loss 0.2957, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 6001, loss 0.3107, train acc 81.87%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8652
epoch 6501, loss 0.2882, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 7001, loss 0.2331, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 7501, loss 0.2984, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 8001, loss 0.3075, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 8501, loss 0.3539, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 9001, loss 0.2660, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 9501, loss 0.2967, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 10001, loss 0.2912, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 10501, loss 0.3120, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 11001, loss 0.2338, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 11501, loss 0.2779, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 12001, loss 0.2766, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 12501, loss 0.2512, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 13001, loss 0.2548, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 13501, loss 0.2278, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 14001, loss 0.2650, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 14501, loss 0.3011, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 15001, loss 0.2910, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 15501, loss 0.1878, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 16001, loss 0.1771, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 16501, loss 0.2379, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 17001, loss 0.1365, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 17501, loss 0.2438, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 18001, loss 0.2111, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 18501, loss 0.1591, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 19001, loss 0.2659, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 19501, loss 0.2525, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 20001, loss 0.2801, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 20501, loss 0.2406, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 21001, loss 0.3027, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 21501, loss 0.2169, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 22001, loss 0.2333, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 22501, loss 0.2314, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 23001, loss 0.2371, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 23501, loss 0.2531, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 24001, loss 0.1953, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 24501, loss 0.2072, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
running_time is 23.639807842
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.38
normal_0.5
./test_glass0/model_MLP_25000_0.38/record_1/MLP_25000_0.38_4
./test_glass0/result_MLP_25000_0.38_normal_0.5/record_1/
----------------------



the AUC is 0.6366995073891626

the Fscore is 0.5652173913043478

the precision is 0.40625

the recall is 0.9285714285714286

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.38/record_1/MLP_25000_0.38_5
----------------------



epoch 1, loss 0.6061, train acc 32.56%, f1 0.4912, precision 0.3256, recall 1.0000, auc 0.5000
epoch 501, loss 0.3857, train acc 66.86%, f1 0.6627, precision 0.4956, recall 1.0000, auc 0.7543
epoch 1001, loss 0.4264, train acc 73.84%, f1 0.7134, precision 0.5545, recall 1.0000, auc 0.8060
epoch 1501, loss 0.3257, train acc 76.74%, f1 0.7368, precision 0.5833, recall 1.0000, auc 0.8276
epoch 2001, loss 0.3907, train acc 78.49%, f1 0.7517, precision 0.6022, recall 1.0000, auc 0.8405
epoch 2501, loss 0.3588, train acc 80.23%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8534
epoch 3001, loss 0.2899, train acc 80.81%, f1 0.7724, precision 0.6292, recall 1.0000, auc 0.8578
epoch 3501, loss 0.2971, train acc 81.40%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8621
epoch 4001, loss 0.3502, train acc 81.98%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8664
epoch 4501, loss 0.2485, train acc 81.98%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8664
epoch 5001, loss 0.3395, train acc 82.56%, f1 0.7887, precision 0.6512, recall 1.0000, auc 0.8707
epoch 5501, loss 0.3330, train acc 82.56%, f1 0.7887, precision 0.6512, recall 1.0000, auc 0.8707
epoch 6001, loss 0.3218, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 6501, loss 0.3372, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 7001, loss 0.3572, train acc 83.72%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8793
epoch 7501, loss 0.2137, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 8001, loss 0.2609, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 8501, loss 0.2692, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 9001, loss 0.3726, train acc 83.72%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8793
epoch 9501, loss 0.3743, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 10001, loss 0.3999, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 10501, loss 0.2721, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 11001, loss 0.3035, train acc 84.30%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8836
epoch 11501, loss 0.3137, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 12001, loss 0.3675, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 12501, loss 0.3072, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 13001, loss 0.3482, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 13501, loss 0.2122, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 14001, loss 0.3612, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 14501, loss 0.2698, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 15001, loss 0.2430, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 15501, loss 0.3838, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 16001, loss 0.2981, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 16501, loss 0.2697, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 17001, loss 0.2733, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 17501, loss 0.2783, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 18001, loss 0.2706, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 18501, loss 0.3305, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 19001, loss 0.2380, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 19501, loss 0.2687, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 20001, loss 0.2319, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 20501, loss 0.3251, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 21001, loss 0.3337, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 21501, loss 0.2533, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 22001, loss 0.3259, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 22501, loss 0.2870, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 23001, loss 0.2848, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 23501, loss 0.3388, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 24001, loss 0.3384, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 24501, loss 0.3003, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
running_time is 23.812092494
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.38
normal_0.5
./test_glass0/model_MLP_25000_0.38/record_1/MLP_25000_0.38_5
./test_glass0/result_MLP_25000_0.38_normal_0.5/record_1/
----------------------



the AUC is 0.6785714285714285

the Fscore is 0.588235294117647

the precision is 0.5

the recall is 0.7142857142857143

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.39/record_1/MLP_25000_0.39_1
----------------------



epoch 1, loss 0.6185, train acc 34.50%, f1 0.5000, precision 0.3333, recall 1.0000, auc 0.5130
epoch 501, loss 0.4868, train acc 67.84%, f1 0.6707, precision 0.5045, recall 1.0000, auc 0.7609
epoch 1001, loss 0.4140, train acc 76.61%, f1 0.7333, precision 0.5851, recall 0.9821, auc 0.8215
epoch 1501, loss 0.3153, train acc 77.78%, f1 0.7467, precision 0.5957, recall 1.0000, auc 0.8348
epoch 2001, loss 0.2993, train acc 80.70%, f1 0.7724, precision 0.6292, recall 1.0000, auc 0.8565
epoch 2501, loss 0.3243, train acc 81.87%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8652
epoch 3001, loss 0.2323, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 3501, loss 0.2576, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 4001, loss 0.2915, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 4501, loss 0.2876, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 5001, loss 0.2670, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 5501, loss 0.3096, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 6001, loss 0.1786, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 6501, loss 0.2711, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 7001, loss 0.3552, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 7501, loss 0.2413, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 8001, loss 0.2831, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 8501, loss 0.2294, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 9001, loss 0.2772, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 9501, loss 0.3001, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 10001, loss 0.2519, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 10501, loss 0.3065, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 11001, loss 0.2553, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 11501, loss 0.3763, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 12001, loss 0.2860, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 12501, loss 0.2173, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 13001, loss 0.3016, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 13501, loss 0.3082, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 14001, loss 0.2560, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 14501, loss 0.2531, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 15001, loss 0.2832, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 15501, loss 0.2245, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 16001, loss 0.3209, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 16501, loss 0.2542, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 17001, loss 0.2527, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 17501, loss 0.2538, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 18001, loss 0.2319, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 18501, loss 0.3169, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 19001, loss 0.2434, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 19501, loss 0.2823, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 20001, loss 0.2744, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 20501, loss 0.3694, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 21001, loss 0.2980, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 21501, loss 0.3019, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 22001, loss 0.2322, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 22501, loss 0.3481, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 23001, loss 0.2637, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 23501, loss 0.2446, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 24001, loss 0.2785, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 24501, loss 0.2727, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
running_time is 23.374838464000003
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.39
normal_0.5
./test_glass0/model_MLP_25000_0.39/record_1/MLP_25000_0.39_1
./test_glass0/result_MLP_25000_0.39_normal_0.5/record_1/
----------------------



the AUC is 0.6724137931034483

the Fscore is 0.5957446808510638

the precision is 0.42424242424242425

the recall is 1.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.39/record_1/MLP_25000_0.39_2
----------------------



epoch 1, loss 0.7018, train acc 53.80%, f1 0.5864, precision 0.4148, recall 1.0000, auc 0.6565
epoch 501, loss 0.4191, train acc 69.01%, f1 0.6788, precision 0.5138, recall 1.0000, auc 0.7696
epoch 1001, loss 0.4198, train acc 74.85%, f1 0.7226, precision 0.5657, recall 1.0000, auc 0.8130
epoch 1501, loss 0.3138, train acc 78.95%, f1 0.7568, precision 0.6087, recall 1.0000, auc 0.8435
epoch 2001, loss 0.3216, train acc 79.53%, f1 0.7619, precision 0.6154, recall 1.0000, auc 0.8478
epoch 2501, loss 0.3760, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 3001, loss 0.3814, train acc 80.70%, f1 0.7724, precision 0.6292, recall 1.0000, auc 0.8565
epoch 3501, loss 0.3060, train acc 81.87%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8652
epoch 4001, loss 0.4457, train acc 81.29%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8609
epoch 4501, loss 0.3242, train acc 81.87%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8652
epoch 5001, loss 0.3253, train acc 82.46%, f1 0.7887, precision 0.6512, recall 1.0000, auc 0.8696
epoch 5501, loss 0.2918, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 6001, loss 0.3201, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 6501, loss 0.2994, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 7001, loss 0.3718, train acc 83.63%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8783
epoch 7501, loss 0.2941, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 8001, loss 0.3528, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 8501, loss 0.2380, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 9001, loss 0.2632, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 9501, loss 0.2465, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 10001, loss 0.3090, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 10501, loss 0.2660, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 11001, loss 0.2787, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 11501, loss 0.1913, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 12001, loss 0.2011, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 12501, loss 0.1938, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 13001, loss 0.2395, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 13501, loss 0.2151, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 14001, loss 0.1823, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 14501, loss 0.1509, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 15001, loss 0.2501, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 15501, loss 0.2389, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 16001, loss 0.2401, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 16501, loss 0.2143, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 17001, loss 0.1537, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 17501, loss 0.1606, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18001, loss 0.1631, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 18501, loss 0.1711, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19001, loss 0.1834, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19501, loss 0.2215, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 20001, loss 0.2813, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 20501, loss 0.2499, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 21001, loss 0.2172, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 21501, loss 0.2148, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 22001, loss 0.2173, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 22501, loss 0.2467, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 23001, loss 0.2220, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 23501, loss 0.1782, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 24001, loss 0.2162, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 24501, loss 0.2135, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
running_time is 23.698405663
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.39
normal_0.5
./test_glass0/model_MLP_25000_0.39/record_1/MLP_25000_0.39_2
./test_glass0/result_MLP_25000_0.39_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.39/record_1/MLP_25000_0.39_3
----------------------



epoch 1, loss 0.7436, train acc 42.11%, f1 0.5308, precision 0.3613, recall 1.0000, auc 0.5696
epoch 501, loss 0.3537, train acc 67.84%, f1 0.6707, precision 0.5045, recall 1.0000, auc 0.7609
epoch 1001, loss 0.3759, train acc 71.93%, f1 0.7000, precision 0.5385, recall 1.0000, auc 0.7913
epoch 1501, loss 0.3697, train acc 73.10%, f1 0.7089, precision 0.5490, recall 1.0000, auc 0.8000
epoch 2001, loss 0.4101, train acc 76.61%, f1 0.7368, precision 0.5833, recall 1.0000, auc 0.8261
epoch 2501, loss 0.3583, train acc 76.61%, f1 0.7368, precision 0.5833, recall 1.0000, auc 0.8261
epoch 3001, loss 0.3243, train acc 78.36%, f1 0.7517, precision 0.6022, recall 1.0000, auc 0.8391
epoch 3501, loss 0.3599, train acc 78.95%, f1 0.7568, precision 0.6087, recall 1.0000, auc 0.8435
epoch 4001, loss 0.2840, train acc 78.95%, f1 0.7568, precision 0.6087, recall 1.0000, auc 0.8435
epoch 4501, loss 0.3158, train acc 81.87%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8652
epoch 5001, loss 0.3478, train acc 80.70%, f1 0.7724, precision 0.6292, recall 1.0000, auc 0.8565
epoch 5501, loss 0.2930, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 6001, loss 0.2919, train acc 83.04%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8739
epoch 6501, loss 0.3233, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 7001, loss 0.2781, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 7501, loss 0.2757, train acc 84.80%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8870
epoch 8001, loss 0.3279, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 8501, loss 0.2992, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 9001, loss 0.3662, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 9501, loss 0.3296, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 10001, loss 0.2743, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 10501, loss 0.2723, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 11001, loss 0.3030, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 11501, loss 0.2943, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 12001, loss 0.3071, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 12501, loss 0.2930, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 13001, loss 0.2192, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 13501, loss 0.2420, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 14001, loss 0.2437, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 14501, loss 0.2412, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 15001, loss 0.2916, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 15501, loss 0.2454, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 16001, loss 0.2148, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 16501, loss 0.2141, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 17001, loss 0.2314, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 17501, loss 0.2773, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 18001, loss 0.2997, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 18501, loss 0.2502, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 19001, loss 0.3274, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 19501, loss 0.1953, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 20001, loss 0.2359, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 20501, loss 0.1918, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 21001, loss 0.2837, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 21501, loss 0.2249, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 22001, loss 0.1931, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 22501, loss 0.2895, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 23001, loss 0.2338, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 23501, loss 0.2276, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 24001, loss 0.2770, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 24501, loss 0.2217, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
running_time is 23.408322590999997
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.39
normal_0.5
./test_glass0/model_MLP_25000_0.39/record_1/MLP_25000_0.39_3
./test_glass0/result_MLP_25000_0.39_normal_0.5/record_1/
----------------------



the AUC is 0.6280788177339902

the Fscore is 0.4799999999999999

the precision is 0.5454545454545454

the recall is 0.42857142857142855

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.39/record_1/MLP_25000_0.39_4
----------------------



epoch 1, loss 0.8277, train acc 31.58%, f1 0.4753, precision 0.3174, recall 0.9464, auc 0.4776
epoch 501, loss 0.4469, train acc 66.67%, f1 0.6627, precision 0.4956, recall 1.0000, auc 0.7522
epoch 1001, loss 0.3953, train acc 70.76%, f1 0.6914, precision 0.5283, recall 1.0000, auc 0.7826
epoch 1501, loss 0.3476, train acc 73.68%, f1 0.7134, precision 0.5545, recall 1.0000, auc 0.8043
epoch 2001, loss 0.3847, train acc 74.27%, f1 0.7179, precision 0.5600, recall 1.0000, auc 0.8087
epoch 2501, loss 0.3694, train acc 76.02%, f1 0.7320, precision 0.5773, recall 1.0000, auc 0.8217
epoch 3001, loss 0.3695, train acc 76.61%, f1 0.7368, precision 0.5833, recall 1.0000, auc 0.8261
epoch 3501, loss 0.3518, train acc 77.78%, f1 0.7467, precision 0.5957, recall 1.0000, auc 0.8348
epoch 4001, loss 0.2646, train acc 78.95%, f1 0.7568, precision 0.6087, recall 1.0000, auc 0.8435
epoch 4501, loss 0.3715, train acc 80.12%, f1 0.7671, precision 0.6222, recall 1.0000, auc 0.8522
epoch 5001, loss 0.2751, train acc 82.46%, f1 0.7887, precision 0.6512, recall 1.0000, auc 0.8696
epoch 5501, loss 0.2456, train acc 81.29%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8609
epoch 6001, loss 0.2997, train acc 81.29%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8609
epoch 6501, loss 0.3573, train acc 81.87%, f1 0.7832, precision 0.6437, recall 1.0000, auc 0.8652
epoch 7001, loss 0.3751, train acc 83.63%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8737
epoch 7501, loss 0.2895, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 8001, loss 0.3447, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 8501, loss 0.3096, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 9001, loss 0.2795, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 9501, loss 0.2584, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 10001, loss 0.3252, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 10501, loss 0.2346, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 11001, loss 0.2736, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 11501, loss 0.2432, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 12001, loss 0.2587, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 12501, loss 0.1990, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 13001, loss 0.3049, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 13501, loss 0.2578, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 14001, loss 0.2634, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 14501, loss 0.2991, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 15001, loss 0.2794, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 15501, loss 0.2507, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 16001, loss 0.2697, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 16501, loss 0.2702, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 17001, loss 0.2470, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 17501, loss 0.2423, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 18001, loss 0.2341, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 18501, loss 0.2601, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 19001, loss 0.2655, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 19501, loss 0.2804, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 20001, loss 0.3520, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 20501, loss 0.2247, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 21001, loss 0.1947, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 21501, loss 0.2431, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 22001, loss 0.2461, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 22501, loss 0.2121, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 23001, loss 0.1952, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 23501, loss 0.2441, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 24001, loss 0.3075, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 24501, loss 0.2005, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
running_time is 23.492804563
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.39
normal_0.5
./test_glass0/model_MLP_25000_0.39/record_1/MLP_25000_0.39_4
./test_glass0/result_MLP_25000_0.39_normal_0.5/record_1/
----------------------



the AUC is 0.7179802955665024

the Fscore is 0.6206896551724138

the precision is 0.6

the recall is 0.6428571428571429

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.39/record_1/MLP_25000_0.39_5
----------------------



epoch 1, loss 0.7019, train acc 41.28%, f1 0.5258, precision 0.3567, recall 1.0000, auc 0.5647
epoch 501, loss 0.4138, train acc 67.44%, f1 0.6667, precision 0.5000, recall 1.0000, auc 0.7586
epoch 1001, loss 0.3648, train acc 70.93%, f1 0.6914, precision 0.5283, recall 1.0000, auc 0.7845
epoch 1501, loss 0.3477, train acc 76.74%, f1 0.7368, precision 0.5833, recall 1.0000, auc 0.8276
epoch 2001, loss 0.3317, train acc 79.65%, f1 0.7619, precision 0.6154, recall 1.0000, auc 0.8491
epoch 2501, loss 0.3167, train acc 81.40%, f1 0.7778, precision 0.6364, recall 1.0000, auc 0.8621
epoch 3001, loss 0.2997, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 3501, loss 0.2740, train acc 83.72%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8793
epoch 4001, loss 0.2972, train acc 83.72%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8793
epoch 4501, loss 0.2976, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 5001, loss 0.2685, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 5501, loss 0.2611, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 6001, loss 0.3120, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 6501, loss 0.3342, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 7001, loss 0.2407, train acc 85.47%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8922
epoch 7501, loss 0.2914, train acc 86.05%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8966
epoch 8001, loss 0.3447, train acc 86.05%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8966
epoch 8501, loss 0.3652, train acc 86.05%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8966
epoch 9001, loss 0.2398, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 9501, loss 0.3134, train acc 86.05%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8966
epoch 10001, loss 0.3338, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 10501, loss 0.3584, train acc 86.05%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8966
epoch 11001, loss 0.2777, train acc 86.05%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8966
epoch 11501, loss 0.3226, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 12001, loss 0.2872, train acc 86.63%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9009
epoch 12501, loss 0.3338, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 13001, loss 0.3057, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 13501, loss 0.2834, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 14001, loss 0.3110, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 14501, loss 0.2355, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 15001, loss 0.2752, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 15501, loss 0.2800, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 16001, loss 0.2713, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 16501, loss 0.3063, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 17001, loss 0.2449, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 17501, loss 0.3129, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 18001, loss 0.2721, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 18501, loss 0.2981, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 19001, loss 0.2728, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 19501, loss 0.3290, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 20001, loss 0.2664, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 20501, loss 0.2074, train acc 88.95%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9181
epoch 21001, loss 0.2634, train acc 88.37%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9138
epoch 21501, loss 0.2611, train acc 88.95%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9181
epoch 22001, loss 0.2495, train acc 88.95%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9181
epoch 22501, loss 0.2499, train acc 89.53%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9224
epoch 23001, loss 0.2887, train acc 89.53%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9224
epoch 23501, loss 0.2543, train acc 89.53%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9224
epoch 24001, loss 0.3167, train acc 89.53%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9224
epoch 24501, loss 0.2794, train acc 89.53%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9224
running_time is 23.405994405
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.39
normal_0.5
./test_glass0/model_MLP_25000_0.39/record_1/MLP_25000_0.39_5
./test_glass0/result_MLP_25000_0.39_normal_0.5/record_1/
----------------------



the AUC is 0.7321428571428572

the Fscore is 0.6511627906976745

the precision is 0.4827586206896552

the recall is 1.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.0/record_1/MLP_20000_0.0_1
----------------------



epoch 1, loss 0.6930, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.2622, train acc 83.04%, f1 0.7290, precision 0.7647, recall 0.6964, auc 0.7960
epoch 1001, loss 0.3766, train acc 87.13%, f1 0.8036, precision 0.8036, recall 0.8036, auc 0.8540
epoch 1501, loss 0.3441, train acc 89.47%, f1 0.8421, precision 0.8276, recall 0.8571, auc 0.8851
epoch 2001, loss 0.2554, train acc 90.06%, f1 0.8522, precision 0.8305, recall 0.8750, auc 0.8940
epoch 2501, loss 0.3191, train acc 90.64%, f1 0.8621, precision 0.8333, recall 0.8929, auc 0.9030
epoch 3001, loss 0.2756, train acc 90.06%, f1 0.8547, precision 0.8197, recall 0.8929, auc 0.8986
epoch 3501, loss 0.2050, train acc 89.47%, f1 0.8448, precision 0.8167, recall 0.8750, auc 0.8897
epoch 4001, loss 0.1923, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 4501, loss 0.2330, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 5001, loss 0.1801, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 5501, loss 0.1537, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 6001, loss 0.1254, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 6501, loss 0.0919, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 7001, loss 0.1195, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 7501, loss 0.1240, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 8001, loss 0.1116, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 8501, loss 0.0820, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 9001, loss 0.0702, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 9501, loss 0.0606, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 10001, loss 0.0575, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10501, loss 0.0964, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11001, loss 0.0616, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11501, loss 0.0969, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12001, loss 0.0831, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12501, loss 0.0634, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13001, loss 0.0925, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13501, loss 0.0718, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14001, loss 0.0575, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14501, loss 0.0740, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.0744, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.0632, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.0496, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.0361, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.0512, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.0518, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.0124, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.0328, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.0689, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.0298, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 19.228885289
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.0
normal_0.5
./test_glass0/model_MLP_20000_0.0/record_1/MLP_20000_0.0_1
./test_glass0/result_MLP_20000_0.0_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.0/record_1/MLP_20000_0.0_2
----------------------



epoch 1, loss 0.6930, train acc 66.67%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4957
epoch 501, loss 0.3661, train acc 79.53%, f1 0.6847, precision 0.6909, recall 0.6786, auc 0.7654
epoch 1001, loss 0.2755, train acc 84.80%, f1 0.7679, precision 0.7679, recall 0.7679, auc 0.8274
epoch 1501, loss 0.2383, train acc 85.96%, f1 0.7778, precision 0.8077, recall 0.7500, auc 0.8315
epoch 2001, loss 0.1993, train acc 90.64%, f1 0.8596, precision 0.8448, recall 0.8750, auc 0.8984
epoch 2501, loss 0.2462, train acc 92.40%, f1 0.8870, precision 0.8644, recall 0.9107, auc 0.9206
epoch 3001, loss 0.2029, train acc 93.57%, f1 0.9043, precision 0.8814, recall 0.9286, auc 0.9339
epoch 3501, loss 0.1726, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 4001, loss 0.0752, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 4501, loss 0.1297, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 5001, loss 0.2263, train acc 95.91%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9558
epoch 5501, loss 0.1398, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 6001, loss 0.0841, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 6501, loss 0.0639, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 7001, loss 0.1330, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 7501, loss 0.0417, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 8001, loss 0.0710, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 8501, loss 0.0592, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 9001, loss 0.0907, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 9501, loss 0.1237, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 10001, loss 0.0742, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 10501, loss 0.0633, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 11001, loss 0.0796, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 11501, loss 0.0818, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 12001, loss 0.0600, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 12501, loss 0.0225, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 13001, loss 0.0785, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 13501, loss 0.0451, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 14001, loss 0.0182, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 14501, loss 0.0305, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 15001, loss 0.0315, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 15501, loss 0.0331, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 16001, loss 0.0545, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 16501, loss 0.0448, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 17001, loss 0.0288, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 17501, loss 0.0365, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 18001, loss 0.0490, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 18501, loss 0.0389, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 19001, loss 0.0230, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 19501, loss 0.0190, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
running_time is 19.149086375
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.0
normal_0.5
./test_glass0/model_MLP_20000_0.0/record_1/MLP_20000_0.0_2
./test_glass0/result_MLP_20000_0.0_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.0/record_1/MLP_20000_0.0_3
----------------------



epoch 1, loss 0.6931, train acc 66.67%, f1 0.0339, precision 0.3333, recall 0.0179, auc 0.5002
epoch 501, loss 0.3695, train acc 81.87%, f1 0.7257, precision 0.7193, recall 0.7321, auc 0.7965
epoch 1001, loss 0.3395, train acc 82.46%, f1 0.7368, precision 0.7241, recall 0.7500, auc 0.8054
epoch 1501, loss 0.3728, train acc 86.55%, f1 0.8000, precision 0.7797, recall 0.8214, auc 0.8542
epoch 2001, loss 0.2522, train acc 87.13%, f1 0.8070, precision 0.7931, recall 0.8214, auc 0.8585
epoch 2501, loss 0.2360, train acc 87.72%, f1 0.8073, precision 0.8302, recall 0.7857, auc 0.8537
epoch 3001, loss 0.2866, train acc 91.81%, f1 0.8750, precision 0.8750, recall 0.8750, auc 0.9071
epoch 3501, loss 0.1724, train acc 92.98%, f1 0.8929, precision 0.8929, recall 0.8929, auc 0.9203
epoch 4001, loss 0.2019, train acc 95.32%, f1 0.9286, precision 0.9286, recall 0.9286, auc 0.9469
epoch 4501, loss 0.1460, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 5001, loss 0.1351, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 5501, loss 0.0597, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 6001, loss 0.1268, train acc 95.91%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9558
epoch 6501, loss 0.1830, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 7001, loss 0.1373, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 7501, loss 0.1086, train acc 96.49%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9602
epoch 8001, loss 0.1112, train acc 95.91%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9558
epoch 8501, loss 0.0744, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 9001, loss 0.0575, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 9501, loss 0.0845, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 10001, loss 0.0377, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 10501, loss 0.1438, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 11001, loss 0.0758, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 11501, loss 0.1127, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 12001, loss 0.0991, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 12501, loss 0.0950, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 13001, loss 0.0417, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 13501, loss 0.0717, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 14001, loss 0.0795, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 14501, loss 0.0534, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 15001, loss 0.0715, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 15501, loss 0.0360, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 16001, loss 0.0488, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 16501, loss 0.0614, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 17001, loss 0.0710, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 17501, loss 0.0544, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 18001, loss 0.0319, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 18501, loss 0.0634, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 19001, loss 0.0344, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 19501, loss 0.0156, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
running_time is 19.126730701
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.0
normal_0.5
./test_glass0/model_MLP_20000_0.0/record_1/MLP_20000_0.0_3
./test_glass0/result_MLP_20000_0.0_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.0/record_1/MLP_20000_0.0_4
----------------------



epoch 1, loss 0.6933, train acc 66.67%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4957
epoch 501, loss 0.3407, train acc 78.36%, f1 0.6337, precision 0.7111, recall 0.5714, auc 0.7292
epoch 1001, loss 0.3174, train acc 81.87%, f1 0.7304, precision 0.7119, recall 0.7500, auc 0.8011
epoch 1501, loss 0.4189, train acc 84.80%, f1 0.7679, precision 0.7679, recall 0.7679, auc 0.8274
epoch 2001, loss 0.2685, train acc 87.13%, f1 0.8036, precision 0.8036, recall 0.8036, auc 0.8540
epoch 2501, loss 0.1844, train acc 88.30%, f1 0.8214, precision 0.8214, recall 0.8214, auc 0.8672
epoch 3001, loss 0.2863, train acc 89.47%, f1 0.8364, precision 0.8519, recall 0.8214, auc 0.8759
epoch 3501, loss 0.2073, train acc 89.47%, f1 0.8364, precision 0.8519, recall 0.8214, auc 0.8759
epoch 4001, loss 0.2747, train acc 89.47%, f1 0.8393, precision 0.8393, recall 0.8393, auc 0.8805
epoch 4501, loss 0.1715, train acc 89.47%, f1 0.8421, precision 0.8276, recall 0.8571, auc 0.8851
epoch 5001, loss 0.2225, train acc 90.06%, f1 0.8496, precision 0.8421, recall 0.8571, auc 0.8894
epoch 5501, loss 0.2012, train acc 90.64%, f1 0.8596, precision 0.8448, recall 0.8750, auc 0.8984
epoch 6001, loss 0.2427, train acc 91.23%, f1 0.8673, precision 0.8596, recall 0.8750, auc 0.9027
epoch 6501, loss 0.1583, train acc 91.81%, f1 0.8772, precision 0.8621, recall 0.8929, auc 0.9116
epoch 7001, loss 0.0881, train acc 91.81%, f1 0.8772, precision 0.8621, recall 0.8929, auc 0.9116
epoch 7501, loss 0.1841, train acc 93.57%, f1 0.9027, precision 0.8947, recall 0.9107, auc 0.9293
epoch 8001, loss 0.1549, train acc 92.40%, f1 0.8870, precision 0.8644, recall 0.9107, auc 0.9206
epoch 8501, loss 0.1562, train acc 94.15%, f1 0.9107, precision 0.9107, recall 0.9107, auc 0.9336
epoch 9001, loss 0.2089, train acc 94.15%, f1 0.9123, precision 0.8966, recall 0.9286, auc 0.9382
epoch 9501, loss 0.1113, train acc 93.57%, f1 0.9043, precision 0.8814, recall 0.9286, auc 0.9339
epoch 10001, loss 0.1367, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 10501, loss 0.0866, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 11001, loss 0.1362, train acc 94.15%, f1 0.9123, precision 0.8966, recall 0.9286, auc 0.9382
epoch 11501, loss 0.1771, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 12001, loss 0.1686, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 12501, loss 0.0848, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 13001, loss 0.1120, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 13501, loss 0.1665, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 14001, loss 0.1114, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 14501, loss 0.0557, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 15001, loss 0.1826, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 15501, loss 0.1319, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 16001, loss 0.0790, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 16501, loss 0.0805, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 17001, loss 0.0540, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 17501, loss 0.0419, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 18001, loss 0.0855, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 18501, loss 0.1187, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 19001, loss 0.0906, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 19501, loss 0.0878, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
running_time is 19.249713892
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.0
normal_0.5
./test_glass0/model_MLP_20000_0.0/record_1/MLP_20000_0.0_4
./test_glass0/result_MLP_20000_0.0_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.0/record_1/MLP_20000_0.0_5
----------------------



epoch 1, loss 0.6929, train acc 67.44%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.2792, train acc 81.40%, f1 0.7193, precision 0.7069, recall 0.7321, auc 0.7928
epoch 1001, loss 0.3332, train acc 83.14%, f1 0.7434, precision 0.7368, recall 0.7500, auc 0.8103
epoch 1501, loss 0.4244, train acc 85.47%, f1 0.7826, precision 0.7627, recall 0.8036, auc 0.8414
epoch 2001, loss 0.3608, train acc 89.53%, f1 0.8448, precision 0.8167, recall 0.8750, auc 0.8901
epoch 2501, loss 0.2801, train acc 90.70%, f1 0.8491, precision 0.9000, recall 0.8036, auc 0.8802
epoch 3001, loss 0.3384, train acc 91.86%, f1 0.8750, precision 0.8750, recall 0.8750, auc 0.9073
epoch 3501, loss 0.2436, train acc 92.44%, f1 0.8807, precision 0.9057, recall 0.8571, auc 0.9070
epoch 4001, loss 0.2121, train acc 93.02%, f1 0.8929, precision 0.8929, recall 0.8929, auc 0.9206
epoch 4501, loss 0.1668, train acc 94.19%, f1 0.9107, precision 0.9107, recall 0.9107, auc 0.9338
epoch 5001, loss 0.1478, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 5501, loss 0.1615, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 6001, loss 0.1454, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 6501, loss 0.1671, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 7001, loss 0.1415, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 7501, loss 0.0951, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 8001, loss 0.1350, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 8501, loss 0.0920, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 9001, loss 0.1029, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 9501, loss 0.1822, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 10001, loss 0.0746, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 10501, loss 0.0705, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 11001, loss 0.0819, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 11501, loss 0.0870, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 12001, loss 0.1189, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 12501, loss 0.1127, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 13001, loss 0.0731, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 13501, loss 0.0696, train acc 95.35%, f1 0.9286, precision 0.9286, recall 0.9286, auc 0.9470
epoch 14001, loss 0.0728, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 14501, loss 0.1237, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 15001, loss 0.1089, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 15501, loss 0.0597, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 16001, loss 0.0544, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 16501, loss 0.1155, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 17001, loss 0.0834, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 17501, loss 0.0588, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 18001, loss 0.0665, train acc 97.09%, f1 0.9550, precision 0.9636, recall 0.9464, auc 0.9646
epoch 18501, loss 0.0686, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 19001, loss 0.0568, train acc 97.09%, f1 0.9550, precision 0.9636, recall 0.9464, auc 0.9646
epoch 19501, loss 0.0483, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
running_time is 19.583588991
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.0
normal_0.5
./test_glass0/model_MLP_20000_0.0/record_1/MLP_20000_0.0_5
./test_glass0/result_MLP_20000_0.0_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.01/record_1/MLP_20000_0.01_1
----------------------



epoch 1, loss 0.6896, train acc 55.56%, f1 0.0256, precision 0.0455, recall 0.0179, auc 0.4176
epoch 501, loss 0.3391, train acc 84.21%, f1 0.7611, precision 0.7544, recall 0.7679, auc 0.8231
epoch 1001, loss 0.3315, train acc 86.55%, f1 0.8000, precision 0.7797, recall 0.8214, auc 0.8542
epoch 1501, loss 0.3398, train acc 88.89%, f1 0.8376, precision 0.8033, recall 0.8750, auc 0.8853
epoch 2001, loss 0.2000, train acc 88.89%, f1 0.8403, precision 0.7937, recall 0.8929, auc 0.8899
epoch 2501, loss 0.2085, train acc 88.30%, f1 0.8333, precision 0.7812, recall 0.8929, auc 0.8856
epoch 3001, loss 0.2282, train acc 89.47%, f1 0.8525, precision 0.7879, recall 0.9286, auc 0.9034
epoch 3501, loss 0.1621, train acc 90.06%, f1 0.8571, precision 0.8095, recall 0.9107, auc 0.9032
epoch 4001, loss 0.1721, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 4501, loss 0.2299, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 5001, loss 0.1316, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 5501, loss 0.1826, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 6001, loss 0.1288, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 6501, loss 0.1388, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 7001, loss 0.0690, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 7501, loss 0.0867, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 8001, loss 0.0435, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 8501, loss 0.1078, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 9001, loss 0.0535, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 9501, loss 0.0514, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10001, loss 0.0477, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10501, loss 0.0660, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11001, loss 0.0587, train acc 98.83%, f1 0.9818, precision 1.0000, recall 0.9643, auc 0.9821
epoch 11501, loss 0.0367, train acc 98.83%, f1 0.9818, precision 1.0000, recall 0.9643, auc 0.9821
epoch 12001, loss 0.0389, train acc 98.83%, f1 0.9818, precision 1.0000, recall 0.9643, auc 0.9821
epoch 12501, loss 0.0747, train acc 98.83%, f1 0.9818, precision 1.0000, recall 0.9643, auc 0.9821
epoch 13001, loss 0.0421, train acc 98.83%, f1 0.9818, precision 1.0000, recall 0.9643, auc 0.9821
epoch 13501, loss 0.0445, train acc 98.83%, f1 0.9818, precision 1.0000, recall 0.9643, auc 0.9821
epoch 14001, loss 0.0263, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 14501, loss 0.0457, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 15001, loss 0.0388, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 15501, loss 0.0364, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 16001, loss 0.0175, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 16501, loss 0.0518, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 17001, loss 0.0118, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 17501, loss 0.0206, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 18001, loss 0.0235, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 18501, loss 0.0224, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 19001, loss 0.0136, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 19501, loss 0.0268, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
running_time is 19.233041138999997
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.01
normal_0.5
./test_glass0/model_MLP_20000_0.01/record_1/MLP_20000_0.01_1
./test_glass0/result_MLP_20000_0.01_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.01/record_1/MLP_20000_0.01_2
----------------------



epoch 1, loss 0.6878, train acc 62.57%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4652
epoch 501, loss 0.4239, train acc 81.29%, f1 0.7091, precision 0.7222, recall 0.6964, auc 0.7830
epoch 1001, loss 0.3241, train acc 83.63%, f1 0.7455, precision 0.7593, recall 0.7321, auc 0.8095
epoch 1501, loss 0.2686, train acc 84.80%, f1 0.7636, precision 0.7778, recall 0.7500, auc 0.8228
epoch 2001, loss 0.2567, train acc 87.72%, f1 0.8073, precision 0.8302, recall 0.7857, auc 0.8537
epoch 2501, loss 0.1768, train acc 90.64%, f1 0.8571, precision 0.8571, recall 0.8571, auc 0.8938
epoch 3001, loss 0.1264, train acc 92.98%, f1 0.8947, precision 0.8793, recall 0.9107, auc 0.9249
epoch 3501, loss 0.2213, train acc 93.57%, f1 0.9043, precision 0.8814, recall 0.9286, auc 0.9339
epoch 4001, loss 0.1365, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 4501, loss 0.2035, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 5001, loss 0.1822, train acc 95.91%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9558
epoch 5501, loss 0.1482, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 6001, loss 0.1214, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 6501, loss 0.0546, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 7001, loss 0.0979, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 7501, loss 0.1198, train acc 96.49%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9602
epoch 8001, loss 0.1682, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 8501, loss 0.0762, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 9001, loss 0.1048, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 9501, loss 0.0933, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 10001, loss 0.1455, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 10501, loss 0.0278, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 11001, loss 0.0962, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 11501, loss 0.0539, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 12001, loss 0.0519, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 12501, loss 0.0280, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 13001, loss 0.0313, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 13501, loss 0.0645, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 14001, loss 0.0401, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 14501, loss 0.0918, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 15001, loss 0.0393, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 15501, loss 0.0230, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 16001, loss 0.0208, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 16501, loss 0.0537, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 17001, loss 0.0241, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 17501, loss 0.0385, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18001, loss 0.0233, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18501, loss 0.0174, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19001, loss 0.0509, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19501, loss 0.0467, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 19.228827322
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.01
normal_0.5
./test_glass0/model_MLP_20000_0.01/record_1/MLP_20000_0.01_2
./test_glass0/result_MLP_20000_0.01_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.01/record_1/MLP_20000_0.01_3
----------------------



epoch 1, loss 0.6878, train acc 66.67%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4957
epoch 501, loss 0.4089, train acc 80.70%, f1 0.7227, precision 0.6825, recall 0.7679, auc 0.7970
epoch 1001, loss 0.4049, train acc 83.04%, f1 0.7521, precision 0.7213, recall 0.7857, auc 0.8189
epoch 1501, loss 0.2224, train acc 85.96%, f1 0.7931, precision 0.7667, recall 0.8214, auc 0.8498
epoch 2001, loss 0.2148, train acc 87.13%, f1 0.8070, precision 0.7931, recall 0.8214, auc 0.8585
epoch 2501, loss 0.2889, train acc 87.72%, f1 0.8073, precision 0.8302, recall 0.7857, auc 0.8537
epoch 3001, loss 0.2458, train acc 89.47%, f1 0.8302, precision 0.8800, recall 0.7857, auc 0.8668
epoch 3501, loss 0.2050, train acc 92.40%, f1 0.8807, precision 0.9057, recall 0.8571, auc 0.9068
epoch 4001, loss 0.2010, train acc 92.40%, f1 0.8850, precision 0.8772, recall 0.8929, auc 0.9160
epoch 4501, loss 0.1935, train acc 92.40%, f1 0.8850, precision 0.8772, recall 0.8929, auc 0.9160
epoch 5001, loss 0.1900, train acc 92.40%, f1 0.8850, precision 0.8772, recall 0.8929, auc 0.9160
epoch 5501, loss 0.1028, train acc 94.15%, f1 0.9107, precision 0.9107, recall 0.9107, auc 0.9336
epoch 6001, loss 0.1144, train acc 94.15%, f1 0.9123, precision 0.8966, recall 0.9286, auc 0.9382
epoch 6501, loss 0.0865, train acc 94.74%, f1 0.9204, precision 0.9123, recall 0.9286, auc 0.9425
epoch 7001, loss 0.0755, train acc 95.32%, f1 0.9286, precision 0.9286, recall 0.9286, auc 0.9469
epoch 7501, loss 0.0905, train acc 95.91%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9558
epoch 8001, loss 0.1241, train acc 95.91%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9558
epoch 8501, loss 0.0880, train acc 96.49%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9602
epoch 9001, loss 0.0803, train acc 96.49%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9602
epoch 9501, loss 0.0820, train acc 95.91%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9558
epoch 10001, loss 0.0774, train acc 96.49%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9602
epoch 10501, loss 0.1198, train acc 96.49%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9602
epoch 11001, loss 0.0654, train acc 96.49%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9602
epoch 11501, loss 0.0583, train acc 97.08%, f1 0.9550, precision 0.9636, recall 0.9464, auc 0.9645
epoch 12001, loss 0.0909, train acc 96.49%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9602
epoch 12501, loss 0.1216, train acc 97.08%, f1 0.9550, precision 0.9636, recall 0.9464, auc 0.9645
epoch 13001, loss 0.0636, train acc 96.49%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9602
epoch 13501, loss 0.0462, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 14001, loss 0.0374, train acc 97.08%, f1 0.9550, precision 0.9636, recall 0.9464, auc 0.9645
epoch 14501, loss 0.0878, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 15001, loss 0.0737, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 15501, loss 0.0864, train acc 98.83%, f1 0.9818, precision 1.0000, recall 0.9643, auc 0.9821
epoch 16001, loss 0.0754, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 16501, loss 0.0368, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 17001, loss 0.0540, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 17501, loss 0.0465, train acc 98.83%, f1 0.9818, precision 1.0000, recall 0.9643, auc 0.9821
epoch 18001, loss 0.0939, train acc 98.83%, f1 0.9818, precision 1.0000, recall 0.9643, auc 0.9821
epoch 18501, loss 0.0265, train acc 98.83%, f1 0.9818, precision 1.0000, recall 0.9643, auc 0.9821
epoch 19001, loss 0.0277, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 19501, loss 0.0365, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
running_time is 19.145553923
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.01
normal_0.5
./test_glass0/model_MLP_20000_0.01/record_1/MLP_20000_0.01_3
./test_glass0/result_MLP_20000_0.01_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.01/record_1/MLP_20000_0.01_4
----------------------



epoch 1, loss 0.6850, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3146, train acc 81.87%, f1 0.7257, precision 0.7193, recall 0.7321, auc 0.7965
epoch 1001, loss 0.3238, train acc 83.04%, f1 0.7434, precision 0.7368, recall 0.7500, auc 0.8098
epoch 1501, loss 0.3040, train acc 85.38%, f1 0.7788, precision 0.7719, recall 0.7857, auc 0.8363
epoch 2001, loss 0.3819, train acc 87.13%, f1 0.8036, precision 0.8036, recall 0.8036, auc 0.8540
epoch 2501, loss 0.2446, train acc 87.13%, f1 0.8036, precision 0.8036, recall 0.8036, auc 0.8540
epoch 3001, loss 0.2382, train acc 88.89%, f1 0.8319, precision 0.8246, recall 0.8393, auc 0.8762
epoch 3501, loss 0.2094, train acc 90.06%, f1 0.8496, precision 0.8421, recall 0.8571, auc 0.8894
epoch 4001, loss 0.2695, train acc 91.23%, f1 0.8718, precision 0.8361, recall 0.9107, auc 0.9119
epoch 4501, loss 0.1995, train acc 91.23%, f1 0.8696, precision 0.8475, recall 0.8929, auc 0.9073
epoch 5001, loss 0.2500, train acc 91.23%, f1 0.8696, precision 0.8475, recall 0.8929, auc 0.9073
epoch 5501, loss 0.1840, train acc 92.40%, f1 0.8850, precision 0.8772, recall 0.8929, auc 0.9160
epoch 6001, loss 0.1847, train acc 92.98%, f1 0.8947, precision 0.8793, recall 0.9107, auc 0.9249
epoch 6501, loss 0.1961, train acc 93.57%, f1 0.9043, precision 0.8814, recall 0.9286, auc 0.9339
epoch 7001, loss 0.1685, train acc 92.98%, f1 0.8947, precision 0.8793, recall 0.9107, auc 0.9249
epoch 7501, loss 0.1478, train acc 92.98%, f1 0.8947, precision 0.8793, recall 0.9107, auc 0.9249
epoch 8001, loss 0.1703, train acc 92.98%, f1 0.8966, precision 0.8667, recall 0.9286, auc 0.9295
epoch 8501, loss 0.1579, train acc 92.98%, f1 0.8947, precision 0.8793, recall 0.9107, auc 0.9249
epoch 9001, loss 0.1470, train acc 92.98%, f1 0.8966, precision 0.8667, recall 0.9286, auc 0.9295
epoch 9501, loss 0.1529, train acc 94.74%, f1 0.9204, precision 0.9123, recall 0.9286, auc 0.9425
epoch 10001, loss 0.0894, train acc 94.15%, f1 0.9123, precision 0.8966, recall 0.9286, auc 0.9382
epoch 10501, loss 0.1357, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 11001, loss 0.1739, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 11501, loss 0.1865, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 12001, loss 0.1051, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 12501, loss 0.1370, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 13001, loss 0.1330, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 13501, loss 0.0966, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 14001, loss 0.0938, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 14501, loss 0.0584, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 15001, loss 0.1529, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 15501, loss 0.1355, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 16001, loss 0.1023, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 16501, loss 0.1163, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 17001, loss 0.1494, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 17501, loss 0.0965, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 18001, loss 0.0356, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 18501, loss 0.1258, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 19001, loss 0.0918, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 19501, loss 0.0829, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
running_time is 19.463862909
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.01
normal_0.5
./test_glass0/model_MLP_20000_0.01/record_1/MLP_20000_0.01_4
./test_glass0/result_MLP_20000_0.01_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.01/record_1/MLP_20000_0.01_5
----------------------



epoch 1, loss 0.6854, train acc 67.44%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.4271, train acc 80.81%, f1 0.7080, precision 0.7018, recall 0.7143, auc 0.7839
epoch 1001, loss 0.3552, train acc 83.72%, f1 0.7544, precision 0.7414, recall 0.7679, auc 0.8193
epoch 1501, loss 0.2882, train acc 87.79%, f1 0.8205, precision 0.7869, recall 0.8571, auc 0.8725
epoch 2001, loss 0.2668, train acc 88.37%, f1 0.8305, precision 0.7903, recall 0.8750, auc 0.8815
epoch 2501, loss 0.2057, train acc 91.28%, f1 0.8696, precision 0.8475, recall 0.8929, auc 0.9076
epoch 3001, loss 0.2633, train acc 93.02%, f1 0.8929, precision 0.8929, recall 0.8929, auc 0.9206
epoch 3501, loss 0.1758, train acc 94.19%, f1 0.9107, precision 0.9107, recall 0.9107, auc 0.9338
epoch 4001, loss 0.2240, train acc 94.19%, f1 0.9107, precision 0.9107, recall 0.9107, auc 0.9338
epoch 4501, loss 0.1658, train acc 94.77%, f1 0.9204, precision 0.9123, recall 0.9286, auc 0.9427
epoch 5001, loss 0.1481, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 5501, loss 0.1468, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 6001, loss 0.1201, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 6501, loss 0.1428, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 7001, loss 0.1210, train acc 93.60%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9387
epoch 7501, loss 0.0864, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 8001, loss 0.1158, train acc 93.60%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9387
epoch 8501, loss 0.0772, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 9001, loss 0.1305, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 9501, loss 0.0795, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 10001, loss 0.0498, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 10501, loss 0.0875, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 11001, loss 0.0541, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 11501, loss 0.1037, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 12001, loss 0.1029, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 12501, loss 0.0948, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 13001, loss 0.1130, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 13501, loss 0.0868, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 14001, loss 0.0751, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 14501, loss 0.0949, train acc 94.77%, f1 0.9204, precision 0.9123, recall 0.9286, auc 0.9427
epoch 15001, loss 0.1090, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 15501, loss 0.0788, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 16001, loss 0.0751, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 16501, loss 0.0425, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 17001, loss 0.0346, train acc 96.51%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9603
epoch 17501, loss 0.0438, train acc 97.09%, f1 0.9550, precision 0.9636, recall 0.9464, auc 0.9646
epoch 18001, loss 0.1194, train acc 97.67%, f1 0.9636, precision 0.9815, recall 0.9464, auc 0.9689
epoch 18501, loss 0.0466, train acc 97.09%, f1 0.9550, precision 0.9636, recall 0.9464, auc 0.9646
epoch 19001, loss 0.0778, train acc 97.09%, f1 0.9550, precision 0.9636, recall 0.9464, auc 0.9646
epoch 19501, loss 0.1145, train acc 97.67%, f1 0.9636, precision 0.9815, recall 0.9464, auc 0.9689
running_time is 19.31907607
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.01
normal_0.5
./test_glass0/model_MLP_20000_0.01/record_1/MLP_20000_0.01_5
./test_glass0/result_MLP_20000_0.01_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.02/record_1/MLP_20000_0.02_1
----------------------



epoch 1, loss 0.6796, train acc 63.16%, f1 0.0870, precision 0.2308, recall 0.0536, auc 0.4833
epoch 501, loss 0.3540, train acc 83.63%, f1 0.7500, precision 0.7500, recall 0.7500, auc 0.8141
epoch 1001, loss 0.3118, train acc 87.13%, f1 0.8103, precision 0.7833, recall 0.8393, auc 0.8631
epoch 1501, loss 0.2970, train acc 88.89%, f1 0.8348, precision 0.8136, recall 0.8571, auc 0.8807
epoch 2001, loss 0.4051, train acc 88.89%, f1 0.8348, precision 0.8136, recall 0.8571, auc 0.8807
epoch 2501, loss 0.3118, train acc 90.06%, f1 0.8547, precision 0.8197, recall 0.8929, auc 0.8986
epoch 3001, loss 0.2010, train acc 91.23%, f1 0.8696, precision 0.8475, recall 0.8929, auc 0.9073
epoch 3501, loss 0.1655, train acc 91.81%, f1 0.8772, precision 0.8621, recall 0.8929, auc 0.9116
epoch 4001, loss 0.1976, train acc 92.98%, f1 0.8966, precision 0.8667, recall 0.9286, auc 0.9295
epoch 4501, loss 0.1598, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 5001, loss 0.1544, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 5501, loss 0.0733, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 6001, loss 0.1126, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 6501, loss 0.1271, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 7001, loss 0.0982, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 7501, loss 0.0998, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 8001, loss 0.1013, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 8501, loss 0.0832, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 9001, loss 0.0560, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 9501, loss 0.1057, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10001, loss 0.0961, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10501, loss 0.1167, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11001, loss 0.0825, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11501, loss 0.1083, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12001, loss 0.0793, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12501, loss 0.0524, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13001, loss 0.0442, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13501, loss 0.0395, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14001, loss 0.0493, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.0761, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15001, loss 0.0512, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 15501, loss 0.0505, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16001, loss 0.0332, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16501, loss 0.0505, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.0243, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17501, loss 0.0465, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18001, loss 0.0374, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18501, loss 0.0204, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19001, loss 0.0494, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19501, loss 0.0566, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 19.368551202
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.02
normal_0.5
./test_glass0/model_MLP_20000_0.02/record_1/MLP_20000_0.02_1
./test_glass0/result_MLP_20000_0.02_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.02/record_1/MLP_20000_0.02_2
----------------------



epoch 1, loss 0.6843, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3812, train acc 81.87%, f1 0.7156, precision 0.7358, recall 0.6964, auc 0.7873
epoch 1001, loss 0.2735, train acc 84.21%, f1 0.7568, precision 0.7636, recall 0.7500, auc 0.8185
epoch 1501, loss 0.2698, train acc 85.96%, f1 0.7857, precision 0.7857, recall 0.7857, auc 0.8407
epoch 2001, loss 0.2712, train acc 85.96%, f1 0.7778, precision 0.8077, recall 0.7500, auc 0.8315
epoch 2501, loss 0.2872, train acc 91.23%, f1 0.8718, precision 0.8361, recall 0.9107, auc 0.9119
epoch 3001, loss 0.2061, train acc 91.81%, f1 0.8814, precision 0.8387, recall 0.9286, auc 0.9208
epoch 3501, loss 0.1705, train acc 91.81%, f1 0.8814, precision 0.8387, recall 0.9286, auc 0.9208
epoch 4001, loss 0.1883, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 4501, loss 0.1442, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 5001, loss 0.2519, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 5501, loss 0.1422, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 6001, loss 0.1385, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 6501, loss 0.1652, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 7001, loss 0.2101, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 7501, loss 0.1204, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 8001, loss 0.0691, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 8501, loss 0.1393, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 9001, loss 0.1613, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 9501, loss 0.1161, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 10001, loss 0.0813, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 10501, loss 0.1218, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 11001, loss 0.1625, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 11501, loss 0.1189, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 12001, loss 0.1423, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 12501, loss 0.1855, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 13001, loss 0.0721, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 13501, loss 0.0726, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 14001, loss 0.0693, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 14501, loss 0.1766, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 15001, loss 0.1121, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 15501, loss 0.1287, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 16001, loss 0.0877, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 16501, loss 0.0915, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 17001, loss 0.1967, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 17501, loss 0.1757, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 18001, loss 0.0719, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 18501, loss 0.0693, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 19001, loss 0.0752, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 19501, loss 0.1555, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
running_time is 19.356651522
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.02
normal_0.5
./test_glass0/model_MLP_20000_0.02/record_1/MLP_20000_0.02_2
./test_glass0/result_MLP_20000_0.02_normal_0.5/record_1/
----------------------



the AUC is 0.5541871921182266

the Fscore is 0.23529411764705882

the precision is 0.6666666666666666

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.02/record_1/MLP_20000_0.02_3
----------------------



epoch 1, loss 0.6778, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.4208, train acc 81.29%, f1 0.7333, precision 0.6875, recall 0.7857, auc 0.8059
epoch 1001, loss 0.2878, train acc 83.63%, f1 0.7586, precision 0.7333, recall 0.7857, auc 0.8233
epoch 1501, loss 0.2290, train acc 85.96%, f1 0.7931, precision 0.7667, recall 0.8214, auc 0.8498
epoch 2001, loss 0.3643, train acc 85.96%, f1 0.7895, precision 0.7759, recall 0.8036, auc 0.8453
epoch 2501, loss 0.1450, train acc 89.47%, f1 0.8393, precision 0.8393, recall 0.8393, auc 0.8805
epoch 3001, loss 0.1991, train acc 91.81%, f1 0.8772, precision 0.8621, recall 0.8929, auc 0.9116
epoch 3501, loss 0.1835, train acc 91.23%, f1 0.8696, precision 0.8475, recall 0.8929, auc 0.9073
epoch 4001, loss 0.1323, train acc 92.98%, f1 0.8947, precision 0.8793, recall 0.9107, auc 0.9249
epoch 4501, loss 0.1658, train acc 94.15%, f1 0.9107, precision 0.9107, recall 0.9107, auc 0.9336
epoch 5001, loss 0.1310, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 5501, loss 0.2013, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 6001, loss 0.0980, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 6501, loss 0.0620, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 7001, loss 0.0864, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 7501, loss 0.1054, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 8001, loss 0.1063, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 8501, loss 0.0553, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 9001, loss 0.0945, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 9501, loss 0.0484, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 10001, loss 0.0269, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 10501, loss 0.0926, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 11001, loss 0.0646, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 11501, loss 0.0529, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 12001, loss 0.0380, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 12501, loss 0.0423, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 13001, loss 0.0420, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 13501, loss 0.0406, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 14001, loss 0.0389, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 14501, loss 0.0207, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 15001, loss 0.0332, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0166, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0387, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16501, loss 0.0236, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0242, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0276, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0176, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0176, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0162, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0147, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
running_time is 19.407757292000003
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.02
normal_0.5
./test_glass0/model_MLP_20000_0.02/record_1/MLP_20000_0.02_3
./test_glass0/result_MLP_20000_0.02_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.02/record_1/MLP_20000_0.02_4
----------------------



epoch 1, loss 0.6749, train acc 65.50%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4870
epoch 501, loss 0.4540, train acc 80.70%, f1 0.7130, precision 0.6949, recall 0.7321, auc 0.7878
epoch 1001, loss 0.3449, train acc 81.87%, f1 0.7257, precision 0.7193, recall 0.7321, auc 0.7965
epoch 1501, loss 0.2544, train acc 83.04%, f1 0.7434, precision 0.7368, recall 0.7500, auc 0.8098
epoch 2001, loss 0.2641, train acc 85.96%, f1 0.7857, precision 0.7857, recall 0.7857, auc 0.8407
epoch 2501, loss 0.2718, train acc 87.13%, f1 0.8070, precision 0.7931, recall 0.8214, auc 0.8585
epoch 3001, loss 0.2516, train acc 87.72%, f1 0.8108, precision 0.8182, recall 0.8036, auc 0.8583
epoch 3501, loss 0.3192, train acc 91.23%, f1 0.8649, precision 0.8727, recall 0.8571, auc 0.8981
epoch 4001, loss 0.1980, train acc 92.40%, f1 0.8870, precision 0.8644, recall 0.9107, auc 0.9206
epoch 4501, loss 0.1456, train acc 93.57%, f1 0.9027, precision 0.8947, recall 0.9107, auc 0.9293
epoch 5001, loss 0.2132, train acc 92.40%, f1 0.8870, precision 0.8644, recall 0.9107, auc 0.9206
epoch 5501, loss 0.1003, train acc 93.57%, f1 0.9043, precision 0.8814, recall 0.9286, auc 0.9339
epoch 6001, loss 0.0593, train acc 93.57%, f1 0.9043, precision 0.8814, recall 0.9286, auc 0.9339
epoch 6501, loss 0.1328, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 7001, loss 0.1127, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 7501, loss 0.1596, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 8001, loss 0.1155, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 8501, loss 0.1129, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 9001, loss 0.1045, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 9501, loss 0.0825, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 10001, loss 0.0882, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 10501, loss 0.0640, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 11001, loss 0.0528, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 11501, loss 0.0449, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 12001, loss 0.0558, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 12501, loss 0.0520, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 13001, loss 0.0582, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 13501, loss 0.0590, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 14001, loss 0.0448, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 14501, loss 0.0518, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0195, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0447, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0388, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16501, loss 0.0333, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0219, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0121, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0335, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0188, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0213, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0160, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
running_time is 19.395266456999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.02
normal_0.5
./test_glass0/model_MLP_20000_0.02/record_1/MLP_20000_0.02_4
./test_glass0/result_MLP_20000_0.02_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.02/record_1/MLP_20000_0.02_5
----------------------



epoch 1, loss 0.6859, train acc 66.86%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4957
epoch 501, loss 0.3776, train acc 79.65%, f1 0.7009, precision 0.6721, recall 0.7321, auc 0.7799
epoch 1001, loss 0.3201, train acc 82.56%, f1 0.7414, precision 0.7167, recall 0.7679, auc 0.8107
epoch 1501, loss 0.3078, train acc 86.63%, f1 0.8034, precision 0.7705, recall 0.8393, auc 0.8593
epoch 2001, loss 0.2721, train acc 87.21%, f1 0.8103, precision 0.7833, recall 0.8393, auc 0.8636
epoch 2501, loss 0.2745, train acc 88.37%, f1 0.8246, precision 0.8103, recall 0.8393, auc 0.8722
epoch 3001, loss 0.2835, train acc 90.70%, f1 0.8644, precision 0.8226, recall 0.9107, auc 0.9079
epoch 3501, loss 0.2713, train acc 93.02%, f1 0.8947, precision 0.8793, recall 0.9107, auc 0.9252
epoch 4001, loss 0.2186, train acc 93.60%, f1 0.9043, precision 0.8814, recall 0.9286, auc 0.9341
epoch 4501, loss 0.2945, train acc 93.60%, f1 0.9043, precision 0.8814, recall 0.9286, auc 0.9341
epoch 5001, loss 0.1462, train acc 93.60%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9387
epoch 5501, loss 0.2231, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 6001, loss 0.1736, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 6501, loss 0.1497, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 7001, loss 0.1776, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 7501, loss 0.1489, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 8001, loss 0.1072, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 8501, loss 0.0806, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 9001, loss 0.1002, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 9501, loss 0.1088, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 10001, loss 0.1095, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 10501, loss 0.0908, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 11001, loss 0.0806, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 11501, loss 0.0794, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 12001, loss 0.0352, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 12501, loss 0.0819, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 13001, loss 0.1243, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 13501, loss 0.0866, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 14001, loss 0.0638, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 14501, loss 0.1051, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 15001, loss 0.0658, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 15501, loss 0.0793, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 16001, loss 0.0693, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 16501, loss 0.0751, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 17001, loss 0.0662, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 17501, loss 0.0845, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 18001, loss 0.0682, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 18501, loss 0.1147, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 19001, loss 0.0301, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 19501, loss 0.0357, train acc 96.51%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9603
running_time is 19.23308224
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.02
normal_0.5
./test_glass0/model_MLP_20000_0.02/record_1/MLP_20000_0.02_5
./test_glass0/result_MLP_20000_0.02_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.03/record_1/MLP_20000_0.03_1
----------------------



epoch 1, loss 0.6781, train acc 68.42%, f1 0.2703, precision 0.5556, recall 0.1786, auc 0.5545
epoch 501, loss 0.3653, train acc 84.80%, f1 0.7679, precision 0.7679, recall 0.7679, auc 0.8274
epoch 1001, loss 0.2677, train acc 87.13%, f1 0.8136, precision 0.7742, recall 0.8571, auc 0.8677
epoch 1501, loss 0.3538, train acc 88.89%, f1 0.8403, precision 0.7937, recall 0.8929, auc 0.8899
epoch 2001, loss 0.2726, train acc 90.64%, f1 0.8644, precision 0.8226, recall 0.9107, auc 0.9075
epoch 2501, loss 0.3073, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 3001, loss 0.2302, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 3501, loss 0.2382, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 4001, loss 0.1789, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 4501, loss 0.1461, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 5001, loss 0.1099, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 5501, loss 0.1082, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 6001, loss 0.2398, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 6501, loss 0.0978, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 7001, loss 0.0616, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 7501, loss 0.0492, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 8001, loss 0.0835, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 8501, loss 0.1505, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 9001, loss 0.0708, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 9501, loss 0.0681, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10001, loss 0.1479, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10501, loss 0.0398, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11001, loss 0.0995, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11501, loss 0.0514, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 12001, loss 0.0685, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12501, loss 0.0455, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 13001, loss 0.0822, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13501, loss 0.0637, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14001, loss 0.0438, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.0832, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15001, loss 0.0600, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15501, loss 0.0679, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16001, loss 0.0256, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.0700, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.0507, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.0493, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.0318, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18501, loss 0.0407, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19001, loss 0.0422, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19501, loss 0.0318, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 19.492443399000003
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.03
normal_0.5
./test_glass0/model_MLP_20000_0.03/record_1/MLP_20000_0.03_1
./test_glass0/result_MLP_20000_0.03_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.03/record_1/MLP_20000_0.03_2
----------------------



epoch 1, loss 0.6686, train acc 66.67%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4957
epoch 501, loss 0.3468, train acc 81.29%, f1 0.7143, precision 0.7143, recall 0.7143, auc 0.7876
epoch 1001, loss 0.2825, train acc 83.63%, f1 0.7500, precision 0.7500, recall 0.7500, auc 0.8141
epoch 1501, loss 0.2814, train acc 87.72%, f1 0.8174, precision 0.7966, recall 0.8393, auc 0.8675
epoch 2001, loss 0.2277, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 2501, loss 0.1687, train acc 92.98%, f1 0.8966, precision 0.8667, recall 0.9286, auc 0.9295
epoch 3001, loss 0.2104, train acc 92.98%, f1 0.8966, precision 0.8667, recall 0.9286, auc 0.9295
epoch 3501, loss 0.1657, train acc 93.57%, f1 0.9043, precision 0.8814, recall 0.9286, auc 0.9339
epoch 4001, loss 0.1903, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 4501, loss 0.1244, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 5001, loss 0.2068, train acc 95.91%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9558
epoch 5501, loss 0.1140, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 6001, loss 0.1522, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 6501, loss 0.0889, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 7001, loss 0.0563, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 7501, loss 0.0947, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 8001, loss 0.1728, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 8501, loss 0.0474, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 9001, loss 0.0520, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 9501, loss 0.0517, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 10001, loss 0.1479, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 10501, loss 0.0476, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 11001, loss 0.0641, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 11501, loss 0.0550, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12001, loss 0.0515, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12501, loss 0.1054, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13001, loss 0.0938, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13501, loss 0.0289, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14001, loss 0.0529, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 14501, loss 0.0307, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 15001, loss 0.0465, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 15501, loss 0.0628, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 16001, loss 0.0245, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 16501, loss 0.1331, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 17001, loss 0.1445, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 17501, loss 0.0616, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 18001, loss 0.0318, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 18501, loss 0.0325, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 19001, loss 0.1484, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 19501, loss 0.0468, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
running_time is 19.210690643
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.03
normal_0.5
./test_glass0/model_MLP_20000_0.03/record_1/MLP_20000_0.03_2
./test_glass0/result_MLP_20000_0.03_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.03/record_1/MLP_20000_0.03_3
----------------------



epoch 1, loss 0.6878, train acc 66.08%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4913
epoch 501, loss 0.3917, train acc 81.29%, f1 0.7288, precision 0.6935, recall 0.7679, auc 0.8013
epoch 1001, loss 0.2614, train acc 83.63%, f1 0.7627, precision 0.7258, recall 0.8036, auc 0.8279
epoch 1501, loss 0.3330, train acc 85.38%, f1 0.7863, precision 0.7541, recall 0.8214, auc 0.8455
epoch 2001, loss 0.2399, train acc 86.55%, f1 0.7965, precision 0.7895, recall 0.8036, auc 0.8496
epoch 2501, loss 0.3399, train acc 88.30%, f1 0.8276, precision 0.8000, recall 0.8571, auc 0.8764
epoch 3001, loss 0.3035, train acc 90.06%, f1 0.8522, precision 0.8305, recall 0.8750, auc 0.8940
epoch 3501, loss 0.2047, train acc 92.40%, f1 0.8870, precision 0.8644, recall 0.9107, auc 0.9206
epoch 4001, loss 0.2204, train acc 92.98%, f1 0.8929, precision 0.8929, recall 0.8929, auc 0.9203
epoch 4501, loss 0.1313, train acc 93.57%, f1 0.9027, precision 0.8947, recall 0.9107, auc 0.9293
epoch 5001, loss 0.1700, train acc 94.74%, f1 0.9204, precision 0.9123, recall 0.9286, auc 0.9425
epoch 5501, loss 0.1045, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 6001, loss 0.0796, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 6501, loss 0.0755, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 7001, loss 0.1244, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 7501, loss 0.1123, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 8001, loss 0.1082, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 8501, loss 0.1188, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 9001, loss 0.1071, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 9501, loss 0.0865, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 10001, loss 0.0561, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 10501, loss 0.0927, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 11001, loss 0.0574, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 11501, loss 0.1085, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 12001, loss 0.0341, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 12501, loss 0.0804, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 13001, loss 0.0474, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 13501, loss 0.1085, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 14001, loss 0.0627, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 14501, loss 0.0455, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 15001, loss 0.0252, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 15501, loss 0.0252, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 16001, loss 0.0420, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 16501, loss 0.0268, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 17001, loss 0.0665, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 17501, loss 0.0472, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 18001, loss 0.0582, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 18501, loss 0.0568, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 19001, loss 0.0479, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19501, loss 0.0604, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
running_time is 19.513039563
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.03
normal_0.5
./test_glass0/model_MLP_20000_0.03/record_1/MLP_20000_0.03_3
./test_glass0/result_MLP_20000_0.03_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.03/record_1/MLP_20000_0.03_4
----------------------



epoch 1, loss 0.6780, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.2847, train acc 81.29%, f1 0.7241, precision 0.7000, recall 0.7500, auc 0.7967
epoch 1001, loss 0.4500, train acc 83.04%, f1 0.7563, precision 0.7143, recall 0.8036, auc 0.8235
epoch 1501, loss 0.2658, train acc 87.13%, f1 0.8136, precision 0.7742, recall 0.8571, auc 0.8677
epoch 2001, loss 0.2938, train acc 87.72%, f1 0.8174, precision 0.7966, recall 0.8393, auc 0.8675
epoch 2501, loss 0.2493, train acc 90.06%, f1 0.8496, precision 0.8421, recall 0.8571, auc 0.8894
epoch 3001, loss 0.2819, train acc 89.47%, f1 0.8421, precision 0.8276, recall 0.8571, auc 0.8851
epoch 3501, loss 0.1654, train acc 90.64%, f1 0.8621, precision 0.8333, recall 0.8929, auc 0.9030
epoch 4001, loss 0.2064, train acc 90.64%, f1 0.8596, precision 0.8448, recall 0.8750, auc 0.8984
epoch 4501, loss 0.1379, train acc 91.23%, f1 0.8696, precision 0.8475, recall 0.8929, auc 0.9073
epoch 5001, loss 0.1112, train acc 91.23%, f1 0.8696, precision 0.8475, recall 0.8929, auc 0.9073
epoch 5501, loss 0.2069, train acc 91.81%, f1 0.8793, precision 0.8500, recall 0.9107, auc 0.9162
epoch 6001, loss 0.1805, train acc 91.81%, f1 0.8793, precision 0.8500, recall 0.9107, auc 0.9162
epoch 6501, loss 0.2253, train acc 91.81%, f1 0.8793, precision 0.8500, recall 0.9107, auc 0.9162
epoch 7001, loss 0.1890, train acc 91.81%, f1 0.8793, precision 0.8500, recall 0.9107, auc 0.9162
epoch 7501, loss 0.1305, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 8001, loss 0.1481, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 8501, loss 0.1199, train acc 91.81%, f1 0.8814, precision 0.8387, recall 0.9286, auc 0.9208
epoch 9001, loss 0.0950, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 9501, loss 0.1445, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 10001, loss 0.1535, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 10501, loss 0.1116, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 11001, loss 0.1091, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 11501, loss 0.1626, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 12001, loss 0.1799, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 12501, loss 0.1155, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 13001, loss 0.1710, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 13501, loss 0.1656, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 14001, loss 0.1672, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 14501, loss 0.1169, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 15001, loss 0.1103, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 15501, loss 0.0618, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 16001, loss 0.0388, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 16501, loss 0.1553, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 17001, loss 0.0639, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 17501, loss 0.2470, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 18001, loss 0.1150, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 18501, loss 0.0675, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 19001, loss 0.1546, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 19501, loss 0.1846, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
running_time is 19.261485109
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.03
normal_0.5
./test_glass0/model_MLP_20000_0.03/record_1/MLP_20000_0.03_4
./test_glass0/result_MLP_20000_0.03_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.03/record_1/MLP_20000_0.03_5
----------------------



epoch 1, loss 0.6733, train acc 67.44%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.4337, train acc 80.81%, f1 0.7179, precision 0.6885, recall 0.7500, auc 0.7931
epoch 1001, loss 0.2881, train acc 83.14%, f1 0.7478, precision 0.7288, recall 0.7679, auc 0.8150
epoch 1501, loss 0.2812, train acc 87.21%, f1 0.8136, precision 0.7742, recall 0.8571, auc 0.8682
epoch 2001, loss 0.3139, train acc 89.53%, f1 0.8448, precision 0.8167, recall 0.8750, auc 0.8901
epoch 2501, loss 0.2573, train acc 90.70%, f1 0.8644, precision 0.8226, recall 0.9107, auc 0.9079
epoch 3001, loss 0.2850, train acc 90.12%, f1 0.8547, precision 0.8197, recall 0.8929, auc 0.8990
epoch 3501, loss 0.2280, train acc 91.86%, f1 0.8793, precision 0.8500, recall 0.9107, auc 0.9166
epoch 4001, loss 0.2647, train acc 93.02%, f1 0.8947, precision 0.8793, recall 0.9107, auc 0.9252
epoch 4501, loss 0.1798, train acc 93.02%, f1 0.8947, precision 0.8793, recall 0.9107, auc 0.9252
epoch 5001, loss 0.1282, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 5501, loss 0.0842, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 6001, loss 0.1585, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 6501, loss 0.1396, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 7001, loss 0.1159, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 7501, loss 0.1194, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 8001, loss 0.1004, train acc 96.51%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9603
epoch 8501, loss 0.0486, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 9001, loss 0.0812, train acc 96.51%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9603
epoch 9501, loss 0.1576, train acc 96.51%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9603
epoch 10001, loss 0.0669, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 10501, loss 0.0711, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 11001, loss 0.0593, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 11501, loss 0.0618, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 12001, loss 0.0673, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 12501, loss 0.0873, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 13001, loss 0.0952, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 13501, loss 0.0727, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 14001, loss 0.0364, train acc 97.09%, f1 0.9550, precision 0.9636, recall 0.9464, auc 0.9646
epoch 14501, loss 0.0795, train acc 97.67%, f1 0.9636, precision 0.9815, recall 0.9464, auc 0.9689
epoch 15001, loss 0.0266, train acc 98.26%, f1 0.9725, precision 1.0000, recall 0.9464, auc 0.9732
epoch 15501, loss 0.0479, train acc 98.84%, f1 0.9818, precision 1.0000, recall 0.9643, auc 0.9821
epoch 16001, loss 0.0632, train acc 98.26%, f1 0.9725, precision 1.0000, recall 0.9464, auc 0.9732
epoch 16501, loss 0.0466, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0536, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0324, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 18001, loss 0.0470, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0391, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0414, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0234, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
running_time is 19.304570504
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.03
normal_0.5
./test_glass0/model_MLP_20000_0.03/record_1/MLP_20000_0.03_5
./test_glass0/result_MLP_20000_0.03_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.04/record_1/MLP_20000_0.04_1
----------------------



epoch 1, loss 0.6740, train acc 66.67%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4957
epoch 501, loss 0.3259, train acc 85.38%, f1 0.7788, precision 0.7719, recall 0.7857, auc 0.8363
epoch 1001, loss 0.2083, train acc 87.13%, f1 0.8103, precision 0.7833, recall 0.8393, auc 0.8631
epoch 1501, loss 0.2882, train acc 88.89%, f1 0.8376, precision 0.8033, recall 0.8750, auc 0.8853
epoch 2001, loss 0.2359, train acc 88.30%, f1 0.8333, precision 0.7812, recall 0.8929, auc 0.8856
epoch 2501, loss 0.2913, train acc 87.72%, f1 0.8264, precision 0.7692, recall 0.8929, auc 0.8812
epoch 3001, loss 0.2778, train acc 88.89%, f1 0.8430, precision 0.7846, recall 0.9107, auc 0.8945
epoch 3501, loss 0.2041, train acc 90.06%, f1 0.8595, precision 0.8000, recall 0.9286, auc 0.9078
epoch 4001, loss 0.1806, train acc 90.64%, f1 0.8667, precision 0.8125, recall 0.9286, auc 0.9121
epoch 4501, loss 0.3084, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 5001, loss 0.2002, train acc 92.98%, f1 0.8966, precision 0.8667, recall 0.9286, auc 0.9295
epoch 5501, loss 0.1893, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 6001, loss 0.1087, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 6501, loss 0.0946, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 7001, loss 0.1114, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 7501, loss 0.1155, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8001, loss 0.0973, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 8501, loss 0.1011, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9001, loss 0.0684, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9501, loss 0.1161, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10001, loss 0.1053, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10501, loss 0.0398, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11001, loss 0.0439, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11501, loss 0.0572, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12001, loss 0.0586, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12501, loss 0.0767, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13001, loss 0.0644, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 13501, loss 0.0766, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14001, loss 0.0591, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14501, loss 0.0809, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15001, loss 0.0743, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15501, loss 0.0561, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16001, loss 0.0641, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16501, loss 0.0521, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17001, loss 0.0475, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.0667, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18001, loss 0.0587, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18501, loss 0.0374, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.0595, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19501, loss 0.0738, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 19.287847902000003
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.04
normal_0.5
./test_glass0/model_MLP_20000_0.04/record_1/MLP_20000_0.04_1
./test_glass0/result_MLP_20000_0.04_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.04/record_1/MLP_20000_0.04_2
----------------------



epoch 1, loss 0.6675, train acc 65.50%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4870
epoch 501, loss 0.3727, train acc 83.04%, f1 0.7563, precision 0.7143, recall 0.8036, auc 0.8235
epoch 1001, loss 0.3589, train acc 87.13%, f1 0.8136, precision 0.7742, recall 0.8571, auc 0.8677
epoch 1501, loss 0.2407, train acc 87.72%, f1 0.8174, precision 0.7966, recall 0.8393, auc 0.8675
epoch 2001, loss 0.2550, train acc 91.81%, f1 0.8772, precision 0.8621, recall 0.8929, auc 0.9116
epoch 2501, loss 0.3091, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 3001, loss 0.1562, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 3501, loss 0.1649, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 4001, loss 0.1180, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 4501, loss 0.1228, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 5001, loss 0.1249, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 5501, loss 0.0916, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 6001, loss 0.1289, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 6501, loss 0.1639, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 7001, loss 0.1880, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 7501, loss 0.1071, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 8001, loss 0.1067, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 8501, loss 0.0715, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 9001, loss 0.0973, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 9501, loss 0.1153, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 10001, loss 0.0601, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 10501, loss 0.0204, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11001, loss 0.0598, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 11501, loss 0.0904, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 12001, loss 0.1107, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 12501, loss 0.0584, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 13001, loss 0.0835, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 13501, loss 0.0899, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 14001, loss 0.1010, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 14501, loss 0.0414, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 15001, loss 0.0371, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 15501, loss 0.0557, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 16001, loss 0.0664, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 16501, loss 0.0795, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 17001, loss 0.0448, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 17501, loss 0.0397, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 18001, loss 0.0496, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 18501, loss 0.0665, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 19001, loss 0.0829, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 19501, loss 0.0447, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
running_time is 19.259553301
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.04
normal_0.5
./test_glass0/model_MLP_20000_0.04/record_1/MLP_20000_0.04_2
./test_glass0/result_MLP_20000_0.04_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.04/record_1/MLP_20000_0.04_3
----------------------



epoch 1, loss 0.6643, train acc 65.50%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4870
epoch 501, loss 0.3990, train acc 81.87%, f1 0.7395, precision 0.6984, recall 0.7857, auc 0.8102
epoch 1001, loss 0.3459, train acc 81.87%, f1 0.7395, precision 0.6984, recall 0.7857, auc 0.8102
epoch 1501, loss 0.3965, train acc 85.38%, f1 0.7863, precision 0.7541, recall 0.8214, auc 0.8455
epoch 2001, loss 0.3538, train acc 86.55%, f1 0.8000, precision 0.7797, recall 0.8214, auc 0.8542
epoch 2501, loss 0.2388, train acc 90.64%, f1 0.8644, precision 0.8226, recall 0.9107, auc 0.9075
epoch 3001, loss 0.2456, train acc 92.40%, f1 0.8870, precision 0.8644, recall 0.9107, auc 0.9206
epoch 3501, loss 0.1802, train acc 92.98%, f1 0.8947, precision 0.8793, recall 0.9107, auc 0.9249
epoch 4001, loss 0.1981, train acc 91.81%, f1 0.8772, precision 0.8621, recall 0.8929, auc 0.9116
epoch 4501, loss 0.1018, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 5001, loss 0.1203, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 5501, loss 0.1374, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 6001, loss 0.1766, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 6501, loss 0.1176, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 7001, loss 0.1102, train acc 95.91%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9558
epoch 7501, loss 0.0837, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 8001, loss 0.1281, train acc 96.49%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9602
epoch 8501, loss 0.1270, train acc 97.08%, f1 0.9550, precision 0.9636, recall 0.9464, auc 0.9645
epoch 9001, loss 0.0635, train acc 95.91%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9558
epoch 9501, loss 0.0826, train acc 96.49%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9602
epoch 10001, loss 0.0581, train acc 97.08%, f1 0.9550, precision 0.9636, recall 0.9464, auc 0.9645
epoch 10501, loss 0.0483, train acc 97.08%, f1 0.9550, precision 0.9636, recall 0.9464, auc 0.9645
epoch 11001, loss 0.0432, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 11501, loss 0.0801, train acc 97.66%, f1 0.9636, precision 0.9815, recall 0.9464, auc 0.9689
epoch 12001, loss 0.0673, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 12501, loss 0.0557, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 13001, loss 0.0435, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 13501, loss 0.0545, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 14001, loss 0.0688, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 14501, loss 0.0429, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 15001, loss 0.0471, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 15501, loss 0.0371, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 16001, loss 0.0417, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 16501, loss 0.0467, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 17001, loss 0.0576, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0415, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0455, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0295, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0336, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0333, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
running_time is 19.182871448
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.04
normal_0.5
./test_glass0/model_MLP_20000_0.04/record_1/MLP_20000_0.04_3
./test_glass0/result_MLP_20000_0.04_normal_0.5/record_1/
----------------------



the AUC is 0.5357142857142857

the Fscore is 0.13333333333333333

the precision is 1.0

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.04/record_1/MLP_20000_0.04_4
----------------------



epoch 1, loss 0.6770, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.4027, train acc 81.29%, f1 0.7288, precision 0.6935, recall 0.7679, auc 0.8013
epoch 1001, loss 0.3826, train acc 83.04%, f1 0.7563, precision 0.7143, recall 0.8036, auc 0.8235
epoch 1501, loss 0.4760, train acc 84.21%, f1 0.7692, precision 0.7377, recall 0.8036, auc 0.8322
epoch 2001, loss 0.2071, train acc 87.13%, f1 0.8103, precision 0.7833, recall 0.8393, auc 0.8631
epoch 2501, loss 0.2733, train acc 88.30%, f1 0.8214, precision 0.8214, recall 0.8214, auc 0.8672
epoch 3001, loss 0.1921, train acc 88.30%, f1 0.8246, precision 0.8103, recall 0.8393, auc 0.8718
epoch 3501, loss 0.2588, train acc 89.47%, f1 0.8421, precision 0.8276, recall 0.8571, auc 0.8851
epoch 4001, loss 0.1925, train acc 91.23%, f1 0.8673, precision 0.8596, recall 0.8750, auc 0.9027
epoch 4501, loss 0.1919, train acc 91.23%, f1 0.8673, precision 0.8596, recall 0.8750, auc 0.9027
epoch 5001, loss 0.1585, train acc 91.23%, f1 0.8718, precision 0.8361, recall 0.9107, auc 0.9119
epoch 5501, loss 0.1420, train acc 91.81%, f1 0.8793, precision 0.8500, recall 0.9107, auc 0.9162
epoch 6001, loss 0.2064, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 6501, loss 0.1724, train acc 92.98%, f1 0.8966, precision 0.8667, recall 0.9286, auc 0.9295
epoch 7001, loss 0.1239, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 7501, loss 0.1071, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 8001, loss 0.1692, train acc 93.57%, f1 0.9027, precision 0.8947, recall 0.9107, auc 0.9293
epoch 8501, loss 0.1167, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 9001, loss 0.1520, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 9501, loss 0.1577, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 10001, loss 0.1269, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 10501, loss 0.1246, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 11001, loss 0.1272, train acc 96.49%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9602
epoch 11501, loss 0.1634, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 12001, loss 0.1124, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 12501, loss 0.1074, train acc 97.08%, f1 0.9550, precision 0.9636, recall 0.9464, auc 0.9645
epoch 13001, loss 0.1291, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 13501, loss 0.1237, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 14001, loss 0.1275, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 14501, loss 0.1295, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 15001, loss 0.1193, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 15501, loss 0.0877, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16001, loss 0.0946, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16501, loss 0.0525, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17001, loss 0.0659, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17501, loss 0.0967, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18001, loss 0.0703, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.0597, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.0374, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19501, loss 0.0441, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 19.149626997
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.04
normal_0.5
./test_glass0/model_MLP_20000_0.04/record_1/MLP_20000_0.04_4
./test_glass0/result_MLP_20000_0.04_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.04/record_1/MLP_20000_0.04_5
----------------------



epoch 1, loss 0.6610, train acc 69.77%, f1 0.2121, precision 0.7000, recall 0.1250, auc 0.5496
epoch 501, loss 0.3679, train acc 81.40%, f1 0.7288, precision 0.6935, recall 0.7679, auc 0.8020
epoch 1001, loss 0.4669, train acc 84.30%, f1 0.7611, precision 0.7544, recall 0.7679, auc 0.8236
epoch 1501, loss 0.3948, train acc 86.63%, f1 0.8067, precision 0.7619, recall 0.8571, auc 0.8639
epoch 2001, loss 0.2612, train acc 90.12%, f1 0.8547, precision 0.8197, recall 0.8929, auc 0.8990
epoch 2501, loss 0.2565, train acc 90.70%, f1 0.8644, precision 0.8226, recall 0.9107, auc 0.9079
epoch 3001, loss 0.2652, train acc 92.44%, f1 0.8870, precision 0.8644, recall 0.9107, auc 0.9209
epoch 3501, loss 0.2367, train acc 93.02%, f1 0.8947, precision 0.8793, recall 0.9107, auc 0.9252
epoch 4001, loss 0.1633, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 4501, loss 0.2331, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 5001, loss 0.2095, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 5501, loss 0.1510, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 6001, loss 0.1170, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 6501, loss 0.1154, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 7001, loss 0.1057, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 7501, loss 0.1188, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 8001, loss 0.0622, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 8501, loss 0.0624, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 9001, loss 0.0753, train acc 96.51%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9603
epoch 9501, loss 0.0842, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 10001, loss 0.0899, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 10501, loss 0.0603, train acc 96.51%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9603
epoch 11001, loss 0.0865, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 11501, loss 0.0849, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 12001, loss 0.1432, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 12501, loss 0.0889, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 13001, loss 0.1020, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 13501, loss 0.1100, train acc 96.51%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9603
epoch 14001, loss 0.0825, train acc 97.67%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9735
epoch 14501, loss 0.0653, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 15001, loss 0.0330, train acc 97.67%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9735
epoch 15501, loss 0.0570, train acc 97.67%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9735
epoch 16001, loss 0.0648, train acc 97.67%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9735
epoch 16501, loss 0.0333, train acc 98.26%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9825
epoch 17001, loss 0.0515, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 17501, loss 0.0550, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 18001, loss 0.0434, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 18501, loss 0.0444, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 19001, loss 0.0728, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 19501, loss 0.0381, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
running_time is 19.627652649999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.04
normal_0.5
./test_glass0/model_MLP_20000_0.04/record_1/MLP_20000_0.04_5
./test_glass0/result_MLP_20000_0.04_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.05/record_1/MLP_20000_0.05_1
----------------------



epoch 1, loss 0.6620, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.2759, train acc 83.63%, f1 0.7705, precision 0.7121, recall 0.8393, auc 0.8370
epoch 1001, loss 0.3015, train acc 85.38%, f1 0.7934, precision 0.7385, recall 0.8571, auc 0.8547
epoch 1501, loss 0.3931, train acc 87.13%, f1 0.8167, precision 0.7656, recall 0.8750, auc 0.8723
epoch 2001, loss 0.3434, train acc 89.47%, f1 0.8500, precision 0.7969, recall 0.9107, auc 0.8988
epoch 2501, loss 0.3374, train acc 89.47%, f1 0.8525, precision 0.7879, recall 0.9286, auc 0.9034
epoch 3001, loss 0.2654, train acc 90.64%, f1 0.8667, precision 0.8125, recall 0.9286, auc 0.9121
epoch 3501, loss 0.2549, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 4001, loss 0.1993, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 4501, loss 0.1991, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 5001, loss 0.0906, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 5501, loss 0.1530, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 6001, loss 0.2148, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 6501, loss 0.2098, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 7001, loss 0.2297, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 7501, loss 0.1907, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 8001, loss 0.1866, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 8501, loss 0.1051, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9001, loss 0.1415, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9501, loss 0.0856, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 10001, loss 0.1253, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 10501, loss 0.1340, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11001, loss 0.1209, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11501, loss 0.0955, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 12001, loss 0.0892, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 12501, loss 0.0739, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 13001, loss 0.0761, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13501, loss 0.1045, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14001, loss 0.0945, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14501, loss 0.1382, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15001, loss 0.1065, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15501, loss 0.0763, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16001, loss 0.0691, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16501, loss 0.0720, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17001, loss 0.0489, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17501, loss 0.0607, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.0384, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18501, loss 0.0675, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19001, loss 0.0503, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.0305, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
running_time is 19.237028289
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.05
normal_0.5
./test_glass0/model_MLP_20000_0.05/record_1/MLP_20000_0.05_1
./test_glass0/result_MLP_20000_0.05_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.05/record_1/MLP_20000_0.05_2
----------------------



epoch 1, loss 0.6580, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3485, train acc 81.87%, f1 0.7395, precision 0.6984, recall 0.7857, auc 0.8102
epoch 1001, loss 0.4429, train acc 83.04%, f1 0.7434, precision 0.7368, recall 0.7500, auc 0.8098
epoch 1501, loss 0.2480, train acc 88.30%, f1 0.8333, precision 0.7812, recall 0.8929, auc 0.8856
epoch 2001, loss 0.2340, train acc 90.06%, f1 0.8571, precision 0.8095, recall 0.9107, auc 0.9032
epoch 2501, loss 0.3124, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 3001, loss 0.1700, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 3501, loss 0.1657, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 4001, loss 0.1882, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 4501, loss 0.1281, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 5001, loss 0.1297, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 5501, loss 0.1208, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 6001, loss 0.1895, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 6501, loss 0.2249, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 7001, loss 0.1321, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 7501, loss 0.1924, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 8001, loss 0.1562, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8501, loss 0.1204, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 9001, loss 0.1303, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 9501, loss 0.1605, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 10001, loss 0.0637, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 10501, loss 0.0963, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11001, loss 0.1469, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11501, loss 0.1746, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 12001, loss 0.1754, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 12501, loss 0.1548, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 13001, loss 0.0756, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 13501, loss 0.0675, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 14001, loss 0.0847, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 14501, loss 0.1490, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 15001, loss 0.1433, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 15501, loss 0.0533, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 16001, loss 0.0656, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 16501, loss 0.0998, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 17001, loss 0.1288, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 17501, loss 0.0449, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 18001, loss 0.0356, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 18501, loss 0.0719, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 19001, loss 0.1475, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 19501, loss 0.1299, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
running_time is 19.146759267
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.05
normal_0.5
./test_glass0/model_MLP_20000_0.05/record_1/MLP_20000_0.05_2
./test_glass0/result_MLP_20000_0.05_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.05/record_1/MLP_20000_0.05_3
----------------------



epoch 1, loss 0.6662, train acc 64.33%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4783
epoch 501, loss 0.4378, train acc 81.29%, f1 0.7333, precision 0.6875, recall 0.7857, auc 0.8059
epoch 1001, loss 0.4408, train acc 84.80%, f1 0.7869, precision 0.7273, recall 0.8571, auc 0.8503
epoch 1501, loss 0.2672, train acc 84.80%, f1 0.7833, precision 0.7344, recall 0.8393, auc 0.8457
epoch 2001, loss 0.3203, train acc 88.30%, f1 0.8305, precision 0.7903, recall 0.8750, auc 0.8810
epoch 2501, loss 0.2279, train acc 88.89%, f1 0.8403, precision 0.7937, recall 0.8929, auc 0.8899
epoch 3001, loss 0.2165, train acc 90.64%, f1 0.8667, precision 0.8125, recall 0.9286, auc 0.9121
epoch 3501, loss 0.1937, train acc 92.40%, f1 0.8870, precision 0.8644, recall 0.9107, auc 0.9206
epoch 4001, loss 0.1867, train acc 91.81%, f1 0.8814, precision 0.8387, recall 0.9286, auc 0.9208
epoch 4501, loss 0.1941, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 5001, loss 0.1265, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 5501, loss 0.1144, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 6001, loss 0.1153, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 6501, loss 0.0784, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 7001, loss 0.0942, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 7501, loss 0.1072, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 8001, loss 0.0767, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 8501, loss 0.1166, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 9001, loss 0.0643, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 9501, loss 0.0785, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 10001, loss 0.0886, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 10501, loss 0.0566, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 11001, loss 0.0986, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 11501, loss 0.0499, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 12001, loss 0.0562, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 12501, loss 0.1049, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 13001, loss 0.0937, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 13501, loss 0.1079, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 14001, loss 0.0766, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 14501, loss 0.0849, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 15001, loss 0.0689, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 15501, loss 0.0620, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 16001, loss 0.0302, train acc 98.83%, f1 0.9818, precision 1.0000, recall 0.9643, auc 0.9821
epoch 16501, loss 0.0495, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 17001, loss 0.0600, train acc 98.83%, f1 0.9818, precision 1.0000, recall 0.9643, auc 0.9821
epoch 17501, loss 0.0318, train acc 98.83%, f1 0.9818, precision 1.0000, recall 0.9643, auc 0.9821
epoch 18001, loss 0.0331, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 18501, loss 0.0502, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0467, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 19501, loss 0.0448, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
running_time is 19.072855675
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.05
normal_0.5
./test_glass0/model_MLP_20000_0.05/record_1/MLP_20000_0.05_3
./test_glass0/result_MLP_20000_0.05_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.05/record_1/MLP_20000_0.05_4
----------------------



epoch 1, loss 0.6541, train acc 66.08%, f1 0.0333, precision 0.2500, recall 0.0179, auc 0.4959
epoch 501, loss 0.3847, train acc 80.70%, f1 0.7227, precision 0.6825, recall 0.7679, auc 0.7970
epoch 1001, loss 0.3492, train acc 84.80%, f1 0.7833, precision 0.7344, recall 0.8393, auc 0.8457
epoch 1501, loss 0.3053, train acc 87.13%, f1 0.8136, precision 0.7742, recall 0.8571, auc 0.8677
epoch 2001, loss 0.3332, train acc 89.47%, f1 0.8448, precision 0.8167, recall 0.8750, auc 0.8897
epoch 2501, loss 0.2852, train acc 90.06%, f1 0.8522, precision 0.8305, recall 0.8750, auc 0.8940
epoch 3001, loss 0.2714, train acc 91.23%, f1 0.8718, precision 0.8361, recall 0.9107, auc 0.9119
epoch 3501, loss 0.2955, train acc 91.81%, f1 0.8793, precision 0.8500, recall 0.9107, auc 0.9162
epoch 4001, loss 0.2025, train acc 92.98%, f1 0.8966, precision 0.8667, recall 0.9286, auc 0.9295
epoch 4501, loss 0.1864, train acc 92.98%, f1 0.8966, precision 0.8667, recall 0.9286, auc 0.9295
epoch 5001, loss 0.0835, train acc 92.98%, f1 0.8966, precision 0.8667, recall 0.9286, auc 0.9295
epoch 5501, loss 0.2098, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 6001, loss 0.2111, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 6501, loss 0.1583, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 7001, loss 0.1292, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 7501, loss 0.1279, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 8001, loss 0.1482, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 8501, loss 0.1144, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 9001, loss 0.0715, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 9501, loss 0.0908, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 10001, loss 0.1021, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 10501, loss 0.0613, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 11001, loss 0.0964, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 11501, loss 0.0971, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 12001, loss 0.0925, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 12501, loss 0.1055, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 13001, loss 0.1063, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 13501, loss 0.0488, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 14001, loss 0.0764, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 14501, loss 0.0552, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 15001, loss 0.0575, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 15501, loss 0.0706, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 16001, loss 0.0571, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 16501, loss 0.0542, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 17001, loss 0.0565, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 17501, loss 0.0571, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0537, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 18501, loss 0.0698, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0494, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0395, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
running_time is 19.257296668
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.05
normal_0.5
./test_glass0/model_MLP_20000_0.05/record_1/MLP_20000_0.05_4
./test_glass0/result_MLP_20000_0.05_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.05/record_1/MLP_20000_0.05_5
----------------------



epoch 1, loss 0.6658, train acc 71.51%, f1 0.2899, precision 0.7692, recall 0.1786, auc 0.5764
epoch 501, loss 0.4147, train acc 81.40%, f1 0.7419, precision 0.6765, recall 0.8214, auc 0.8159
epoch 1001, loss 0.3506, train acc 81.98%, f1 0.7480, precision 0.6866, recall 0.8214, auc 0.8202
epoch 1501, loss 0.3175, train acc 86.63%, f1 0.8160, precision 0.7391, recall 0.9107, auc 0.8778
epoch 2001, loss 0.2543, train acc 87.21%, f1 0.8197, precision 0.7576, recall 0.8929, auc 0.8775
epoch 2501, loss 0.2378, train acc 90.70%, f1 0.8689, precision 0.8030, recall 0.9464, auc 0.9172
epoch 3001, loss 0.2711, train acc 91.28%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9215
epoch 3501, loss 0.2260, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 4001, loss 0.1966, train acc 92.44%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9301
epoch 4501, loss 0.1315, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 5001, loss 0.1332, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 5501, loss 0.0995, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 6001, loss 0.1087, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 6501, loss 0.1372, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 7001, loss 0.1776, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 7501, loss 0.1196, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 8001, loss 0.0741, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 8501, loss 0.1306, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 9001, loss 0.1647, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 9501, loss 0.0721, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 10001, loss 0.1262, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 10501, loss 0.1254, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 11001, loss 0.1181, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 11501, loss 0.0880, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 12001, loss 0.0741, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 12501, loss 0.0984, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 13001, loss 0.0558, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 13501, loss 0.0672, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 14001, loss 0.0549, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 14501, loss 0.0933, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 15001, loss 0.0307, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 15501, loss 0.0513, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 16001, loss 0.0808, train acc 98.26%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 16501, loss 0.0921, train acc 97.67%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9735
epoch 17001, loss 0.0648, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 17501, loss 0.1255, train acc 98.26%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 18001, loss 0.0944, train acc 98.26%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 18501, loss 0.0582, train acc 97.67%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9781
epoch 19001, loss 0.0584, train acc 98.84%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9868
epoch 19501, loss 0.0599, train acc 97.67%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9735
running_time is 19.382843961
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.05
normal_0.5
./test_glass0/model_MLP_20000_0.05/record_1/MLP_20000_0.05_5
./test_glass0/result_MLP_20000_0.05_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.06/record_1/MLP_20000_0.06_1
----------------------



epoch 1, loss 0.6812, train acc 61.40%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4565
epoch 501, loss 0.3869, train acc 82.46%, f1 0.7500, precision 0.7031, recall 0.8036, auc 0.8192
epoch 1001, loss 0.3785, train acc 86.55%, f1 0.8130, precision 0.7463, recall 0.8929, auc 0.8725
epoch 1501, loss 0.2999, train acc 88.89%, f1 0.8455, precision 0.7761, recall 0.9286, auc 0.8991
epoch 2001, loss 0.2937, train acc 88.30%, f1 0.8387, precision 0.7647, recall 0.9286, auc 0.8947
epoch 2501, loss 0.2851, train acc 87.72%, f1 0.8320, precision 0.7536, recall 0.9286, auc 0.8904
epoch 3001, loss 0.2578, train acc 88.89%, f1 0.8455, precision 0.7761, recall 0.9286, auc 0.8991
epoch 3501, loss 0.2007, train acc 89.47%, f1 0.8500, precision 0.7969, recall 0.9107, auc 0.8988
epoch 4001, loss 0.2464, train acc 90.64%, f1 0.8689, precision 0.8030, recall 0.9464, auc 0.9167
epoch 4501, loss 0.1899, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 5001, loss 0.1959, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 5501, loss 0.1507, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 6001, loss 0.1170, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 6501, loss 0.1565, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 7001, loss 0.1363, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 7501, loss 0.1523, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 8001, loss 0.1056, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 8501, loss 0.1230, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9001, loss 0.1249, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9501, loss 0.1132, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10001, loss 0.1245, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10501, loss 0.1171, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11001, loss 0.0821, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11501, loss 0.0596, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12001, loss 0.0808, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12501, loss 0.0782, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13001, loss 0.0384, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13501, loss 0.1011, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14001, loss 0.0523, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14501, loss 0.0948, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.0751, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15501, loss 0.0744, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16001, loss 0.0308, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.0685, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17001, loss 0.0512, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 17501, loss 0.0750, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 18001, loss 0.0454, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 18501, loss 0.0562, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 19001, loss 0.0376, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 19501, loss 0.0543, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
running_time is 19.359411170999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.06
normal_0.5
./test_glass0/model_MLP_20000_0.06/record_1/MLP_20000_0.06_1
./test_glass0/result_MLP_20000_0.06_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.06/record_1/MLP_20000_0.06_2
----------------------



epoch 1, loss 0.6713, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3364, train acc 83.04%, f1 0.7603, precision 0.7077, recall 0.8214, auc 0.8281
epoch 1001, loss 0.3180, train acc 84.80%, f1 0.7833, precision 0.7344, recall 0.8393, auc 0.8457
epoch 1501, loss 0.2816, train acc 87.13%, f1 0.8197, precision 0.7576, recall 0.8929, auc 0.8769
epoch 2001, loss 0.2384, train acc 90.06%, f1 0.8595, precision 0.8000, recall 0.9286, auc 0.9078
epoch 2501, loss 0.2445, train acc 91.81%, f1 0.8814, precision 0.8387, recall 0.9286, auc 0.9208
epoch 3001, loss 0.1810, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 3501, loss 0.2782, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 4001, loss 0.2261, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 4501, loss 0.1101, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 5001, loss 0.1190, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 5501, loss 0.1024, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 6001, loss 0.1248, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 6501, loss 0.0489, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 7001, loss 0.1113, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 7501, loss 0.0846, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 8001, loss 0.1033, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 8501, loss 0.1807, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 9001, loss 0.0877, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 9501, loss 0.0523, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 10001, loss 0.0651, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 10501, loss 0.0828, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11001, loss 0.1101, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11501, loss 0.1095, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12001, loss 0.0323, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12501, loss 0.1147, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13001, loss 0.1358, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13501, loss 0.1099, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14001, loss 0.0410, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 14501, loss 0.0820, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15001, loss 0.1199, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 15501, loss 0.0588, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 16001, loss 0.0859, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 16501, loss 0.0802, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 17001, loss 0.1501, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 17501, loss 0.0617, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18001, loss 0.1283, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 18501, loss 0.0644, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 19001, loss 0.0443, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19501, loss 0.0888, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
running_time is 19.408506486
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.06
normal_0.5
./test_glass0/model_MLP_20000_0.06/record_1/MLP_20000_0.06_2
./test_glass0/result_MLP_20000_0.06_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.06/record_1/MLP_20000_0.06_3
----------------------



epoch 1, loss 0.6619, train acc 63.74%, f1 0.0882, precision 0.2500, recall 0.0536, auc 0.4877
epoch 501, loss 0.3801, train acc 81.29%, f1 0.7333, precision 0.6875, recall 0.7857, auc 0.8059
epoch 1001, loss 0.4256, train acc 82.46%, f1 0.7458, precision 0.7097, recall 0.7857, auc 0.8146
epoch 1501, loss 0.2990, train acc 85.38%, f1 0.7899, precision 0.7460, recall 0.8393, auc 0.8501
epoch 2001, loss 0.3024, train acc 87.72%, f1 0.8174, precision 0.7966, recall 0.8393, auc 0.8675
epoch 2501, loss 0.2627, train acc 90.06%, f1 0.8496, precision 0.8421, recall 0.8571, auc 0.8894
epoch 3001, loss 0.2152, train acc 91.81%, f1 0.8772, precision 0.8621, recall 0.8929, auc 0.9116
epoch 3501, loss 0.1892, train acc 92.40%, f1 0.8850, precision 0.8772, recall 0.8929, auc 0.9160
epoch 4001, loss 0.1876, train acc 92.40%, f1 0.8870, precision 0.8644, recall 0.9107, auc 0.9206
epoch 4501, loss 0.1072, train acc 93.57%, f1 0.9043, precision 0.8814, recall 0.9286, auc 0.9339
epoch 5001, loss 0.2179, train acc 94.15%, f1 0.9123, precision 0.8966, recall 0.9286, auc 0.9382
epoch 5501, loss 0.1446, train acc 95.91%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9558
epoch 6001, loss 0.1166, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 6501, loss 0.0647, train acc 95.91%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9558
epoch 7001, loss 0.1044, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 7501, loss 0.0439, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 8001, loss 0.0584, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 8501, loss 0.0598, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 9001, loss 0.0772, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 9501, loss 0.0667, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 10001, loss 0.0850, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 10501, loss 0.0498, train acc 98.83%, f1 0.9818, precision 1.0000, recall 0.9643, auc 0.9821
epoch 11001, loss 0.0752, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 11501, loss 0.0462, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 12001, loss 0.0708, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 12501, loss 0.0525, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 13001, loss 0.0566, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 13501, loss 0.0366, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0535, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0304, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0409, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0558, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0360, train acc 99.42%, f1 0.9910, precision 1.0000, recall 0.9821, auc 0.9911
epoch 16501, loss 0.0326, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0285, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0342, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0439, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0427, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0337, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0288, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
running_time is 19.643246050000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.06
normal_0.5
./test_glass0/model_MLP_20000_0.06/record_1/MLP_20000_0.06_3
./test_glass0/result_MLP_20000_0.06_normal_0.5/record_1/
----------------------



the AUC is 0.625615763546798

the Fscore is 0.4210526315789473

the precision is 0.8

the recall is 0.2857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.06/record_1/MLP_20000_0.06_4
----------------------



epoch 1, loss 0.6522, train acc 65.50%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4870
epoch 501, loss 0.4104, train acc 78.95%, f1 0.7049, precision 0.6515, recall 0.7679, auc 0.7839
epoch 1001, loss 0.2686, train acc 81.87%, f1 0.7520, precision 0.6812, recall 0.8393, auc 0.8240
epoch 1501, loss 0.3321, train acc 84.80%, f1 0.7833, precision 0.7344, recall 0.8393, auc 0.8457
epoch 2001, loss 0.2560, train acc 85.38%, f1 0.7934, precision 0.7385, recall 0.8571, auc 0.8547
epoch 2501, loss 0.2362, train acc 88.89%, f1 0.8403, precision 0.7937, recall 0.8929, auc 0.8899
epoch 3001, loss 0.2717, train acc 88.30%, f1 0.8333, precision 0.7812, recall 0.8929, auc 0.8856
epoch 3501, loss 0.1910, train acc 89.47%, f1 0.8500, precision 0.7969, recall 0.9107, auc 0.8988
epoch 4001, loss 0.2493, train acc 90.06%, f1 0.8595, precision 0.8000, recall 0.9286, auc 0.9078
epoch 4501, loss 0.1103, train acc 91.81%, f1 0.8814, precision 0.8387, recall 0.9286, auc 0.9208
epoch 5001, loss 0.2353, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 5501, loss 0.1632, train acc 92.98%, f1 0.8966, precision 0.8667, recall 0.9286, auc 0.9295
epoch 6001, loss 0.1507, train acc 92.98%, f1 0.8966, precision 0.8667, recall 0.9286, auc 0.9295
epoch 6501, loss 0.2331, train acc 92.98%, f1 0.8966, precision 0.8667, recall 0.9286, auc 0.9295
epoch 7001, loss 0.1876, train acc 94.15%, f1 0.9123, precision 0.8966, recall 0.9286, auc 0.9382
epoch 7501, loss 0.1125, train acc 94.15%, f1 0.9123, precision 0.8966, recall 0.9286, auc 0.9382
epoch 8001, loss 0.1562, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 8501, loss 0.1642, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 9001, loss 0.2026, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 9501, loss 0.1337, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 10001, loss 0.0906, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 10501, loss 0.1516, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 11001, loss 0.1396, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 11501, loss 0.0846, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 12001, loss 0.1749, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 12501, loss 0.1301, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 13001, loss 0.1557, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 13501, loss 0.1861, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 14001, loss 0.0712, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 14501, loss 0.0929, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 15001, loss 0.0798, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 15501, loss 0.1478, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 16001, loss 0.1360, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 16501, loss 0.1485, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 17001, loss 0.1263, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 17501, loss 0.0972, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 18001, loss 0.1577, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 18501, loss 0.0923, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 19001, loss 0.1041, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 19501, loss 0.0915, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
running_time is 19.346152281000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.06
normal_0.5
./test_glass0/model_MLP_20000_0.06/record_1/MLP_20000_0.06_4
./test_glass0/result_MLP_20000_0.06_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.06/record_1/MLP_20000_0.06_5
----------------------



epoch 1, loss 0.6473, train acc 62.21%, f1 0.0580, precision 0.1538, recall 0.0357, auc 0.4704
epoch 501, loss 0.3935, train acc 79.65%, f1 0.7107, precision 0.6615, recall 0.7679, auc 0.7891
epoch 1001, loss 0.3346, train acc 81.40%, f1 0.7377, precision 0.6818, recall 0.8036, auc 0.8113
epoch 1501, loss 0.3267, train acc 84.88%, f1 0.7869, precision 0.7273, recall 0.8571, auc 0.8510
epoch 2001, loss 0.2782, train acc 86.63%, f1 0.8099, precision 0.7538, recall 0.8750, auc 0.8685
epoch 2501, loss 0.3034, train acc 86.05%, f1 0.7895, precision 0.7759, recall 0.8036, auc 0.8458
epoch 3001, loss 0.2224, train acc 88.37%, f1 0.8305, precision 0.7903, recall 0.8750, auc 0.8815
epoch 3501, loss 0.2016, train acc 89.53%, f1 0.8475, precision 0.8065, recall 0.8929, auc 0.8947
epoch 4001, loss 0.2755, train acc 91.86%, f1 0.8793, precision 0.8500, recall 0.9107, auc 0.9166
epoch 4501, loss 0.2451, train acc 91.86%, f1 0.8793, precision 0.8500, recall 0.9107, auc 0.9166
epoch 5001, loss 0.2000, train acc 91.28%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9169
epoch 5501, loss 0.1620, train acc 92.44%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9255
epoch 6001, loss 0.2207, train acc 92.44%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9301
epoch 6501, loss 0.1565, train acc 91.86%, f1 0.8814, precision 0.8387, recall 0.9286, auc 0.9212
epoch 7001, loss 0.2242, train acc 92.44%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9301
epoch 7501, loss 0.2537, train acc 92.44%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9301
epoch 8001, loss 0.1814, train acc 93.02%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9344
epoch 8501, loss 0.0826, train acc 93.02%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9344
epoch 9001, loss 0.1421, train acc 93.02%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9344
epoch 9501, loss 0.1465, train acc 93.60%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9387
epoch 10001, loss 0.1326, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 10501, loss 0.1536, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 11001, loss 0.0922, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 11501, loss 0.1375, train acc 93.02%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9344
epoch 12001, loss 0.1535, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 12501, loss 0.1769, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 13001, loss 0.1281, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 13501, loss 0.0765, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 14001, loss 0.1172, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 14501, loss 0.1112, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 15001, loss 0.1198, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 15501, loss 0.1075, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 16001, loss 0.0992, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 16501, loss 0.0600, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 17001, loss 0.1526, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 17501, loss 0.0675, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 18001, loss 0.0607, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 18501, loss 0.0896, train acc 97.67%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9735
epoch 19001, loss 0.0574, train acc 97.67%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9735
epoch 19501, loss 0.0496, train acc 97.67%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9735
running_time is 19.243810141999997
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.06
normal_0.5
./test_glass0/model_MLP_20000_0.06/record_1/MLP_20000_0.06_5
./test_glass0/result_MLP_20000_0.06_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.07/record_1/MLP_20000_0.07_1
----------------------



epoch 1, loss 0.6580, train acc 64.91%, f1 0.0323, precision 0.1667, recall 0.0179, auc 0.4872
epoch 501, loss 0.3830, train acc 82.46%, f1 0.7581, precision 0.6912, recall 0.8393, auc 0.8283
epoch 1001, loss 0.2890, train acc 87.13%, f1 0.8197, precision 0.7576, recall 0.8929, auc 0.8769
epoch 1501, loss 0.3374, train acc 88.89%, f1 0.8455, precision 0.7761, recall 0.9286, auc 0.8991
epoch 2001, loss 0.2328, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 2501, loss 0.1766, train acc 90.64%, f1 0.8689, precision 0.8030, recall 0.9464, auc 0.9167
epoch 3001, loss 0.1698, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 3501, loss 0.2861, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 4001, loss 0.1540, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 4501, loss 0.1990, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 5001, loss 0.0601, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 5501, loss 0.0739, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 6001, loss 0.0722, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 6501, loss 0.1113, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 7001, loss 0.0978, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 7501, loss 0.0417, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 8001, loss 0.1076, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 8501, loss 0.0843, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 9001, loss 0.0462, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 9501, loss 0.0809, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10001, loss 0.0858, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10501, loss 0.1339, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11001, loss 0.1393, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11501, loss 0.0841, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12001, loss 0.1354, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12501, loss 0.0284, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13001, loss 0.1320, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13501, loss 0.0734, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14001, loss 0.0853, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.0220, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15001, loss 0.0247, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.1328, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.0279, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.0813, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.0848, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.0235, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.1800, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.0887, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.0766, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.0839, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 19.337374414000003
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.07
normal_0.5
./test_glass0/model_MLP_20000_0.07/record_1/MLP_20000_0.07_1
./test_glass0/result_MLP_20000_0.07_normal_0.5/record_1/
----------------------



the AUC is 0.5049261083743841

the Fscore is 0.30769230769230765

the precision is 0.3333333333333333

the recall is 0.2857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.07/record_1/MLP_20000_0.07_2
----------------------



epoch 1, loss 0.6411, train acc 64.91%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4826
epoch 501, loss 0.3558, train acc 83.04%, f1 0.7563, precision 0.7143, recall 0.8036, auc 0.8235
epoch 1001, loss 0.2019, train acc 84.80%, f1 0.7759, precision 0.7500, recall 0.8036, auc 0.8366
epoch 1501, loss 0.3389, train acc 85.96%, f1 0.7931, precision 0.7667, recall 0.8214, auc 0.8498
epoch 2001, loss 0.2276, train acc 89.47%, f1 0.8448, precision 0.8167, recall 0.8750, auc 0.8897
epoch 2501, loss 0.2048, train acc 90.64%, f1 0.8621, precision 0.8333, recall 0.8929, auc 0.9030
epoch 3001, loss 0.1340, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 3501, loss 0.1309, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 4001, loss 0.1804, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 4501, loss 0.0886, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 5001, loss 0.1158, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 5501, loss 0.1221, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 6001, loss 0.1947, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 6501, loss 0.1438, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 7001, loss 0.1762, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 7501, loss 0.0942, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 8001, loss 0.1194, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 8501, loss 0.0553, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 9001, loss 0.1173, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 9501, loss 0.0840, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 10001, loss 0.0828, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 10501, loss 0.0918, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11001, loss 0.0617, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 11501, loss 0.1278, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12001, loss 0.1286, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12501, loss 0.1011, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13001, loss 0.1009, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13501, loss 0.0840, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14001, loss 0.0713, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14501, loss 0.0527, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 15001, loss 0.0965, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15501, loss 0.1297, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16001, loss 0.0429, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.1114, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 17001, loss 0.0382, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17501, loss 0.0437, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.0602, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.0953, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19001, loss 0.1012, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.1065, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
running_time is 19.598102973
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.07
normal_0.5
./test_glass0/model_MLP_20000_0.07/record_1/MLP_20000_0.07_2
./test_glass0/result_MLP_20000_0.07_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.07/record_1/MLP_20000_0.07_3
----------------------



epoch 1, loss 0.6694, train acc 55.56%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4130
epoch 501, loss 0.3810, train acc 80.70%, f1 0.7317, precision 0.6716, recall 0.8036, auc 0.8061
epoch 1001, loss 0.4431, train acc 84.80%, f1 0.7869, precision 0.7273, recall 0.8571, auc 0.8503
epoch 1501, loss 0.3598, train acc 84.80%, f1 0.7869, precision 0.7273, recall 0.8571, auc 0.8503
epoch 2001, loss 0.2775, train acc 88.30%, f1 0.8276, precision 0.8000, recall 0.8571, auc 0.8764
epoch 2501, loss 0.2936, train acc 90.64%, f1 0.8644, precision 0.8226, recall 0.9107, auc 0.9075
epoch 3001, loss 0.2054, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 3501, loss 0.1493, train acc 92.40%, f1 0.8870, precision 0.8644, recall 0.9107, auc 0.9206
epoch 4001, loss 0.1910, train acc 91.81%, f1 0.8793, precision 0.8500, recall 0.9107, auc 0.9162
epoch 4501, loss 0.1827, train acc 92.98%, f1 0.8966, precision 0.8667, recall 0.9286, auc 0.9295
epoch 5001, loss 0.1363, train acc 92.98%, f1 0.8966, precision 0.8667, recall 0.9286, auc 0.9295
epoch 5501, loss 0.1854, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 6001, loss 0.1337, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 6501, loss 0.1430, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 7001, loss 0.1758, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 7501, loss 0.1210, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 8001, loss 0.0828, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 8501, loss 0.1780, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 9001, loss 0.1179, train acc 95.91%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9558
epoch 9501, loss 0.0934, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 10001, loss 0.1528, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 10501, loss 0.1040, train acc 96.49%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9602
epoch 11001, loss 0.1329, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 11501, loss 0.1339, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 12001, loss 0.0631, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 12501, loss 0.1100, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 13001, loss 0.1057, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 13501, loss 0.0643, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 14001, loss 0.0887, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 14501, loss 0.1214, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 15001, loss 0.1060, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 15501, loss 0.0768, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 16001, loss 0.0787, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 16501, loss 0.0523, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 17001, loss 0.0864, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 17501, loss 0.0966, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 18001, loss 0.0607, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 18501, loss 0.1045, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 19001, loss 0.1091, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 19501, loss 0.0872, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
running_time is 19.201328263
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.07
normal_0.5
./test_glass0/model_MLP_20000_0.07/record_1/MLP_20000_0.07_3
./test_glass0/result_MLP_20000_0.07_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.07/record_1/MLP_20000_0.07_4
----------------------



epoch 1, loss 0.6918, train acc 46.20%, f1 0.5490, precision 0.3784, recall 1.0000, auc 0.6000
epoch 501, loss 0.3298, train acc 79.53%, f1 0.7200, precision 0.6522, recall 0.8036, auc 0.7974
epoch 1001, loss 0.3861, train acc 83.04%, f1 0.7642, precision 0.7015, recall 0.8393, auc 0.8327
epoch 1501, loss 0.2834, train acc 84.80%, f1 0.7869, precision 0.7273, recall 0.8571, auc 0.8503
epoch 2001, loss 0.2113, train acc 86.55%, f1 0.8067, precision 0.7619, recall 0.8571, auc 0.8634
epoch 2501, loss 0.3183, train acc 88.30%, f1 0.8305, precision 0.7903, recall 0.8750, auc 0.8810
epoch 3001, loss 0.2270, train acc 89.47%, f1 0.8500, precision 0.7969, recall 0.9107, auc 0.8988
epoch 3501, loss 0.2889, train acc 90.06%, f1 0.8571, precision 0.8095, recall 0.9107, auc 0.9032
epoch 4001, loss 0.2789, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 4501, loss 0.2117, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 5001, loss 0.2178, train acc 91.81%, f1 0.8814, precision 0.8387, recall 0.9286, auc 0.9208
epoch 5501, loss 0.2484, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 6001, loss 0.2324, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 6501, loss 0.1725, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 7001, loss 0.1708, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 7501, loss 0.1882, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 8001, loss 0.1454, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 8501, loss 0.2250, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 9001, loss 0.1670, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 9501, loss 0.1756, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 10001, loss 0.1656, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 10501, loss 0.1165, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 11001, loss 0.0939, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 11501, loss 0.1562, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 12001, loss 0.1366, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 12501, loss 0.0676, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 13001, loss 0.1166, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 13501, loss 0.1459, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 14001, loss 0.0668, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 14501, loss 0.0578, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 15001, loss 0.0992, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 15501, loss 0.1285, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 16001, loss 0.0994, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 16501, loss 0.0649, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 17001, loss 0.1026, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 17501, loss 0.1428, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 18001, loss 0.1348, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 18501, loss 0.1566, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 19001, loss 0.1190, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 19501, loss 0.1375, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
running_time is 19.446414565
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.07
normal_0.5
./test_glass0/model_MLP_20000_0.07/record_1/MLP_20000_0.07_4
./test_glass0/result_MLP_20000_0.07_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.07/record_1/MLP_20000_0.07_5
----------------------



epoch 1, loss 0.6806, train acc 65.12%, f1 0.0323, precision 0.1667, recall 0.0179, auc 0.4874
epoch 501, loss 0.3624, train acc 79.65%, f1 0.7059, precision 0.6667, recall 0.7500, auc 0.7845
epoch 1001, loss 0.2683, train acc 83.72%, f1 0.7742, precision 0.7059, recall 0.8571, auc 0.8424
epoch 1501, loss 0.3184, train acc 87.79%, f1 0.8320, precision 0.7536, recall 0.9286, auc 0.8910
epoch 2001, loss 0.2962, train acc 87.79%, f1 0.8293, precision 0.7612, recall 0.9107, auc 0.8864
epoch 2501, loss 0.3106, train acc 90.70%, f1 0.8667, precision 0.8125, recall 0.9286, auc 0.9126
epoch 3001, loss 0.2883, train acc 90.12%, f1 0.8571, precision 0.8095, recall 0.9107, auc 0.9036
epoch 3501, loss 0.1884, train acc 90.12%, f1 0.8571, precision 0.8095, recall 0.9107, auc 0.9036
epoch 4001, loss 0.1814, train acc 92.44%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9301
epoch 4501, loss 0.1488, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 5001, loss 0.1410, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 5501, loss 0.1893, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 6001, loss 0.1141, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 6501, loss 0.0875, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 7001, loss 0.0718, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 7501, loss 0.1957, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 8001, loss 0.2033, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 8501, loss 0.0913, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 9001, loss 0.1400, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 9501, loss 0.1956, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 10001, loss 0.0967, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 10501, loss 0.0907, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 11001, loss 0.1241, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 11501, loss 0.0738, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 12001, loss 0.1511, train acc 96.51%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9603
epoch 12501, loss 0.0987, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 13001, loss 0.1298, train acc 96.51%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9603
epoch 13501, loss 0.0890, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 14001, loss 0.1156, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 14501, loss 0.1342, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 15001, loss 0.0944, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 15501, loss 0.0926, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 16001, loss 0.1198, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 16501, loss 0.0515, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 17001, loss 0.0767, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 17501, loss 0.0573, train acc 96.51%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9603
epoch 18001, loss 0.0965, train acc 96.51%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9603
epoch 18501, loss 0.1049, train acc 97.67%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9735
epoch 19001, loss 0.0627, train acc 97.67%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9735
epoch 19501, loss 0.0507, train acc 97.67%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9735
running_time is 18.967845766
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.07
normal_0.5
./test_glass0/model_MLP_20000_0.07/record_1/MLP_20000_0.07_5
./test_glass0/result_MLP_20000_0.07_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_20000_0.08/record_1/MLP_20000_0.08_1
----------------------



epoch 1, loss 0.6351, train acc 66.08%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4913
epoch 501, loss 0.3317, train acc 84.80%, f1 0.7903, precision 0.7206, recall 0.8750, auc 0.8549
epoch 1001, loss 0.3376, train acc 85.96%, f1 0.8125, precision 0.7222, recall 0.9286, auc 0.8773
epoch 1501, loss 0.2506, train acc 88.30%, f1 0.8387, precision 0.7647, recall 0.9286, auc 0.8947
epoch 2001, loss 0.3259, train acc 89.47%, f1 0.8525, precision 0.7879, recall 0.9286, auc 0.9034
epoch 2501, loss 0.3092, train acc 90.06%, f1 0.8595, precision 0.8000, recall 0.9286, auc 0.9078
epoch 3001, loss 0.1907, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 3501, loss 0.2941, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 4001, loss 0.1494, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 4501, loss 0.1638, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 5001, loss 0.1670, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 5501, loss 0.1140, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 6001, loss 0.1406, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 6501, loss 0.1346, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 7001, loss 0.0942, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 7501, loss 0.1179, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 8001, loss 0.1283, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 8501, loss 0.1130, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9001, loss 0.0968, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9501, loss 0.0824, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10001, loss 0.1068, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10501, loss 0.0858, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11001, loss 0.0444, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11501, loss 0.1917, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12001, loss 0.0342, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12501, loss 0.1421, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13001, loss 0.1314, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13501, loss 0.0952, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14001, loss 0.0321, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.0339, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15001, loss 0.1356, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.0803, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.1357, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.1290, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.1265, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.0839, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.0784, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.1323, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.1220, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.0937, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 19.113037711
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_20000_0.08
normal_0.5
./test_glass0/model_MLP_20000_0.08/record_1/MLP_20000_0.08_1
./test_glass0/result_MLP_20000_0.08_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_20000_0.08/record_1/MLP_20000_0.08_2
----------------------



epoch 1, loss 0.6674, train acc 66.08%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4913
epoch 501, loss 0.3452, train acc 82.46%, f1 0.7541, precision 0.6970, recall 0.8214, auc 0.8238
epoch 1001, loss 0.2996, train acc 85.38%, f1 0.7863, precision 0.7541, recall 0.8214, auc 0.8455
epoch 1501, loss 0.3455, train acc 88.89%, f1 0.8455, precision 0.7761, recall 0.9286, auc 0.8991
epoch 2001, loss 0.2092, train acc 90.06%, f1 0.8595, precision 0.8000, recall 0.9286, auc 0.9078
epoch 2501, loss 0.3393, train acc 91.23%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9210
epoch 3001, loss 0.3191, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 3501, loss 0.1779, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 4001, loss 0.2182, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 4501, loss 0.1447, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 5001, loss 0.1047, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 5501, loss 0.0980, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 6001, loss 0.1622, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 6501, loss 0.1203, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 7001, loss 0.1409, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 7501, loss 0.0718, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 8001, loss 0.0746, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 8501, loss 0.0697, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 9001, loss 0.1079, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 9501, loss 0.1276, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 10001, loss 0.1361, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 10501, loss 0.0518, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11001, loss 0.0867, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 11501, loss 0.0721, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12001, loss 0.0956, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12501, loss 0.0949, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13001, loss 0.1299, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13501, loss 0.1218, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14001, loss 0.0376, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14501, loss 0.0747, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 15001, loss 0.0952, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 15501, loss 0.0357, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 16001, loss 0.0406, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16501, loss 0.0456, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17001, loss 0.0741, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17501, loss 0.0440, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18001, loss 0.0907, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18501, loss 0.1185, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19001, loss 0.0770, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19501, loss 0.0491, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
running_time is 18.920095201000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_20000_0.08
normal_0.5
./test_glass0/model_MLP_20000_0.08/record_1/MLP_20000_0.08_2
./test_glass0/result_MLP_20000_0.08_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_20000_0.08/record_1/MLP_20000_0.08_3
----------------------



epoch 1, loss 0.6673, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3832, train acc 80.70%, f1 0.7360, precision 0.6667, recall 0.8214, auc 0.8107
epoch 1001, loss 0.4516, train acc 83.63%, f1 0.7705, precision 0.7121, recall 0.8393, auc 0.8370
epoch 1501, loss 0.2794, train acc 84.80%, f1 0.7869, precision 0.7273, recall 0.8571, auc 0.8503
epoch 2001, loss 0.3297, train acc 87.13%, f1 0.8136, precision 0.7742, recall 0.8571, auc 0.8677
epoch 2501, loss 0.2817, train acc 88.89%, f1 0.8430, precision 0.7846, recall 0.9107, auc 0.8945
epoch 3001, loss 0.2732, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 3501, loss 0.2258, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 4001, loss 0.1994, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 4501, loss 0.1310, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 5001, loss 0.1907, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 5501, loss 0.1112, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 6001, loss 0.1606, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 6501, loss 0.0731, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 7001, loss 0.1277, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 7501, loss 0.1082, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 8001, loss 0.0761, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 8501, loss 0.1087, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 9001, loss 0.1247, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 9501, loss 0.1230, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 10001, loss 0.0548, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 10501, loss 0.0816, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 11001, loss 0.0790, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 11501, loss 0.0827, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 12001, loss 0.1267, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 12501, loss 0.0842, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 13001, loss 0.1136, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 13501, loss 0.1404, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 14001, loss 0.1514, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 14501, loss 0.1454, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 15001, loss 0.0487, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 15501, loss 0.0885, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 16001, loss 0.1224, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 16501, loss 0.0682, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 17001, loss 0.0832, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 17501, loss 0.0737, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18001, loss 0.0833, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 18501, loss 0.0447, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19001, loss 0.1137, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.1360, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
running_time is 19.001988357000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_20000_0.08
normal_0.5
./test_glass0/model_MLP_20000_0.08/record_1/MLP_20000_0.08_3
./test_glass0/result_MLP_20000_0.08_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_20000_0.08/record_1/MLP_20000_0.08_4
----------------------



epoch 1, loss 0.6802, train acc 72.51%, f1 0.3562, precision 0.7647, recall 0.2321, auc 0.5987
epoch 501, loss 0.3926, train acc 78.95%, f1 0.7097, precision 0.6471, recall 0.7857, auc 0.7885
epoch 1001, loss 0.3921, train acc 83.63%, f1 0.7705, precision 0.7121, recall 0.8393, auc 0.8370
epoch 1501, loss 0.3259, train acc 84.80%, f1 0.7869, precision 0.7273, recall 0.8571, auc 0.8503
epoch 2001, loss 0.3532, train acc 84.80%, f1 0.7903, precision 0.7206, recall 0.8750, auc 0.8549
epoch 2501, loss 0.3068, train acc 87.13%, f1 0.8136, precision 0.7742, recall 0.8571, auc 0.8677
epoch 3001, loss 0.2401, train acc 89.47%, f1 0.8500, precision 0.7969, recall 0.9107, auc 0.8988
epoch 3501, loss 0.2460, train acc 90.64%, f1 0.8667, precision 0.8125, recall 0.9286, auc 0.9121
epoch 4001, loss 0.1988, train acc 90.64%, f1 0.8667, precision 0.8125, recall 0.9286, auc 0.9121
epoch 4501, loss 0.2370, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 5001, loss 0.1616, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 5501, loss 0.2162, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 6001, loss 0.2428, train acc 91.81%, f1 0.8793, precision 0.8500, recall 0.9107, auc 0.9162
epoch 6501, loss 0.2000, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 7001, loss 0.1963, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 7501, loss 0.1374, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 8001, loss 0.2262, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 8501, loss 0.1163, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 9001, loss 0.1348, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 9501, loss 0.0995, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 10001, loss 0.2396, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 10501, loss 0.1829, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 11001, loss 0.1127, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 11501, loss 0.1534, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 12001, loss 0.1269, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 12501, loss 0.1930, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 13001, loss 0.1195, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 13501, loss 0.1681, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 14001, loss 0.1170, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 14501, loss 0.0540, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 15001, loss 0.1259, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 15501, loss 0.0875, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 16001, loss 0.1685, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 16501, loss 0.2693, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 17001, loss 0.1525, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 17501, loss 0.0982, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 18001, loss 0.0948, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 18501, loss 0.0993, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 19001, loss 0.0858, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 19501, loss 0.1504, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
running_time is 18.936066618
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_20000_0.08
normal_0.5
./test_glass0/model_MLP_20000_0.08/record_1/MLP_20000_0.08_4
./test_glass0/result_MLP_20000_0.08_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_20000_0.08/record_1/MLP_20000_0.08_5
----------------------



epoch 1, loss 0.6416, train acc 66.28%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4914
epoch 501, loss 0.3250, train acc 80.81%, f1 0.7317, precision 0.6716, recall 0.8036, auc 0.8070
epoch 1001, loss 0.2827, train acc 81.98%, f1 0.7480, precision 0.6866, recall 0.8214, auc 0.8202
epoch 1501, loss 0.3741, train acc 86.05%, f1 0.8125, precision 0.7222, recall 0.9286, auc 0.8781
epoch 2001, loss 0.3455, train acc 87.21%, f1 0.8281, precision 0.7361, recall 0.9464, auc 0.8913
epoch 2501, loss 0.2118, train acc 90.12%, f1 0.8595, precision 0.8000, recall 0.9286, auc 0.9083
epoch 3001, loss 0.3236, train acc 90.12%, f1 0.8547, precision 0.8197, recall 0.8929, auc 0.8990
epoch 3501, loss 0.1455, train acc 93.02%, f1 0.8966, precision 0.8667, recall 0.9286, auc 0.9298
epoch 4001, loss 0.1735, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 4501, loss 0.1669, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 5001, loss 0.1789, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 5501, loss 0.1582, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 6001, loss 0.2274, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 6501, loss 0.1177, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 7001, loss 0.1575, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 7501, loss 0.1096, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 8001, loss 0.1114, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 8501, loss 0.1089, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 9001, loss 0.1288, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 9501, loss 0.0799, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 10001, loss 0.1062, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 10501, loss 0.2066, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 11001, loss 0.1204, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 11501, loss 0.1228, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 12001, loss 0.0872, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 12501, loss 0.0967, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 13001, loss 0.0973, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 13501, loss 0.1157, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 14001, loss 0.1303, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 14501, loss 0.1240, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 15001, loss 0.0663, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 15501, loss 0.1008, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 16001, loss 0.0698, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 16501, loss 0.1450, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 17001, loss 0.1221, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 17501, loss 0.0749, train acc 97.67%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9781
epoch 18001, loss 0.0635, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 18501, loss 0.0702, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 19001, loss 0.0848, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 19501, loss 0.0688, train acc 97.67%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9781
running_time is 18.895949932999997
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_20000_0.08
normal_0.5
./test_glass0/model_MLP_20000_0.08/record_1/MLP_20000_0.08_5
./test_glass0/result_MLP_20000_0.08_normal_0.5/record_1/
----------------------



the AUC is 0.5357142857142857

the Fscore is 0.13333333333333333

the precision is 1.0

the recall is 0.07142857142857142

Done
