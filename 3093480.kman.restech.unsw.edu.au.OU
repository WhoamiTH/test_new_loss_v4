/home/z5102138/anaconda3/envs/py36/bin/python
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.07/record_1/MLP_25000_0.07_1
----------------------



epoch 1, loss 0.6298, train acc 65.50%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4870
epoch 501, loss 0.3930, train acc 83.63%, f1 0.7705, precision 0.7121, recall 0.8393, auc 0.8370
epoch 1001, loss 0.4552, train acc 86.55%, f1 0.8099, precision 0.7538, recall 0.8750, auc 0.8679
epoch 1501, loss 0.3739, train acc 87.72%, f1 0.8320, precision 0.7536, recall 0.9286, auc 0.8904
epoch 2001, loss 0.3499, train acc 90.64%, f1 0.8667, precision 0.8125, recall 0.9286, auc 0.9121
epoch 2501, loss 0.2257, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 3001, loss 0.2465, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 3501, loss 0.1480, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 4001, loss 0.2321, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 4501, loss 0.1322, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 5001, loss 0.1376, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 5501, loss 0.1596, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 6001, loss 0.1721, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 6501, loss 0.1210, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 7001, loss 0.1043, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 7501, loss 0.0654, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 8001, loss 0.1052, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 8501, loss 0.1328, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 9001, loss 0.1159, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 9501, loss 0.0395, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10001, loss 0.0600, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10501, loss 0.0425, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11001, loss 0.1443, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11501, loss 0.0484, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12001, loss 0.0604, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12501, loss 0.0869, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13001, loss 0.0456, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13501, loss 0.0390, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14001, loss 0.0513, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.0712, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15001, loss 0.0840, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.1045, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16001, loss 0.0524, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.0604, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.0727, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.0438, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.0474, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.0585, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.1008, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.0531, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20001, loss 0.0877, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 20501, loss 0.0414, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 21001, loss 0.0337, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 21501, loss 0.1007, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 22001, loss 0.0256, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22501, loss 0.0869, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 23001, loss 0.0331, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 23501, loss 0.0353, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24001, loss 0.0382, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24501, loss 0.0457, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 25.468092735
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.07
normal_0.5
./test_glass0/model_MLP_25000_0.07/record_1/MLP_25000_0.07_1
./test_glass0/result_MLP_25000_0.07_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.07/record_1/MLP_25000_0.07_2
----------------------



epoch 1, loss 0.6692, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.2943, train acc 83.63%, f1 0.7705, precision 0.7121, recall 0.8393, auc 0.8370
epoch 1001, loss 0.3009, train acc 84.80%, f1 0.7797, precision 0.7419, recall 0.8214, auc 0.8411
epoch 1501, loss 0.3279, train acc 87.72%, f1 0.8264, precision 0.7692, recall 0.8929, auc 0.8812
epoch 2001, loss 0.3101, train acc 88.89%, f1 0.8455, precision 0.7761, recall 0.9286, auc 0.8991
epoch 2501, loss 0.2228, train acc 91.23%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9210
epoch 3001, loss 0.2177, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 3501, loss 0.1618, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 4001, loss 0.1657, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 4501, loss 0.1807, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 5001, loss 0.1670, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 5501, loss 0.0777, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 6001, loss 0.0830, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 6501, loss 0.1425, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 7001, loss 0.1036, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 7501, loss 0.1283, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 8001, loss 0.1233, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 8501, loss 0.1531, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9001, loss 0.1308, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9501, loss 0.1070, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10001, loss 0.1580, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10501, loss 0.1528, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 11001, loss 0.1004, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11501, loss 0.0976, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 12001, loss 0.0710, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 12501, loss 0.0667, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 13001, loss 0.1162, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13501, loss 0.1205, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 14001, loss 0.0589, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 14501, loss 0.0652, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 15001, loss 0.1303, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15501, loss 0.1245, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 16001, loss 0.1237, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 16501, loss 0.1568, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 17001, loss 0.1093, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 17501, loss 0.1487, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18001, loss 0.0946, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 18501, loss 0.1230, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 19001, loss 0.1030, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 19501, loss 0.0556, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 20001, loss 0.0474, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 20501, loss 0.1153, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 21001, loss 0.1024, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 21501, loss 0.0690, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 22001, loss 0.0992, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 22501, loss 0.0702, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 23001, loss 0.0542, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 23501, loss 0.0556, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 24001, loss 0.0923, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 24501, loss 0.0644, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
running_time is 25.331897954
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.07
normal_0.5
./test_glass0/model_MLP_25000_0.07/record_1/MLP_25000_0.07_2
./test_glass0/result_MLP_25000_0.07_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.07/record_1/MLP_25000_0.07_3
----------------------



epoch 1, loss 0.6691, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3692, train acc 81.87%, f1 0.7395, precision 0.6984, recall 0.7857, auc 0.8102
epoch 1001, loss 0.3182, train acc 84.21%, f1 0.7769, precision 0.7231, recall 0.8393, auc 0.8414
epoch 1501, loss 0.3679, train acc 84.80%, f1 0.7833, precision 0.7344, recall 0.8393, auc 0.8457
epoch 2001, loss 0.2061, train acc 87.13%, f1 0.8103, precision 0.7833, recall 0.8393, auc 0.8631
epoch 2501, loss 0.2751, train acc 88.89%, f1 0.8430, precision 0.7846, recall 0.9107, auc 0.8945
epoch 3001, loss 0.2726, train acc 89.47%, f1 0.8448, precision 0.8167, recall 0.8750, auc 0.8897
epoch 3501, loss 0.2194, train acc 90.06%, f1 0.8547, precision 0.8197, recall 0.8929, auc 0.8986
epoch 4001, loss 0.2141, train acc 91.81%, f1 0.8793, precision 0.8500, recall 0.9107, auc 0.9162
epoch 4501, loss 0.1292, train acc 92.40%, f1 0.8870, precision 0.8644, recall 0.9107, auc 0.9206
epoch 5001, loss 0.1439, train acc 93.57%, f1 0.9043, precision 0.8814, recall 0.9286, auc 0.9339
epoch 5501, loss 0.1541, train acc 95.32%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9515
epoch 6001, loss 0.0807, train acc 96.49%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9602
epoch 6501, loss 0.1288, train acc 95.91%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9558
epoch 7001, loss 0.1206, train acc 95.91%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9558
epoch 7501, loss 0.1252, train acc 96.49%, f1 0.9464, precision 0.9464, recall 0.9464, auc 0.9602
epoch 8001, loss 0.0937, train acc 95.91%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9558
epoch 8501, loss 0.1051, train acc 95.91%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9558
epoch 9001, loss 0.0509, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 9501, loss 0.1235, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 10001, loss 0.0961, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 10501, loss 0.1520, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 11001, loss 0.0842, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 11501, loss 0.0762, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 12001, loss 0.1393, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 12501, loss 0.0978, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 13001, loss 0.1098, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 13501, loss 0.0954, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 14001, loss 0.1214, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 14501, loss 0.0845, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 15001, loss 0.0578, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 15501, loss 0.0329, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16001, loss 0.0489, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16501, loss 0.0312, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 17001, loss 0.0530, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 17501, loss 0.1043, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 18001, loss 0.0731, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 18501, loss 0.1195, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19001, loss 0.0456, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 19501, loss 0.0692, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 20001, loss 0.0993, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 20501, loss 0.0517, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 21001, loss 0.0675, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 21501, loss 0.0466, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22001, loss 0.0478, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 22501, loss 0.0606, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 23001, loss 0.0501, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23501, loss 0.0605, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 24001, loss 0.0306, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24501, loss 0.0477, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 25.526493990000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.07
normal_0.5
./test_glass0/model_MLP_25000_0.07/record_1/MLP_25000_0.07_3
./test_glass0/result_MLP_25000_0.07_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.07/record_1/MLP_25000_0.07_4
----------------------



epoch 1, loss 0.6862, train acc 41.52%, f1 0.5238, precision 0.3571, recall 0.9821, auc 0.5606
epoch 501, loss 0.3443, train acc 79.53%, f1 0.7287, precision 0.6438, recall 0.8393, auc 0.8066
epoch 1001, loss 0.3423, train acc 83.04%, f1 0.7603, precision 0.7077, recall 0.8214, auc 0.8281
epoch 1501, loss 0.2258, train acc 84.21%, f1 0.7805, precision 0.7164, recall 0.8571, auc 0.8460
epoch 2001, loss 0.3062, train acc 87.13%, f1 0.8167, precision 0.7656, recall 0.8750, auc 0.8723
epoch 2501, loss 0.2246, train acc 90.06%, f1 0.8547, precision 0.8197, recall 0.8929, auc 0.8986
epoch 3001, loss 0.2203, train acc 90.64%, f1 0.8644, precision 0.8226, recall 0.9107, auc 0.9075
epoch 3501, loss 0.2222, train acc 90.06%, f1 0.8571, precision 0.8095, recall 0.9107, auc 0.9032
epoch 4001, loss 0.1910, train acc 91.81%, f1 0.8793, precision 0.8500, recall 0.9107, auc 0.9162
epoch 4501, loss 0.1761, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 5001, loss 0.2350, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 5501, loss 0.1655, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 6001, loss 0.1234, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 6501, loss 0.1579, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 7001, loss 0.2436, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 7501, loss 0.2602, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 8001, loss 0.2002, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 8501, loss 0.1595, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 9001, loss 0.1645, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 9501, loss 0.1840, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 10001, loss 0.1618, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 10501, loss 0.1369, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 11001, loss 0.1353, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 11501, loss 0.1092, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 12001, loss 0.1850, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 12501, loss 0.1571, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 13001, loss 0.1630, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 13501, loss 0.1971, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 14001, loss 0.1798, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 14501, loss 0.1831, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 15001, loss 0.1453, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 15501, loss 0.1973, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 16001, loss 0.1794, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 16501, loss 0.1516, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 17001, loss 0.1528, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 17501, loss 0.1376, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 18001, loss 0.0969, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 18501, loss 0.2382, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 19001, loss 0.1315, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 19501, loss 0.1428, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 20001, loss 0.1522, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 20501, loss 0.1743, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 21001, loss 0.1623, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 21501, loss 0.1118, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 22001, loss 0.0755, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 22501, loss 0.1764, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 23001, loss 0.1938, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 23501, loss 0.2326, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 24001, loss 0.1339, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 24501, loss 0.1764, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
running_time is 25.573905751
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.07
normal_0.5
./test_glass0/model_MLP_25000_0.07/record_1/MLP_25000_0.07_4
./test_glass0/result_MLP_25000_0.07_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.07/record_1/MLP_25000_0.07_5
----------------------



epoch 1, loss 0.6355, train acc 59.30%, f1 0.0541, precision 0.1111, recall 0.0357, auc 0.4489
epoch 501, loss 0.3814, train acc 79.65%, f1 0.7244, precision 0.6479, recall 0.8214, auc 0.8030
epoch 1001, loss 0.3730, train acc 81.40%, f1 0.7419, precision 0.6765, recall 0.8214, auc 0.8159
epoch 1501, loss 0.4221, train acc 83.72%, f1 0.7742, precision 0.7059, recall 0.8571, auc 0.8424
epoch 2001, loss 0.3197, train acc 84.88%, f1 0.7937, precision 0.7143, recall 0.8929, auc 0.8602
epoch 2501, loss 0.3180, train acc 86.63%, f1 0.8130, precision 0.7463, recall 0.8929, auc 0.8732
epoch 3001, loss 0.3190, train acc 85.47%, f1 0.7934, precision 0.7385, recall 0.8571, auc 0.8553
epoch 3501, loss 0.2503, train acc 87.79%, f1 0.8264, precision 0.7692, recall 0.8929, auc 0.8818
epoch 4001, loss 0.2425, train acc 88.95%, f1 0.8430, precision 0.7846, recall 0.9107, auc 0.8950
epoch 4501, loss 0.2011, train acc 90.12%, f1 0.8595, precision 0.8000, recall 0.9286, auc 0.9083
epoch 5001, loss 0.2533, train acc 91.28%, f1 0.8718, precision 0.8361, recall 0.9107, auc 0.9123
epoch 5501, loss 0.2002, train acc 91.86%, f1 0.8814, precision 0.8387, recall 0.9286, auc 0.9212
epoch 6001, loss 0.2278, train acc 91.86%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9304
epoch 6501, loss 0.1827, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 7001, loss 0.1765, train acc 93.60%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9387
epoch 7501, loss 0.1689, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 8001, loss 0.1534, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 8501, loss 0.1222, train acc 92.44%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9301
epoch 9001, loss 0.1606, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 9501, loss 0.1977, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 10001, loss 0.1633, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 10501, loss 0.1914, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 11001, loss 0.1529, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 11501, loss 0.2305, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 12001, loss 0.0940, train acc 93.60%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9387
epoch 12501, loss 0.1645, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 13001, loss 0.1252, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 13501, loss 0.1892, train acc 93.60%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9387
epoch 14001, loss 0.1353, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 14501, loss 0.1821, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 15001, loss 0.1206, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 15501, loss 0.0804, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 16001, loss 0.0777, train acc 94.19%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9430
epoch 16501, loss 0.1989, train acc 93.02%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9344
epoch 17001, loss 0.1204, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 17501, loss 0.0979, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 18001, loss 0.1516, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 18501, loss 0.0688, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 19001, loss 0.0829, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 19501, loss 0.1353, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 20001, loss 0.0699, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 20501, loss 0.1164, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 21001, loss 0.0839, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 21501, loss 0.1108, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 22001, loss 0.1013, train acc 95.93%, f1 0.9381, precision 0.9298, recall 0.9464, auc 0.9560
epoch 22501, loss 0.1613, train acc 94.77%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9474
epoch 23001, loss 0.1121, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 23501, loss 0.0605, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 24001, loss 0.1878, train acc 95.35%, f1 0.9298, precision 0.9138, recall 0.9464, auc 0.9517
epoch 24501, loss 0.0980, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
running_time is 25.535370055
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.07
normal_0.5
./test_glass0/model_MLP_25000_0.07/record_1/MLP_25000_0.07_5
./test_glass0/result_MLP_25000_0.07_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.08/record_1/MLP_25000_0.08_1
----------------------



epoch 1, loss 0.6416, train acc 66.67%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4957
epoch 501, loss 0.3445, train acc 83.63%, f1 0.7705, precision 0.7121, recall 0.8393, auc 0.8370
epoch 1001, loss 0.2875, train acc 87.72%, f1 0.8320, precision 0.7536, recall 0.9286, auc 0.8904
epoch 1501, loss 0.2651, train acc 88.30%, f1 0.8387, precision 0.7647, recall 0.9286, auc 0.8947
epoch 2001, loss 0.3392, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 2501, loss 0.3068, train acc 91.23%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9210
epoch 3001, loss 0.1954, train acc 91.23%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9210
epoch 3501, loss 0.1768, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 4001, loss 0.2361, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 4501, loss 0.2071, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 5001, loss 0.1216, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 5501, loss 0.1798, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 6001, loss 0.0984, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 6501, loss 0.2278, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 7001, loss 0.0598, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 7501, loss 0.0785, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 8001, loss 0.1061, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 8501, loss 0.1582, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 9001, loss 0.1183, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 9501, loss 0.1024, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10001, loss 0.1356, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10501, loss 0.0760, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11001, loss 0.1035, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11501, loss 0.1234, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12001, loss 0.0387, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12501, loss 0.0813, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13001, loss 0.0821, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13501, loss 0.1032, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14001, loss 0.1402, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14501, loss 0.1058, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15001, loss 0.1162, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15501, loss 0.0592, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 16001, loss 0.1459, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16501, loss 0.0385, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.0973, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17501, loss 0.0490, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.0815, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18501, loss 0.0398, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19001, loss 0.0876, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19501, loss 0.0867, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 20001, loss 0.0393, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20501, loss 0.0874, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 21001, loss 0.0458, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21501, loss 0.0335, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 22001, loss 0.0968, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 22501, loss 0.0983, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23001, loss 0.0303, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23501, loss 0.0310, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 24001, loss 0.0315, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 24501, loss 0.0493, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 25.76478917
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.08
normal_0.5
./test_glass0/model_MLP_25000_0.08/record_1/MLP_25000_0.08_1
./test_glass0/result_MLP_25000_0.08_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.08/record_1/MLP_25000_0.08_2
----------------------



epoch 1, loss 0.6546, train acc 66.08%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4913
epoch 501, loss 0.3446, train acc 83.63%, f1 0.7705, precision 0.7121, recall 0.8393, auc 0.8370
epoch 1001, loss 0.4061, train acc 84.21%, f1 0.7731, precision 0.7302, recall 0.8214, auc 0.8368
epoch 1501, loss 0.2136, train acc 88.30%, f1 0.8361, precision 0.7727, recall 0.9107, auc 0.8901
epoch 2001, loss 0.2329, train acc 88.89%, f1 0.8455, precision 0.7761, recall 0.9286, auc 0.8991
epoch 2501, loss 0.3162, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 3001, loss 0.2213, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 3501, loss 0.1522, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 4001, loss 0.1872, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 4501, loss 0.1837, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 5001, loss 0.1403, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 5501, loss 0.1149, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 6001, loss 0.1177, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 6501, loss 0.2267, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 7001, loss 0.1332, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 7501, loss 0.0906, train acc 94.74%, f1 0.9217, precision 0.8983, recall 0.9464, auc 0.9471
epoch 8001, loss 0.0478, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 8501, loss 0.0925, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 9001, loss 0.1350, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 9501, loss 0.1456, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 10001, loss 0.0632, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 10501, loss 0.0983, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 11001, loss 0.0515, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 11501, loss 0.1200, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 12001, loss 0.1106, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 12501, loss 0.0728, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 13001, loss 0.0868, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 13501, loss 0.0491, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 14001, loss 0.1087, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 14501, loss 0.0939, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 15001, loss 0.0836, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 15501, loss 0.0662, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 16001, loss 0.0684, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 16501, loss 0.0700, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 17001, loss 0.1095, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 17501, loss 0.1341, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 18001, loss 0.0867, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 18501, loss 0.1102, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 19001, loss 0.0651, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 19501, loss 0.0504, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 20001, loss 0.0579, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 20501, loss 0.0598, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 21001, loss 0.1198, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 21501, loss 0.0731, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 22001, loss 0.0715, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 22501, loss 0.0298, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 23001, loss 0.0623, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 23501, loss 0.0835, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 24001, loss 0.0662, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 24501, loss 0.1174, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 25.330748499000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.08
normal_0.5
./test_glass0/model_MLP_25000_0.08/record_1/MLP_25000_0.08_2
./test_glass0/result_MLP_25000_0.08_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.08/record_1/MLP_25000_0.08_3
----------------------



epoch 1, loss 0.6480, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3141, train acc 81.29%, f1 0.7419, precision 0.6765, recall 0.8214, auc 0.8151
epoch 1001, loss 0.3405, train acc 83.04%, f1 0.7680, precision 0.6957, recall 0.8571, auc 0.8373
epoch 1501, loss 0.3562, train acc 87.72%, f1 0.8235, precision 0.7778, recall 0.8750, auc 0.8766
epoch 2001, loss 0.2611, train acc 88.30%, f1 0.8305, precision 0.7903, recall 0.8750, auc 0.8810
epoch 2501, loss 0.3438, train acc 89.47%, f1 0.8500, precision 0.7969, recall 0.9107, auc 0.8988
epoch 3001, loss 0.1809, train acc 89.47%, f1 0.8475, precision 0.8065, recall 0.8929, auc 0.8943
epoch 3501, loss 0.2285, train acc 90.06%, f1 0.8571, precision 0.8095, recall 0.9107, auc 0.9032
epoch 4001, loss 0.2079, train acc 90.64%, f1 0.8644, precision 0.8226, recall 0.9107, auc 0.9075
epoch 4501, loss 0.2706, train acc 91.81%, f1 0.8814, precision 0.8387, recall 0.9286, auc 0.9208
epoch 5001, loss 0.1403, train acc 92.98%, f1 0.8966, precision 0.8667, recall 0.9286, auc 0.9295
epoch 5501, loss 0.1259, train acc 93.57%, f1 0.9043, precision 0.8814, recall 0.9286, auc 0.9339
epoch 6001, loss 0.1407, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 6501, loss 0.1905, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 7001, loss 0.0623, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 7501, loss 0.1189, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 8001, loss 0.0934, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 8501, loss 0.0734, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 9001, loss 0.0955, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 9501, loss 0.0909, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 10001, loss 0.1160, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 10501, loss 0.1091, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 11001, loss 0.1406, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 11501, loss 0.0890, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 12001, loss 0.0724, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 12501, loss 0.0747, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 13001, loss 0.0568, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 13501, loss 0.0668, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 14001, loss 0.1115, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 14501, loss 0.1115, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 15001, loss 0.0894, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 15501, loss 0.0717, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 16001, loss 0.0524, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 16501, loss 0.0447, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 17001, loss 0.0619, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 17501, loss 0.0686, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18001, loss 0.0518, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18501, loss 0.0742, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19001, loss 0.0621, train acc 98.25%, f1 0.9730, precision 0.9818, recall 0.9643, auc 0.9778
epoch 19501, loss 0.0633, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 20001, loss 0.1053, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 20501, loss 0.0449, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 21001, loss 0.0613, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 21501, loss 0.0778, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 22001, loss 0.0478, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 22501, loss 0.0364, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 23001, loss 0.0913, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 23501, loss 0.1002, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 24001, loss 0.0478, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 24501, loss 0.0527, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
running_time is 25.752706637000003
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.08
normal_0.5
./test_glass0/model_MLP_25000_0.08/record_1/MLP_25000_0.08_3
./test_glass0/result_MLP_25000_0.08_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.08/record_1/MLP_25000_0.08_4
----------------------



epoch 1, loss 0.6157, train acc 66.67%, f1 0.0952, precision 0.4286, recall 0.0536, auc 0.5094
epoch 501, loss 0.3500, train acc 79.53%, f1 0.7200, precision 0.6522, recall 0.8036, auc 0.7974
epoch 1001, loss 0.3514, train acc 83.04%, f1 0.7642, precision 0.7015, recall 0.8393, auc 0.8327
epoch 1501, loss 0.3239, train acc 84.21%, f1 0.7805, precision 0.7164, recall 0.8571, auc 0.8460
epoch 2001, loss 0.2842, train acc 86.55%, f1 0.8130, precision 0.7463, recall 0.8929, auc 0.8725
epoch 2501, loss 0.2651, train acc 88.89%, f1 0.8430, precision 0.7846, recall 0.9107, auc 0.8945
epoch 3001, loss 0.2488, train acc 88.89%, f1 0.8430, precision 0.7846, recall 0.9107, auc 0.8945
epoch 3501, loss 0.1777, train acc 90.64%, f1 0.8644, precision 0.8226, recall 0.9107, auc 0.9075
epoch 4001, loss 0.1989, train acc 90.06%, f1 0.8595, precision 0.8000, recall 0.9286, auc 0.9078
epoch 4501, loss 0.2185, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 5001, loss 0.1740, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 5501, loss 0.1683, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 6001, loss 0.2125, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 6501, loss 0.2084, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 7001, loss 0.1179, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 7501, loss 0.1025, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 8001, loss 0.2106, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 8501, loss 0.0618, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 9001, loss 0.1703, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 9501, loss 0.1917, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 10001, loss 0.0934, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 10501, loss 0.0778, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 11001, loss 0.0392, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 11501, loss 0.0881, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 12001, loss 0.1084, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 12501, loss 0.0980, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 13001, loss 0.0920, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 13501, loss 0.1024, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14001, loss 0.0798, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 14501, loss 0.0676, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 15001, loss 0.0835, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15501, loss 0.0516, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.0626, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.0799, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.0810, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0785, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18001, loss 0.0585, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18501, loss 0.0705, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0680, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0490, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 20001, loss 0.0658, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 20501, loss 0.0650, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 21001, loss 0.0332, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 21501, loss 0.0429, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 22001, loss 0.0754, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 22501, loss 0.0513, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 23001, loss 0.0365, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 23501, loss 0.0676, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 24001, loss 0.0349, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 24501, loss 0.0515, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
running_time is 25.457461596
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.08
normal_0.5
./test_glass0/model_MLP_25000_0.08/record_1/MLP_25000_0.08_4
./test_glass0/result_MLP_25000_0.08_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.08/record_1/MLP_25000_0.08_5
----------------------



epoch 1, loss 0.6352, train acc 62.79%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4655
epoch 501, loss 0.4250, train acc 81.40%, f1 0.7460, precision 0.6714, recall 0.8393, auc 0.8205
epoch 1001, loss 0.3395, train acc 83.14%, f1 0.7752, precision 0.6849, recall 0.8929, auc 0.8473
epoch 1501, loss 0.3148, train acc 84.88%, f1 0.7937, precision 0.7143, recall 0.8929, auc 0.8602
epoch 2001, loss 0.3304, train acc 85.47%, f1 0.8000, precision 0.7246, recall 0.8929, auc 0.8645
epoch 2501, loss 0.3048, train acc 85.47%, f1 0.7934, precision 0.7385, recall 0.8571, auc 0.8553
epoch 3001, loss 0.2497, train acc 87.79%, f1 0.8320, precision 0.7536, recall 0.9286, auc 0.8910
epoch 3501, loss 0.2016, train acc 87.79%, f1 0.8293, precision 0.7612, recall 0.9107, auc 0.8864
epoch 4001, loss 0.1702, train acc 89.53%, f1 0.8525, precision 0.7879, recall 0.9286, auc 0.9039
epoch 4501, loss 0.2675, train acc 91.28%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9215
epoch 5001, loss 0.2017, train acc 91.28%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9215
epoch 5501, loss 0.2688, train acc 91.28%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9215
epoch 6001, loss 0.1763, train acc 92.44%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9301
epoch 6501, loss 0.1697, train acc 92.44%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9301
epoch 7001, loss 0.1771, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 7501, loss 0.1786, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 8001, loss 0.2043, train acc 93.02%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9344
epoch 8501, loss 0.2256, train acc 93.60%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9387
epoch 9001, loss 0.1081, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 9501, loss 0.1584, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 10001, loss 0.1463, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 10501, loss 0.1706, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 11001, loss 0.1529, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 11501, loss 0.2139, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 12001, loss 0.1257, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 12501, loss 0.1222, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 13001, loss 0.1933, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 13501, loss 0.1095, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 14001, loss 0.0910, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 14501, loss 0.1160, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 15001, loss 0.0607, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 15501, loss 0.1010, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 16001, loss 0.1140, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 16501, loss 0.0852, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 17001, loss 0.1354, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 17501, loss 0.0402, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 18001, loss 0.1363, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 18501, loss 0.1104, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 19001, loss 0.1681, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 19501, loss 0.0637, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 20001, loss 0.0682, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 20501, loss 0.0623, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 21001, loss 0.0481, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 21501, loss 0.0799, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 22001, loss 0.0621, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 22501, loss 0.1126, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 23001, loss 0.1061, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 23501, loss 0.1236, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 24001, loss 0.0414, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 24501, loss 0.0715, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
running_time is 25.520337495
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.08
normal_0.5
./test_glass0/model_MLP_25000_0.08/record_1/MLP_25000_0.08_5
./test_glass0/result_MLP_25000_0.08_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.09/record_1/MLP_25000_0.09_1
----------------------



epoch 1, loss 0.6441, train acc 66.67%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4957
epoch 501, loss 0.4594, train acc 83.04%, f1 0.7752, precision 0.6849, recall 0.8929, auc 0.8464
epoch 1001, loss 0.3941, train acc 84.80%, f1 0.7969, precision 0.7083, recall 0.9107, auc 0.8641
epoch 1501, loss 0.3068, train acc 86.55%, f1 0.8189, precision 0.7324, recall 0.9286, auc 0.8817
epoch 2001, loss 0.2439, train acc 87.13%, f1 0.8254, precision 0.7429, recall 0.9286, auc 0.8860
epoch 2501, loss 0.2788, train acc 87.72%, f1 0.8320, precision 0.7536, recall 0.9286, auc 0.8904
epoch 3001, loss 0.1977, train acc 88.30%, f1 0.8387, precision 0.7647, recall 0.9286, auc 0.8947
epoch 3501, loss 0.2638, train acc 88.30%, f1 0.8387, precision 0.7647, recall 0.9286, auc 0.8947
epoch 4001, loss 0.1992, train acc 89.47%, f1 0.8548, precision 0.7794, recall 0.9464, auc 0.9080
epoch 4501, loss 0.2283, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 5001, loss 0.2084, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 5501, loss 0.1442, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 6001, loss 0.1368, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 6501, loss 0.1347, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 7001, loss 0.1650, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 7501, loss 0.1808, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 8001, loss 0.1929, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 8501, loss 0.1327, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9001, loss 0.0877, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9501, loss 0.1021, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10001, loss 0.1465, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10501, loss 0.1176, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11001, loss 0.1102, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11501, loss 0.1117, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12001, loss 0.0566, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12501, loss 0.0984, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13001, loss 0.0963, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13501, loss 0.0961, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14001, loss 0.0948, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14501, loss 0.1125, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.0858, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.0797, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.0787, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16501, loss 0.1052, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.0673, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.1020, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.0894, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.1093, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.0787, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.0552, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20001, loss 0.0561, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20501, loss 0.0669, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21001, loss 0.1160, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21501, loss 0.0467, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22001, loss 0.1181, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22501, loss 0.0681, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23001, loss 0.0779, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23501, loss 0.0993, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24001, loss 0.0424, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24501, loss 0.0628, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 25.372914693000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.09
normal_0.5
./test_glass0/model_MLP_25000_0.09/record_1/MLP_25000_0.09_1
./test_glass0/result_MLP_25000_0.09_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.09/record_1/MLP_25000_0.09_2
----------------------



epoch 1, loss 0.6733, train acc 63.16%, f1 0.2588, precision 0.3793, recall 0.1964, auc 0.5200
epoch 501, loss 0.3664, train acc 83.04%, f1 0.7603, precision 0.7077, recall 0.8214, auc 0.8281
epoch 1001, loss 0.3506, train acc 84.21%, f1 0.7652, precision 0.7458, recall 0.7857, auc 0.8276
epoch 1501, loss 0.3676, train acc 87.72%, f1 0.8264, precision 0.7692, recall 0.8929, auc 0.8812
epoch 2001, loss 0.2682, train acc 88.89%, f1 0.8455, precision 0.7761, recall 0.9286, auc 0.8991
epoch 2501, loss 0.2625, train acc 91.23%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9210
epoch 3001, loss 0.1684, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 3501, loss 0.1973, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 4001, loss 0.2274, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 4501, loss 0.2362, train acc 94.15%, f1 0.9138, precision 0.8833, recall 0.9464, auc 0.9428
epoch 5001, loss 0.1058, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 5501, loss 0.1067, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 6001, loss 0.1677, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 6501, loss 0.0720, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 7001, loss 0.0790, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 7501, loss 0.1507, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 8001, loss 0.1386, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 8501, loss 0.0583, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 9001, loss 0.1406, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 9501, loss 0.0858, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 10001, loss 0.0752, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 10501, loss 0.0608, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 11001, loss 0.0676, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 11501, loss 0.0476, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 12001, loss 0.0890, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 12501, loss 0.0996, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 13001, loss 0.0856, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 13501, loss 0.0803, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 14001, loss 0.0532, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 14501, loss 0.0528, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 15001, loss 0.0450, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 15501, loss 0.0761, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 16001, loss 0.0475, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 16501, loss 0.0816, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17001, loss 0.0494, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17501, loss 0.0682, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18001, loss 0.0410, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18501, loss 0.0402, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19001, loss 0.0413, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19501, loss 0.0270, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 20001, loss 0.0680, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 20501, loss 0.0294, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 21001, loss 0.0462, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 21501, loss 0.0365, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 22001, loss 0.0768, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 22501, loss 0.0307, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23001, loss 0.0733, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23501, loss 0.0396, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 24001, loss 0.0353, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 24501, loss 0.0662, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 25.567673923
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.09
normal_0.5
./test_glass0/model_MLP_25000_0.09/record_1/MLP_25000_0.09_2
./test_glass0/result_MLP_25000_0.09_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.09/record_1/MLP_25000_0.09_3
----------------------



epoch 1, loss 0.6806, train acc 32.75%, f1 0.4934, precision 0.3275, recall 1.0000, auc 0.5000
epoch 501, loss 0.3709, train acc 80.70%, f1 0.7360, precision 0.6667, recall 0.8214, auc 0.8107
epoch 1001, loss 0.3466, train acc 84.21%, f1 0.7805, precision 0.7164, recall 0.8571, auc 0.8460
epoch 1501, loss 0.3626, train acc 85.38%, f1 0.7934, precision 0.7385, recall 0.8571, auc 0.8547
epoch 2001, loss 0.2540, train acc 87.72%, f1 0.8293, precision 0.7612, recall 0.9107, auc 0.8858
epoch 2501, loss 0.2781, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 3001, loss 0.2640, train acc 91.23%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9210
epoch 3501, loss 0.1959, train acc 91.81%, f1 0.8814, precision 0.8387, recall 0.9286, auc 0.9208
epoch 4001, loss 0.1358, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 4501, loss 0.2113, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 5001, loss 0.1757, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 5501, loss 0.1642, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 6001, loss 0.1387, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 6501, loss 0.1490, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 7001, loss 0.1700, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 7501, loss 0.0936, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 8001, loss 0.1069, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 8501, loss 0.1618, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 9001, loss 0.0866, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 9501, loss 0.0794, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 10001, loss 0.1086, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 10501, loss 0.1440, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 11001, loss 0.1271, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 11501, loss 0.0765, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 12001, loss 0.0942, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 12501, loss 0.0926, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 13001, loss 0.0747, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 13501, loss 0.1313, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 14001, loss 0.0501, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 14501, loss 0.1162, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 15001, loss 0.0761, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 15501, loss 0.0626, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16001, loss 0.0885, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16501, loss 0.0797, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17001, loss 0.0525, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 17501, loss 0.0749, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 18001, loss 0.0620, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18501, loss 0.0309, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19001, loss 0.1144, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19501, loss 0.0857, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 20001, loss 0.0570, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 20501, loss 0.0468, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 21001, loss 0.0535, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 21501, loss 0.0935, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 22001, loss 0.0811, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 22501, loss 0.0579, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 23001, loss 0.0340, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 23501, loss 0.0422, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24001, loss 0.0932, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24501, loss 0.0369, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 25.510529980999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.09
normal_0.5
./test_glass0/model_MLP_25000_0.09/record_1/MLP_25000_0.09_3
./test_glass0/result_MLP_25000_0.09_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.09/record_1/MLP_25000_0.09_4
----------------------



epoch 1, loss 0.6588, train acc 61.40%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4565
epoch 501, loss 0.3784, train acc 79.53%, f1 0.7287, precision 0.6438, recall 0.8393, auc 0.8066
epoch 1001, loss 0.2979, train acc 81.87%, f1 0.7520, precision 0.6812, recall 0.8393, auc 0.8240
epoch 1501, loss 0.3956, train acc 83.63%, f1 0.7742, precision 0.7059, recall 0.8571, auc 0.8416
epoch 2001, loss 0.2615, train acc 87.13%, f1 0.8167, precision 0.7656, recall 0.8750, auc 0.8723
epoch 2501, loss 0.1694, train acc 89.47%, f1 0.8475, precision 0.8065, recall 0.8929, auc 0.8943
epoch 3001, loss 0.1917, train acc 90.64%, f1 0.8621, precision 0.8333, recall 0.8929, auc 0.9030
epoch 3501, loss 0.1312, train acc 91.81%, f1 0.8793, precision 0.8500, recall 0.9107, auc 0.9162
epoch 4001, loss 0.2382, train acc 91.23%, f1 0.8718, precision 0.8361, recall 0.9107, auc 0.9119
epoch 4501, loss 0.1399, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 5001, loss 0.1794, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 5501, loss 0.1168, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 6001, loss 0.0955, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 6501, loss 0.1576, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 7001, loss 0.1705, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 7501, loss 0.1267, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 8001, loss 0.1306, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 8501, loss 0.2621, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 9001, loss 0.1546, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 9501, loss 0.2287, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 10001, loss 0.2911, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 10501, loss 0.1671, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 11001, loss 0.1100, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 11501, loss 0.0630, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 12001, loss 0.1622, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 12501, loss 0.1569, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13001, loss 0.0959, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13501, loss 0.1990, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14001, loss 0.1442, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14501, loss 0.0849, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15001, loss 0.1075, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15501, loss 0.1359, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 16001, loss 0.0932, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 16501, loss 0.1293, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 17001, loss 0.1254, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 17501, loss 0.0476, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18001, loss 0.1208, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18501, loss 0.0655, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.0667, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.1082, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20001, loss 0.1035, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20501, loss 0.0384, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21001, loss 0.0595, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 21501, loss 0.0459, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 22001, loss 0.0472, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 22501, loss 0.0993, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23001, loss 0.0945, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23501, loss 0.1443, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 24001, loss 0.0335, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 24501, loss 0.1047, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 25.533297695999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.09
normal_0.5
./test_glass0/model_MLP_25000_0.09/record_1/MLP_25000_0.09_4
./test_glass0/result_MLP_25000_0.09_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.09/record_1/MLP_25000_0.09_5
----------------------



epoch 1, loss 0.6659, train acc 69.77%, f1 0.3158, precision 0.6000, recall 0.2143, auc 0.5727
epoch 501, loss 0.3879, train acc 80.81%, f1 0.7360, precision 0.6667, recall 0.8214, auc 0.8116
epoch 1001, loss 0.3603, train acc 81.98%, f1 0.7559, precision 0.6761, recall 0.8571, auc 0.8294
epoch 1501, loss 0.2556, train acc 84.88%, f1 0.7937, precision 0.7143, recall 0.8929, auc 0.8602
epoch 2001, loss 0.3669, train acc 84.88%, f1 0.7969, precision 0.7083, recall 0.9107, auc 0.8648
epoch 2501, loss 0.3918, train acc 86.05%, f1 0.8095, precision 0.7286, recall 0.9107, auc 0.8735
epoch 3001, loss 0.3278, train acc 86.05%, f1 0.8033, precision 0.7424, recall 0.8750, auc 0.8642
epoch 3501, loss 0.2537, train acc 87.79%, f1 0.8320, precision 0.7536, recall 0.9286, auc 0.8910
epoch 4001, loss 0.2390, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 4501, loss 0.1652, train acc 91.86%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9304
epoch 5001, loss 0.1697, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 5501, loss 0.1430, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 6001, loss 0.1299, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 6501, loss 0.2799, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 7001, loss 0.0779, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 7501, loss 0.1970, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 8001, loss 0.1474, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 8501, loss 0.1708, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 9001, loss 0.1348, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 9501, loss 0.1519, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 10001, loss 0.1125, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 10501, loss 0.1178, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 11001, loss 0.1101, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 11501, loss 0.1510, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 12001, loss 0.1440, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 12501, loss 0.1130, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 13001, loss 0.0690, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 13501, loss 0.1012, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 14001, loss 0.1093, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 14501, loss 0.0695, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 15001, loss 0.1265, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 15501, loss 0.1108, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 16001, loss 0.0948, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 16501, loss 0.0623, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 17001, loss 0.0897, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 17501, loss 0.0991, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 18001, loss 0.0907, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 18501, loss 0.0807, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 19001, loss 0.0827, train acc 97.67%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9781
epoch 19501, loss 0.0811, train acc 98.26%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9871
epoch 20001, loss 0.0516, train acc 98.26%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9871
epoch 20501, loss 0.1214, train acc 98.26%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9871
epoch 21001, loss 0.1123, train acc 97.67%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9781
epoch 21501, loss 0.0520, train acc 97.67%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9781
epoch 22001, loss 0.0987, train acc 98.26%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9825
epoch 22501, loss 0.0523, train acc 98.84%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9914
epoch 23001, loss 0.1037, train acc 98.84%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9914
epoch 23501, loss 0.0913, train acc 98.26%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9825
epoch 24001, loss 0.0816, train acc 98.84%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9914
epoch 24501, loss 0.0662, train acc 98.84%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9914
running_time is 25.484777396000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.09
normal_0.5
./test_glass0/model_MLP_25000_0.09/record_1/MLP_25000_0.09_5
./test_glass0/result_MLP_25000_0.09_normal_0.5/record_1/
----------------------



the AUC is 0.5357142857142857

the Fscore is 0.13333333333333333

the precision is 1.0

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.1/record_1/MLP_25000_0.1_1
----------------------



epoch 1, loss 0.6487, train acc 66.08%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4913
epoch 501, loss 0.2900, train acc 84.21%, f1 0.7874, precision 0.7042, recall 0.8929, auc 0.8551
epoch 1001, loss 0.3393, train acc 85.96%, f1 0.8125, precision 0.7222, recall 0.9286, auc 0.8773
epoch 1501, loss 0.2788, train acc 87.72%, f1 0.8320, precision 0.7536, recall 0.9286, auc 0.8904
epoch 2001, loss 0.2820, train acc 86.55%, f1 0.8189, precision 0.7324, recall 0.9286, auc 0.8817
epoch 2501, loss 0.2915, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 3001, loss 0.2736, train acc 91.23%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9210
epoch 3501, loss 0.1830, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 4001, loss 0.1685, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 4501, loss 0.1644, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 5001, loss 0.1511, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 5501, loss 0.2170, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 6001, loss 0.1381, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 6501, loss 0.1682, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 7001, loss 0.0988, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 7501, loss 0.1366, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 8001, loss 0.1121, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 8501, loss 0.1678, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 9001, loss 0.1317, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 9501, loss 0.0747, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10001, loss 0.0561, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 10501, loss 0.0467, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 11001, loss 0.0982, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 11501, loss 0.0490, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 12001, loss 0.0453, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 12501, loss 0.1057, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 13001, loss 0.0942, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 13501, loss 0.0914, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14001, loss 0.1384, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14501, loss 0.0570, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15001, loss 0.0838, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15501, loss 0.1107, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16001, loss 0.0411, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16501, loss 0.0692, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.0508, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.0854, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.0731, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.0456, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.0398, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.0919, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20001, loss 0.0560, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20501, loss 0.0482, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21001, loss 0.0825, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21501, loss 0.0428, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 22001, loss 0.0887, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 22501, loss 0.0911, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23001, loss 0.0849, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23501, loss 0.0878, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 24001, loss 0.0303, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 24501, loss 0.0384, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 25.454998934
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.1
normal_0.5
./test_glass0/model_MLP_25000_0.1/record_1/MLP_25000_0.1_1
./test_glass0/result_MLP_25000_0.1_normal_0.5/record_1/
----------------------



the AUC is 0.5369458128078817

the Fscore is 0.22222222222222224

the precision is 0.5

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.1/record_1/MLP_25000_0.1_2
----------------------



epoch 1, loss 0.6487, train acc 68.42%, f1 0.2059, precision 0.5833, recall 0.1250, auc 0.5408
epoch 501, loss 0.3117, train acc 83.04%, f1 0.7680, precision 0.6957, recall 0.8571, auc 0.8373
epoch 1001, loss 0.3475, train acc 84.21%, f1 0.7840, precision 0.7101, recall 0.8750, auc 0.8505
epoch 1501, loss 0.2659, train acc 88.89%, f1 0.8455, precision 0.7761, recall 0.9286, auc 0.8991
epoch 2001, loss 0.2137, train acc 90.06%, f1 0.8595, precision 0.8000, recall 0.9286, auc 0.9078
epoch 2501, loss 0.2581, train acc 91.23%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9210
epoch 3001, loss 0.1596, train acc 91.23%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9210
epoch 3501, loss 0.1539, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 4001, loss 0.1468, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 4501, loss 0.2190, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 5001, loss 0.1415, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 5501, loss 0.1336, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 6001, loss 0.1510, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 6501, loss 0.1525, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 7001, loss 0.1167, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 7501, loss 0.1816, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 8001, loss 0.1381, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 8501, loss 0.1508, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 9001, loss 0.1323, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 9501, loss 0.0957, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 10001, loss 0.0827, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 10501, loss 0.1146, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 11001, loss 0.1497, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 11501, loss 0.0762, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 12001, loss 0.1282, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 12501, loss 0.1376, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 13001, loss 0.1478, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13501, loss 0.1267, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 14001, loss 0.0909, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14501, loss 0.0742, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15001, loss 0.0640, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 15501, loss 0.0706, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 16001, loss 0.0585, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 16501, loss 0.0822, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 17001, loss 0.0836, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 17501, loss 0.0720, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 18001, loss 0.1238, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18501, loss 0.1085, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 19001, loss 0.0534, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 19501, loss 0.0410, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 20001, loss 0.0705, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 20501, loss 0.0577, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 21001, loss 0.1114, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 21501, loss 0.0899, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 22001, loss 0.0521, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 22501, loss 0.1157, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 23001, loss 0.0477, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 23501, loss 0.1168, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 24001, loss 0.0717, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 24501, loss 0.0815, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
running_time is 25.474167386999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.1
normal_0.5
./test_glass0/model_MLP_25000_0.1/record_1/MLP_25000_0.1_2
./test_glass0/result_MLP_25000_0.1_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.1/record_1/MLP_25000_0.1_3
----------------------



epoch 1, loss 0.6323, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.4401, train acc 80.12%, f1 0.7258, precision 0.6618, recall 0.8036, auc 0.8018
epoch 1001, loss 0.3562, train acc 81.87%, f1 0.7480, precision 0.6866, recall 0.8214, auc 0.8194
epoch 1501, loss 0.3821, train acc 85.38%, f1 0.7967, precision 0.7313, recall 0.8750, auc 0.8592
epoch 2001, loss 0.3273, train acc 87.13%, f1 0.8167, precision 0.7656, recall 0.8750, auc 0.8723
epoch 2501, loss 0.2790, train acc 87.72%, f1 0.8293, precision 0.7612, recall 0.9107, auc 0.8858
epoch 3001, loss 0.1985, train acc 88.89%, f1 0.8480, precision 0.7681, recall 0.9464, auc 0.9036
epoch 3501, loss 0.2541, train acc 89.47%, f1 0.8548, precision 0.7794, recall 0.9464, auc 0.9080
epoch 4001, loss 0.1229, train acc 90.64%, f1 0.8667, precision 0.8125, recall 0.9286, auc 0.9121
epoch 4501, loss 0.2219, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 5001, loss 0.1686, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 5501, loss 0.1270, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 6001, loss 0.1729, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 6501, loss 0.1882, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 7001, loss 0.1280, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 7501, loss 0.1302, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 8001, loss 0.1096, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 8501, loss 0.1751, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 9001, loss 0.0851, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 9501, loss 0.1223, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 10001, loss 0.0720, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 10501, loss 0.1357, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 11001, loss 0.1097, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 11501, loss 0.1471, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 12001, loss 0.0895, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12501, loss 0.0951, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13001, loss 0.1378, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 13501, loss 0.0777, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 14001, loss 0.1351, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 14501, loss 0.0764, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15001, loss 0.1394, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15501, loss 0.1341, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 16001, loss 0.0950, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 16501, loss 0.0754, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 17001, loss 0.1022, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 17501, loss 0.1020, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18001, loss 0.0554, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18501, loss 0.0826, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 19001, loss 0.1055, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 19501, loss 0.1168, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 20001, loss 0.0739, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 20501, loss 0.1374, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 21001, loss 0.0876, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 21501, loss 0.0589, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 22001, loss 0.0454, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 22501, loss 0.0888, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 23001, loss 0.0967, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 23501, loss 0.0769, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 24001, loss 0.0594, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 24501, loss 0.1103, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
running_time is 25.420059958
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.1
normal_0.5
./test_glass0/model_MLP_25000_0.1/record_1/MLP_25000_0.1_3
./test_glass0/result_MLP_25000_0.1_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.1/record_1/MLP_25000_0.1_4
----------------------



epoch 1, loss 0.6650, train acc 60.82%, f1 0.0290, precision 0.0769, recall 0.0179, auc 0.4568
epoch 501, loss 0.3871, train acc 78.36%, f1 0.7218, precision 0.6234, recall 0.8571, auc 0.8025
epoch 1001, loss 0.4402, train acc 81.87%, f1 0.7559, precision 0.6761, recall 0.8571, auc 0.8286
epoch 1501, loss 0.3153, train acc 83.63%, f1 0.7778, precision 0.7000, recall 0.8750, auc 0.8462
epoch 2001, loss 0.3222, train acc 85.38%, f1 0.7967, precision 0.7313, recall 0.8750, auc 0.8592
epoch 2501, loss 0.2932, train acc 89.47%, f1 0.8475, precision 0.8065, recall 0.8929, auc 0.8943
epoch 3001, loss 0.2968, train acc 89.47%, f1 0.8475, precision 0.8065, recall 0.8929, auc 0.8943
epoch 3501, loss 0.2760, train acc 89.47%, f1 0.8448, precision 0.8167, recall 0.8750, auc 0.8897
epoch 4001, loss 0.2914, train acc 90.06%, f1 0.8571, precision 0.8095, recall 0.9107, auc 0.9032
epoch 4501, loss 0.2326, train acc 90.64%, f1 0.8667, precision 0.8125, recall 0.9286, auc 0.9121
epoch 5001, loss 0.1653, train acc 90.64%, f1 0.8644, precision 0.8226, recall 0.9107, auc 0.9075
epoch 5501, loss 0.2414, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 6001, loss 0.2437, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 6501, loss 0.1835, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 7001, loss 0.1640, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 7501, loss 0.1549, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 8001, loss 0.1586, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 8501, loss 0.1556, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 9001, loss 0.1595, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 9501, loss 0.2108, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 10001, loss 0.2083, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 10501, loss 0.1607, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 11001, loss 0.1527, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 11501, loss 0.1993, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 12001, loss 0.1143, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 12501, loss 0.1636, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 13001, loss 0.1707, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 13501, loss 0.1257, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 14001, loss 0.1096, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 14501, loss 0.2204, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 15001, loss 0.0841, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 15501, loss 0.1261, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 16001, loss 0.2233, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 16501, loss 0.0660, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 17001, loss 0.1003, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 17501, loss 0.1067, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 18001, loss 0.1264, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 18501, loss 0.1022, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 19001, loss 0.0591, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 19501, loss 0.1089, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 20001, loss 0.0531, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 20501, loss 0.0704, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 21001, loss 0.0452, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 21501, loss 0.1028, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 22001, loss 0.1104, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 22501, loss 0.0604, train acc 97.66%, f1 0.9643, precision 0.9643, recall 0.9643, auc 0.9734
epoch 23001, loss 0.0514, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 23501, loss 0.0875, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 24001, loss 0.1094, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 24501, loss 0.0773, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
running_time is 25.371043604
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.1
normal_0.5
./test_glass0/model_MLP_25000_0.1/record_1/MLP_25000_0.1_4
./test_glass0/result_MLP_25000_0.1_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.1/record_1/MLP_25000_0.1_5
----------------------



epoch 1, loss 0.6485, train acc 67.44%, f1 0.0968, precision 0.5000, recall 0.0536, auc 0.5139
epoch 501, loss 0.4684, train acc 81.40%, f1 0.7500, precision 0.6667, recall 0.8571, auc 0.8251
epoch 1001, loss 0.3118, train acc 83.14%, f1 0.7786, precision 0.6800, recall 0.9107, auc 0.8519
epoch 1501, loss 0.3466, train acc 86.05%, f1 0.8125, precision 0.7222, recall 0.9286, auc 0.8781
epoch 2001, loss 0.3154, train acc 86.05%, f1 0.8154, precision 0.7162, recall 0.9464, auc 0.8827
epoch 2501, loss 0.3316, train acc 87.79%, f1 0.8346, precision 0.7465, recall 0.9464, auc 0.8956
epoch 3001, loss 0.2146, train acc 87.79%, f1 0.8346, precision 0.7465, recall 0.9464, auc 0.8956
epoch 3501, loss 0.2364, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 4001, loss 0.2509, train acc 91.28%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9261
epoch 4501, loss 0.2551, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 5001, loss 0.1948, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 5501, loss 0.1804, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 6001, loss 0.1917, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 6501, loss 0.1880, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 7001, loss 0.1245, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 7501, loss 0.1496, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 8001, loss 0.1665, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 8501, loss 0.1043, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 9001, loss 0.1246, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 9501, loss 0.1232, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 10001, loss 0.1143, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 10501, loss 0.2034, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 11001, loss 0.1788, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 11501, loss 0.1194, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 12001, loss 0.0901, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 12501, loss 0.1446, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 13001, loss 0.0832, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 13501, loss 0.1067, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 14001, loss 0.1003, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 14501, loss 0.0603, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 15001, loss 0.0851, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 15501, loss 0.0858, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 16001, loss 0.1236, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 16501, loss 0.0766, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 17001, loss 0.1317, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 17501, loss 0.0819, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 18001, loss 0.1274, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 18501, loss 0.1187, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 19001, loss 0.0989, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 19501, loss 0.0480, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 20001, loss 0.0698, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 20501, loss 0.1115, train acc 97.09%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9692
epoch 21001, loss 0.1017, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 21501, loss 0.1028, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 22001, loss 0.0642, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 22501, loss 0.0353, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 23001, loss 0.0602, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 23501, loss 0.0966, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 24001, loss 0.0693, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 24501, loss 0.0890, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
running_time is 25.423759287
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.1
normal_0.5
./test_glass0/model_MLP_25000_0.1/record_1/MLP_25000_0.1_5
./test_glass0/result_MLP_25000_0.1_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.11/record_1/MLP_25000_0.11_1
----------------------



epoch 1, loss 0.5927, train acc 66.08%, f1 0.1471, precision 0.4167, recall 0.0893, auc 0.5142
epoch 501, loss 0.3976, train acc 83.04%, f1 0.7717, precision 0.6901, recall 0.8750, auc 0.8418
epoch 1001, loss 0.3996, train acc 84.80%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8686
epoch 1501, loss 0.2468, train acc 85.38%, f1 0.8062, precision 0.7123, recall 0.9286, auc 0.8730
epoch 2001, loss 0.3509, train acc 87.13%, f1 0.8254, precision 0.7429, recall 0.9286, auc 0.8860
epoch 2501, loss 0.2830, train acc 88.30%, f1 0.8387, precision 0.7647, recall 0.9286, auc 0.8947
epoch 3001, loss 0.2364, train acc 88.89%, f1 0.8480, precision 0.7681, recall 0.9464, auc 0.9036
epoch 3501, loss 0.1261, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 4001, loss 0.2649, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 4501, loss 0.1717, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 5001, loss 0.2516, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 5501, loss 0.2124, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 6001, loss 0.1825, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 6501, loss 0.1466, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 7001, loss 0.1752, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 7501, loss 0.1442, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8001, loss 0.1271, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8501, loss 0.1142, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 9001, loss 0.1332, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9501, loss 0.1008, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10001, loss 0.1237, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10501, loss 0.1191, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11001, loss 0.0741, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11501, loss 0.1334, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12001, loss 0.1239, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12501, loss 0.1061, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13001, loss 0.0984, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13501, loss 0.0847, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14001, loss 0.1043, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.1106, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.1700, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.0584, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16001, loss 0.0613, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16501, loss 0.0523, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.1368, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.1020, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.0511, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.0682, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.1457, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.0602, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20001, loss 0.1084, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 20501, loss 0.0517, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21001, loss 0.0405, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 21501, loss 0.1085, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 22001, loss 0.1056, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 22501, loss 0.0485, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23001, loss 0.0857, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23501, loss 0.0448, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 24001, loss 0.0983, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 24501, loss 0.0563, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 25.768139788000003
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.11
normal_0.5
./test_glass0/model_MLP_25000_0.11/record_1/MLP_25000_0.11_1
./test_glass0/result_MLP_25000_0.11_normal_0.5/record_1/
----------------------



the AUC is 0.46551724137931033

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.11/record_1/MLP_25000_0.11_2
----------------------



epoch 1, loss 0.6553, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.4190, train acc 82.46%, f1 0.7581, precision 0.6912, recall 0.8393, auc 0.8283
epoch 1001, loss 0.3505, train acc 85.38%, f1 0.7863, precision 0.7541, recall 0.8214, auc 0.8455
epoch 1501, loss 0.2722, train acc 87.72%, f1 0.8264, precision 0.7692, recall 0.8929, auc 0.8812
epoch 2001, loss 0.2483, train acc 88.30%, f1 0.8361, precision 0.7727, recall 0.9107, auc 0.8901
epoch 2501, loss 0.2163, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 3001, loss 0.1438, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 3501, loss 0.1383, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 4001, loss 0.2096, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 4501, loss 0.1012, train acc 93.57%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9384
epoch 5001, loss 0.2498, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 5501, loss 0.1656, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 6001, loss 0.1094, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 6501, loss 0.1109, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 7001, loss 0.1437, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 7501, loss 0.0282, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 8001, loss 0.0962, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 8501, loss 0.1005, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 9001, loss 0.0833, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 9501, loss 0.1012, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 10001, loss 0.0985, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 10501, loss 0.0784, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11001, loss 0.1383, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11501, loss 0.1630, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 12001, loss 0.0685, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 12501, loss 0.0811, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 13001, loss 0.0794, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 13501, loss 0.0840, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14001, loss 0.1213, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14501, loss 0.1225, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15001, loss 0.0757, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15501, loss 0.1107, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 16001, loss 0.0679, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16501, loss 0.0610, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.0910, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 17501, loss 0.0698, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18001, loss 0.1237, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 18501, loss 0.0810, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19001, loss 0.0660, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19501, loss 0.0428, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 20001, loss 0.0630, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 20501, loss 0.0516, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 21001, loss 0.0483, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 21501, loss 0.0556, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 22001, loss 0.0441, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 22501, loss 0.0396, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 23001, loss 0.0377, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 23501, loss 0.0484, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24001, loss 0.0610, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24501, loss 0.0843, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 25.610305559
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.11
normal_0.5
./test_glass0/model_MLP_25000_0.11/record_1/MLP_25000_0.11_2
./test_glass0/result_MLP_25000_0.11_normal_0.5/record_1/
----------------------



the AUC is 0.6982758620689655

the Fscore is 0.5833333333333334

the precision is 0.7

the recall is 0.5

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.11/record_1/MLP_25000_0.11_3
----------------------



epoch 1, loss 0.6193, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.4472, train acc 80.12%, f1 0.7302, precision 0.6571, recall 0.8214, auc 0.8064
epoch 1001, loss 0.3057, train acc 83.04%, f1 0.7717, precision 0.6901, recall 0.8750, auc 0.8418
epoch 1501, loss 0.3316, train acc 85.38%, f1 0.7967, precision 0.7313, recall 0.8750, auc 0.8592
epoch 2001, loss 0.2903, train acc 84.21%, f1 0.7840, precision 0.7101, recall 0.8750, auc 0.8505
epoch 2501, loss 0.1929, train acc 87.13%, f1 0.8254, precision 0.7429, recall 0.9286, auc 0.8860
epoch 3001, loss 0.1694, train acc 87.13%, f1 0.8226, precision 0.7500, recall 0.9107, auc 0.8814
epoch 3501, loss 0.1500, train acc 90.06%, f1 0.8618, precision 0.7910, recall 0.9464, auc 0.9123
epoch 4001, loss 0.2047, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 4501, loss 0.2238, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 5001, loss 0.1976, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 5501, loss 0.2282, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 6001, loss 0.1466, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 6501, loss 0.1101, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 7001, loss 0.1032, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 7501, loss 0.1351, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 8001, loss 0.1579, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 8501, loss 0.1365, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 9001, loss 0.1334, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 9501, loss 0.1388, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10001, loss 0.0961, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 10501, loss 0.1042, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 11001, loss 0.0861, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 11501, loss 0.1071, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12001, loss 0.1046, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12501, loss 0.1029, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13001, loss 0.0855, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13501, loss 0.0903, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14001, loss 0.0558, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14501, loss 0.0914, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15001, loss 0.0987, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 15501, loss 0.1099, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 16001, loss 0.0821, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 16501, loss 0.0877, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 17001, loss 0.0950, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17501, loss 0.1136, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18001, loss 0.0663, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.0845, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 19001, loss 0.0567, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 19501, loss 0.0365, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 20001, loss 0.0588, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 20501, loss 0.0771, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 21001, loss 0.1108, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 21501, loss 0.1170, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 22001, loss 0.0420, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 22501, loss 0.0603, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 23001, loss 0.0649, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 23501, loss 0.0555, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24001, loss 0.0560, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24501, loss 0.0394, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 25.313059908
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.11
normal_0.5
./test_glass0/model_MLP_25000_0.11/record_1/MLP_25000_0.11_3
./test_glass0/result_MLP_25000_0.11_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.11/record_1/MLP_25000_0.11_4
----------------------



epoch 1, loss 0.6016, train acc 66.08%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4913
epoch 501, loss 0.4460, train acc 79.53%, f1 0.7328, precision 0.6400, recall 0.8571, auc 0.8112
epoch 1001, loss 0.2898, train acc 81.87%, f1 0.7597, precision 0.6712, recall 0.8750, auc 0.8332
epoch 1501, loss 0.3365, train acc 84.80%, f1 0.7903, precision 0.7206, recall 0.8750, auc 0.8549
epoch 2001, loss 0.3385, train acc 85.96%, f1 0.8065, precision 0.7353, recall 0.8929, auc 0.8682
epoch 2501, loss 0.2727, train acc 88.30%, f1 0.8387, precision 0.7647, recall 0.9286, auc 0.8947
epoch 3001, loss 0.2862, train acc 89.47%, f1 0.8500, precision 0.7969, recall 0.9107, auc 0.8988
epoch 3501, loss 0.1939, train acc 90.06%, f1 0.8571, precision 0.8095, recall 0.9107, auc 0.9032
epoch 4001, loss 0.2419, train acc 90.06%, f1 0.8595, precision 0.8000, recall 0.9286, auc 0.9078
epoch 4501, loss 0.1230, train acc 90.64%, f1 0.8689, precision 0.8030, recall 0.9464, auc 0.9167
epoch 5001, loss 0.2824, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 5501, loss 0.2407, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 6001, loss 0.1780, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 6501, loss 0.1645, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 7001, loss 0.2359, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 7501, loss 0.1803, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 8001, loss 0.0791, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 8501, loss 0.1641, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 9001, loss 0.1909, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 9501, loss 0.1710, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 10001, loss 0.1342, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 10501, loss 0.1237, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 11001, loss 0.1287, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 11501, loss 0.0912, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 12001, loss 0.1414, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 12501, loss 0.1044, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 13001, loss 0.0646, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 13501, loss 0.1186, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 14001, loss 0.1624, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 14501, loss 0.1257, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 15001, loss 0.0906, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 15501, loss 0.0699, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 16001, loss 0.0564, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 16501, loss 0.0885, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 17001, loss 0.1026, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 17501, loss 0.0617, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 18001, loss 0.0882, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18501, loss 0.1138, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19001, loss 0.1609, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19501, loss 0.0638, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20001, loss 0.1084, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20501, loss 0.0568, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 21001, loss 0.1398, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21501, loss 0.0677, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22001, loss 0.0632, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22501, loss 0.1431, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23001, loss 0.1008, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23501, loss 0.0555, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24001, loss 0.0704, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24501, loss 0.0410, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 25.386943662
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.11
normal_0.5
./test_glass0/model_MLP_25000_0.11/record_1/MLP_25000_0.11_4
./test_glass0/result_MLP_25000_0.11_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.11/record_1/MLP_25000_0.11_5
----------------------



epoch 1, loss 0.6822, train acc 44.19%, f1 0.5385, precision 0.3684, recall 1.0000, auc 0.5862
epoch 501, loss 0.4330, train acc 79.65%, f1 0.7328, precision 0.6400, recall 0.8571, auc 0.8122
epoch 1001, loss 0.3954, train acc 81.40%, f1 0.7538, precision 0.6622, recall 0.8750, auc 0.8297
epoch 1501, loss 0.3353, train acc 84.88%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8695
epoch 2001, loss 0.4246, train acc 84.30%, f1 0.7939, precision 0.6933, recall 0.9286, auc 0.8651
epoch 2501, loss 0.3753, train acc 86.63%, f1 0.8189, precision 0.7324, recall 0.9286, auc 0.8824
epoch 3001, loss 0.3012, train acc 87.79%, f1 0.8346, precision 0.7465, recall 0.9464, auc 0.8956
epoch 3501, loss 0.2392, train acc 88.95%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9089
epoch 4001, loss 0.1668, train acc 87.79%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.9002
epoch 4501, loss 0.2347, train acc 88.37%, f1 0.8437, precision 0.7500, recall 0.9643, auc 0.9046
epoch 5001, loss 0.1694, train acc 90.70%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9264
epoch 5501, loss 0.2702, train acc 91.28%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9261
epoch 6001, loss 0.2111, train acc 91.28%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9215
epoch 6501, loss 0.1620, train acc 91.28%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9261
epoch 7001, loss 0.1557, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 7501, loss 0.1795, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 8001, loss 0.2276, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 8501, loss 0.2047, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 9001, loss 0.1898, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 9501, loss 0.2345, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 10001, loss 0.1458, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 10501, loss 0.1534, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 11001, loss 0.1199, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 11501, loss 0.0932, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 12001, loss 0.1853, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 12501, loss 0.1548, train acc 93.60%, f1 0.9060, precision 0.8689, recall 0.9464, auc 0.9387
epoch 13001, loss 0.1088, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 13501, loss 0.0984, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 14001, loss 0.1193, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 14501, loss 0.2077, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 15001, loss 0.1353, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 15501, loss 0.1335, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 16001, loss 0.1836, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 16501, loss 0.0962, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 17001, loss 0.1945, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 17501, loss 0.1821, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 18001, loss 0.1264, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 18501, loss 0.1055, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 19001, loss 0.0576, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 19501, loss 0.0915, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 20001, loss 0.1118, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 20501, loss 0.1262, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 21001, loss 0.0750, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 21501, loss 0.1000, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 22001, loss 0.1109, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 22501, loss 0.1636, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 23001, loss 0.1304, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 23501, loss 0.0691, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 24001, loss 0.1340, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 24501, loss 0.1408, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
running_time is 25.372431764
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.11
normal_0.5
./test_glass0/model_MLP_25000_0.11/record_1/MLP_25000_0.11_5
./test_glass0/result_MLP_25000_0.11_normal_0.5/record_1/
----------------------



the AUC is 0.5357142857142857

the Fscore is 0.22222222222222224

the precision is 0.5

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.12/record_1/MLP_25000_0.12_1
----------------------



epoch 1, loss 0.6541, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.2983, train acc 83.63%, f1 0.7879, precision 0.6842, recall 0.9286, auc 0.8599
epoch 1001, loss 0.3513, train acc 83.04%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8556
epoch 1501, loss 0.2955, train acc 84.21%, f1 0.7939, precision 0.6933, recall 0.9286, auc 0.8643
epoch 2001, loss 0.3443, train acc 84.80%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8686
epoch 2501, loss 0.3257, train acc 85.38%, f1 0.8062, precision 0.7123, recall 0.9286, auc 0.8730
epoch 3001, loss 0.2762, train acc 85.38%, f1 0.8062, precision 0.7123, recall 0.9286, auc 0.8730
epoch 3501, loss 0.3041, train acc 87.13%, f1 0.8254, precision 0.7429, recall 0.9286, auc 0.8860
epoch 4001, loss 0.1709, train acc 88.30%, f1 0.8413, precision 0.7571, recall 0.9464, auc 0.8993
epoch 4501, loss 0.2015, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 5001, loss 0.1810, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 5501, loss 0.2017, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 6001, loss 0.1567, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 6501, loss 0.1190, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 7001, loss 0.1984, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 7501, loss 0.1128, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 8001, loss 0.1394, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 8501, loss 0.1130, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 9001, loss 0.1313, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 9501, loss 0.1160, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10001, loss 0.0969, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10501, loss 0.0815, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 11001, loss 0.0909, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 11501, loss 0.0674, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 12001, loss 0.0724, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 12501, loss 0.0825, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13001, loss 0.0769, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 13501, loss 0.0422, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14001, loss 0.0645, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0556, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0539, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0479, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0326, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16501, loss 0.0486, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0479, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0589, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0523, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0492, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0423, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0383, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 20001, loss 0.0477, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 20501, loss 0.0426, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 21001, loss 0.0443, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 21501, loss 0.0530, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 22001, loss 0.0366, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 22501, loss 0.0475, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 23001, loss 0.0468, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 23501, loss 0.0442, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 24001, loss 0.0387, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 24501, loss 0.0344, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
running_time is 25.479246859
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.12
normal_0.5
./test_glass0/model_MLP_25000_0.12/record_1/MLP_25000_0.12_1
./test_glass0/result_MLP_25000_0.12_normal_0.5/record_1/
----------------------



the AUC is 0.518472906403941

the Fscore is 0.125

the precision is 0.5

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.12/record_1/MLP_25000_0.12_2
----------------------



epoch 1, loss 0.6151, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.4173, train acc 80.70%, f1 0.7481, precision 0.6533, recall 0.8750, auc 0.8245
epoch 1001, loss 0.2659, train acc 85.96%, f1 0.8154, precision 0.7162, recall 0.9464, auc 0.8819
epoch 1501, loss 0.2532, train acc 87.72%, f1 0.8320, precision 0.7536, recall 0.9286, auc 0.8904
epoch 2001, loss 0.1860, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 2501, loss 0.2432, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 3001, loss 0.2760, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 3501, loss 0.1720, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 4001, loss 0.2316, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 4501, loss 0.1468, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 5001, loss 0.1434, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 5501, loss 0.1284, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 6001, loss 0.1451, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 6501, loss 0.0962, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 7001, loss 0.0865, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 7501, loss 0.1142, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 8001, loss 0.1124, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 8501, loss 0.0820, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 9001, loss 0.1435, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 9501, loss 0.1063, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 10001, loss 0.0944, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 10501, loss 0.0731, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 11001, loss 0.1530, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 11501, loss 0.0578, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12001, loss 0.0872, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 12501, loss 0.0537, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 13001, loss 0.1256, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 13501, loss 0.0459, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 14001, loss 0.0447, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 14501, loss 0.1061, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 15001, loss 0.0682, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 15501, loss 0.0548, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 16001, loss 0.0984, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 16501, loss 0.0911, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 17001, loss 0.0553, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 17501, loss 0.0506, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18001, loss 0.0449, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18501, loss 0.0487, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19001, loss 0.0563, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19501, loss 0.0418, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 20001, loss 0.0425, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 20501, loss 0.0490, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 21001, loss 0.0885, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 21501, loss 0.0570, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 22001, loss 0.0340, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 22501, loss 0.0468, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 23001, loss 0.0912, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 23501, loss 0.0597, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24001, loss 0.0605, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24501, loss 0.0530, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 25.460076338
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.12
normal_0.5
./test_glass0/model_MLP_25000_0.12/record_1/MLP_25000_0.12_2
./test_glass0/result_MLP_25000_0.12_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.12/record_1/MLP_25000_0.12_3
----------------------



epoch 1, loss 0.5958, train acc 63.74%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4739
epoch 501, loss 0.3553, train acc 80.70%, f1 0.7442, precision 0.6575, recall 0.8571, auc 0.8199
epoch 1001, loss 0.3537, train acc 83.04%, f1 0.7717, precision 0.6901, recall 0.8750, auc 0.8418
epoch 1501, loss 0.3658, train acc 84.80%, f1 0.7903, precision 0.7206, recall 0.8750, auc 0.8549
epoch 2001, loss 0.3194, train acc 85.38%, f1 0.7967, precision 0.7313, recall 0.8750, auc 0.8592
epoch 2501, loss 0.3052, train acc 87.13%, f1 0.8226, precision 0.7500, recall 0.9107, auc 0.8814
epoch 3001, loss 0.2363, train acc 87.13%, f1 0.8254, precision 0.7429, recall 0.9286, auc 0.8860
epoch 3501, loss 0.2456, train acc 88.30%, f1 0.8361, precision 0.7727, recall 0.9107, auc 0.8901
epoch 4001, loss 0.1585, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 4501, loss 0.1445, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 5001, loss 0.1360, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 5501, loss 0.1691, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 6001, loss 0.1250, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 6501, loss 0.1409, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 7001, loss 0.1704, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 7501, loss 0.1168, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 8001, loss 0.1486, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 8501, loss 0.1181, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 9001, loss 0.1118, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 9501, loss 0.1095, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 10001, loss 0.1210, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 10501, loss 0.0864, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 11001, loss 0.1402, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 11501, loss 0.1026, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12001, loss 0.1144, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12501, loss 0.1176, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 13001, loss 0.1641, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 13501, loss 0.1028, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 14001, loss 0.1387, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 14501, loss 0.1121, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15001, loss 0.0850, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 15501, loss 0.0899, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16001, loss 0.1057, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16501, loss 0.1121, train acc 97.08%, f1 0.9558, precision 0.9474, recall 0.9643, auc 0.9691
epoch 17001, loss 0.0879, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.0784, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.1094, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18501, loss 0.0522, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19001, loss 0.1319, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.1167, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20001, loss 0.1133, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20501, loss 0.1028, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21001, loss 0.0993, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 21501, loss 0.0467, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22001, loss 0.0924, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 22501, loss 0.0817, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23001, loss 0.0618, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23501, loss 0.0839, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24001, loss 0.0759, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24501, loss 0.0620, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 25.560675121000003
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.12
normal_0.5
./test_glass0/model_MLP_25000_0.12/record_1/MLP_25000_0.12_3
./test_glass0/result_MLP_25000_0.12_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.12/record_1/MLP_25000_0.12_4
----------------------



epoch 1, loss 0.6542, train acc 67.25%, f1 0.0968, precision 0.5000, recall 0.0536, auc 0.5137
epoch 501, loss 0.3999, train acc 78.95%, f1 0.7273, precision 0.6316, recall 0.8571, auc 0.8068
epoch 1001, loss 0.3280, train acc 81.87%, f1 0.7597, precision 0.6712, recall 0.8750, auc 0.8332
epoch 1501, loss 0.3603, train acc 83.63%, f1 0.7778, precision 0.7000, recall 0.8750, auc 0.8462
epoch 2001, loss 0.2291, train acc 86.55%, f1 0.8130, precision 0.7463, recall 0.8929, auc 0.8725
epoch 2501, loss 0.3116, train acc 90.06%, f1 0.8547, precision 0.8197, recall 0.8929, auc 0.8986
epoch 3001, loss 0.2907, train acc 89.47%, f1 0.8500, precision 0.7969, recall 0.9107, auc 0.8988
epoch 3501, loss 0.2545, train acc 91.23%, f1 0.8718, precision 0.8361, recall 0.9107, auc 0.9119
epoch 4001, loss 0.1416, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 4501, loss 0.2708, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 5001, loss 0.2024, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 5501, loss 0.1166, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 6001, loss 0.1647, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 6501, loss 0.1346, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 7001, loss 0.1540, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 7501, loss 0.1086, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 8001, loss 0.1447, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 8501, loss 0.1365, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 9001, loss 0.1570, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 9501, loss 0.1886, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 10001, loss 0.1250, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 10501, loss 0.1670, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 11001, loss 0.1278, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 11501, loss 0.1589, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 12001, loss 0.1272, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 12501, loss 0.2377, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 13001, loss 0.1344, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 13501, loss 0.0849, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 14001, loss 0.2550, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 14501, loss 0.1539, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 15001, loss 0.0832, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 15501, loss 0.1517, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 16001, loss 0.1646, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 16501, loss 0.2224, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 17001, loss 0.0858, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 17501, loss 0.1095, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 18001, loss 0.0471, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 18501, loss 0.1537, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 19001, loss 0.0680, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 19501, loss 0.1728, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 20001, loss 0.1806, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 20501, loss 0.1201, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 21001, loss 0.1646, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 21501, loss 0.1536, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 22001, loss 0.1076, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 22501, loss 0.1135, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 23001, loss 0.2311, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 23501, loss 0.1286, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 24001, loss 0.0895, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 24501, loss 0.1278, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
running_time is 25.161255045
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.12
normal_0.5
./test_glass0/model_MLP_25000_0.12/record_1/MLP_25000_0.12_4
./test_glass0/result_MLP_25000_0.12_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.12/record_1/MLP_25000_0.12_5
----------------------



epoch 1, loss 0.5956, train acc 59.88%, f1 0.0282, precision 0.0667, recall 0.0179, auc 0.4486
epoch 501, loss 0.3931, train acc 79.65%, f1 0.7407, precision 0.6329, recall 0.8929, auc 0.8214
epoch 1001, loss 0.4414, train acc 80.81%, f1 0.7591, precision 0.6420, recall 0.9286, auc 0.8393
epoch 1501, loss 0.3485, train acc 83.14%, f1 0.7786, precision 0.6800, recall 0.9107, auc 0.8519
epoch 2001, loss 0.3499, train acc 84.30%, f1 0.7874, precision 0.7042, recall 0.8929, auc 0.8559
epoch 2501, loss 0.3143, train acc 85.47%, f1 0.8092, precision 0.7067, recall 0.9464, auc 0.8784
epoch 3001, loss 0.2395, train acc 86.05%, f1 0.8065, precision 0.7353, recall 0.8929, auc 0.8688
epoch 3501, loss 0.2623, train acc 86.05%, f1 0.8095, precision 0.7286, recall 0.9107, auc 0.8735
epoch 4001, loss 0.2915, train acc 88.95%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9089
epoch 4501, loss 0.1917, train acc 90.12%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9221
epoch 5001, loss 0.2204, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 5501, loss 0.1652, train acc 90.70%, f1 0.8689, precision 0.8030, recall 0.9464, auc 0.9172
epoch 6001, loss 0.1657, train acc 91.28%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9261
epoch 6501, loss 0.1281, train acc 91.86%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9304
epoch 7001, loss 0.1519, train acc 91.28%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9215
epoch 7501, loss 0.1600, train acc 91.28%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9215
epoch 8001, loss 0.1209, train acc 92.44%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9301
epoch 8501, loss 0.1765, train acc 91.86%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9304
epoch 9001, loss 0.1406, train acc 93.02%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9344
epoch 9501, loss 0.1720, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 10001, loss 0.1403, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 10501, loss 0.1184, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 11001, loss 0.2087, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 11501, loss 0.1111, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 12001, loss 0.1145, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 12501, loss 0.1818, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 13001, loss 0.1730, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 13501, loss 0.1407, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 14001, loss 0.1392, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 14501, loss 0.0988, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 15001, loss 0.1758, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 15501, loss 0.1953, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 16001, loss 0.2067, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 16501, loss 0.1155, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 17001, loss 0.1270, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 17501, loss 0.0939, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 18001, loss 0.0922, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 18501, loss 0.1468, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 19001, loss 0.1475, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 19501, loss 0.1722, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 20001, loss 0.1948, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 20501, loss 0.1519, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 21001, loss 0.1000, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 21501, loss 0.0820, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 22001, loss 0.1136, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 22501, loss 0.1329, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 23001, loss 0.1117, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 23501, loss 0.1036, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 24001, loss 0.0994, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 24501, loss 0.0856, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
running_time is 25.564571688999997
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.12
normal_0.5
./test_glass0/model_MLP_25000_0.12/record_1/MLP_25000_0.12_5
./test_glass0/result_MLP_25000_0.12_normal_0.5/record_1/
----------------------



the AUC is 0.625

the Fscore is 0.4799999999999999

the precision is 0.5454545454545454

the recall is 0.42857142857142855

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.13/record_1/MLP_25000_0.13_1
----------------------



epoch 1, loss 0.6004, train acc 65.50%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4870
epoch 501, loss 0.3212, train acc 82.46%, f1 0.7794, precision 0.6625, recall 0.9464, auc 0.8558
epoch 1001, loss 0.2997, train acc 83.04%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8556
epoch 1501, loss 0.3259, train acc 83.04%, f1 0.7786, precision 0.6800, recall 0.9107, auc 0.8510
epoch 2001, loss 0.3783, train acc 87.13%, f1 0.8281, precision 0.7361, recall 0.9464, auc 0.8906
epoch 2501, loss 0.2567, train acc 87.13%, f1 0.8281, precision 0.7361, recall 0.9464, auc 0.8906
epoch 3001, loss 0.2421, train acc 88.30%, f1 0.8437, precision 0.7500, recall 0.9643, auc 0.9039
epoch 3501, loss 0.2232, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 4001, loss 0.2391, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 4501, loss 0.2319, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 5001, loss 0.1617, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 5501, loss 0.1906, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 6001, loss 0.1984, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 6501, loss 0.1719, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 7001, loss 0.1222, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 7501, loss 0.0913, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 8001, loss 0.0894, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 8501, loss 0.0992, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 9001, loss 0.1374, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 9501, loss 0.0864, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10001, loss 0.0986, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10501, loss 0.1419, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 11001, loss 0.0972, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 11501, loss 0.1282, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 12001, loss 0.0789, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 12501, loss 0.0371, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 13001, loss 0.0515, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 13501, loss 0.0641, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 14001, loss 0.0592, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 14501, loss 0.0911, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 15001, loss 0.0913, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 15501, loss 0.0475, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 16001, loss 0.0393, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 16501, loss 0.0469, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 17001, loss 0.0957, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 17501, loss 0.0937, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18001, loss 0.0554, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18501, loss 0.0450, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19001, loss 0.0854, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19501, loss 0.0653, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 20001, loss 0.0595, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 20501, loss 0.0460, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 21001, loss 0.0574, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 21501, loss 0.0486, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 22001, loss 0.0517, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 22501, loss 0.0424, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 23001, loss 0.0455, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 23501, loss 0.0485, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24001, loss 0.0602, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24501, loss 0.0368, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 25.268575492
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.13
normal_0.5
./test_glass0/model_MLP_25000_0.13/record_1/MLP_25000_0.13_1
./test_glass0/result_MLP_25000_0.13_normal_0.5/record_1/
----------------------



the AUC is 0.5923645320197044

the Fscore is 0.41666666666666663

the precision is 0.5

the recall is 0.35714285714285715

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.13/record_1/MLP_25000_0.13_2
----------------------



epoch 1, loss 0.6430, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3504, train acc 81.29%, f1 0.7538, precision 0.6622, recall 0.8750, auc 0.8288
epoch 1001, loss 0.2610, train acc 87.72%, f1 0.8346, precision 0.7465, recall 0.9464, auc 0.8950
epoch 1501, loss 0.2841, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 2001, loss 0.1688, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 2501, loss 0.1748, train acc 90.06%, f1 0.8618, precision 0.7910, recall 0.9464, auc 0.9123
epoch 3001, loss 0.2016, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 3501, loss 0.1431, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 4001, loss 0.1314, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 4501, loss 0.1260, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 5001, loss 0.2334, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 5501, loss 0.1465, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 6001, loss 0.1017, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 6501, loss 0.1619, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 7001, loss 0.2323, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 7501, loss 0.1762, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 8001, loss 0.1602, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 8501, loss 0.0701, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9001, loss 0.1006, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9501, loss 0.1139, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10001, loss 0.1115, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10501, loss 0.1143, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 11001, loss 0.1467, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11501, loss 0.0990, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 12001, loss 0.1074, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 12501, loss 0.1257, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13001, loss 0.1438, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13501, loss 0.1133, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14001, loss 0.0492, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14501, loss 0.1444, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.1097, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 15501, loss 0.0693, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 16001, loss 0.1160, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16501, loss 0.0821, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17001, loss 0.1620, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17501, loss 0.1052, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18001, loss 0.0625, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18501, loss 0.1436, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.0579, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19501, loss 0.0856, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 20001, loss 0.0476, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 20501, loss 0.0742, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 21001, loss 0.0856, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 21501, loss 0.1069, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22001, loss 0.1026, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22501, loss 0.1161, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 23001, loss 0.0988, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23501, loss 0.0745, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 24001, loss 0.1175, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24501, loss 0.0672, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
running_time is 25.461596283000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.13
normal_0.5
./test_glass0/model_MLP_25000_0.13/record_1/MLP_25000_0.13_2
./test_glass0/result_MLP_25000_0.13_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.13/record_1/MLP_25000_0.13_3
----------------------



epoch 1, loss 0.6005, train acc 64.33%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4783
epoch 501, loss 0.4774, train acc 79.53%, f1 0.7368, precision 0.6364, recall 0.8750, auc 0.8158
epoch 1001, loss 0.4023, train acc 82.46%, f1 0.7656, precision 0.6806, recall 0.8750, auc 0.8375
epoch 1501, loss 0.3121, train acc 84.80%, f1 0.7903, precision 0.7206, recall 0.8750, auc 0.8549
epoch 2001, loss 0.3367, train acc 85.96%, f1 0.8033, precision 0.7424, recall 0.8750, auc 0.8636
epoch 2501, loss 0.2229, train acc 87.13%, f1 0.8254, precision 0.7429, recall 0.9286, auc 0.8860
epoch 3001, loss 0.2803, train acc 88.89%, f1 0.8480, precision 0.7681, recall 0.9464, auc 0.9036
epoch 3501, loss 0.1832, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 4001, loss 0.1813, train acc 91.23%, f1 0.8739, precision 0.8254, recall 0.9286, auc 0.9165
epoch 4501, loss 0.1919, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 5001, loss 0.1509, train acc 92.98%, f1 0.8983, precision 0.8548, recall 0.9464, auc 0.9341
epoch 5501, loss 0.1597, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 6001, loss 0.1847, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 6501, loss 0.1267, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 7001, loss 0.1408, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 7501, loss 0.1274, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 8001, loss 0.1684, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 8501, loss 0.0994, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9001, loss 0.1016, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9501, loss 0.0893, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10001, loss 0.0950, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 10501, loss 0.0746, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11001, loss 0.0926, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11501, loss 0.1316, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12001, loss 0.0483, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12501, loss 0.0733, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13001, loss 0.1042, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13501, loss 0.1313, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14001, loss 0.0710, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 14501, loss 0.0794, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 15001, loss 0.0929, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 15501, loss 0.0796, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16001, loss 0.1086, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16501, loss 0.0797, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17001, loss 0.0685, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17501, loss 0.1025, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18001, loss 0.0603, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 18501, loss 0.0686, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.0729, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 19501, loss 0.0741, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20001, loss 0.0543, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 20501, loss 0.0551, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 21001, loss 0.0913, train acc 98.83%, f1 0.9821, precision 0.9821, recall 0.9821, auc 0.9867
epoch 21501, loss 0.0706, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 22001, loss 0.0724, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22501, loss 0.1117, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 23001, loss 0.0903, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 23501, loss 0.0798, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24001, loss 0.0573, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24501, loss 0.0966, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 25.662788121
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.13
normal_0.5
./test_glass0/model_MLP_25000_0.13/record_1/MLP_25000_0.13_3
./test_glass0/result_MLP_25000_0.13_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.13/record_1/MLP_25000_0.13_4
----------------------



epoch 1, loss 0.6537, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3943, train acc 80.12%, f1 0.7463, precision 0.6410, recall 0.8929, auc 0.8247
epoch 1001, loss 0.3512, train acc 83.04%, f1 0.7786, precision 0.6800, recall 0.9107, auc 0.8510
epoch 1501, loss 0.3276, train acc 84.80%, f1 0.7969, precision 0.7083, recall 0.9107, auc 0.8641
epoch 2001, loss 0.3498, train acc 85.96%, f1 0.8125, precision 0.7222, recall 0.9286, auc 0.8773
epoch 2501, loss 0.3007, train acc 85.38%, f1 0.8000, precision 0.7246, recall 0.8929, auc 0.8638
epoch 3001, loss 0.3079, train acc 87.13%, f1 0.8254, precision 0.7429, recall 0.9286, auc 0.8860
epoch 3501, loss 0.1910, train acc 88.89%, f1 0.8455, precision 0.7761, recall 0.9286, auc 0.8991
epoch 4001, loss 0.2356, train acc 90.06%, f1 0.8595, precision 0.8000, recall 0.9286, auc 0.9078
epoch 4501, loss 0.1972, train acc 90.64%, f1 0.8689, precision 0.8030, recall 0.9464, auc 0.9167
epoch 5001, loss 0.2680, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 5501, loss 0.1559, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 6001, loss 0.2240, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 6501, loss 0.2243, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 7001, loss 0.1359, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 7501, loss 0.1135, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 8001, loss 0.1921, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 8501, loss 0.2022, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 9001, loss 0.1176, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 9501, loss 0.1445, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 10001, loss 0.1727, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 10501, loss 0.2220, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 11001, loss 0.1449, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 11501, loss 0.0981, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 12001, loss 0.1750, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 12501, loss 0.0670, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 13001, loss 0.1141, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 13501, loss 0.1653, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 14001, loss 0.1039, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 14501, loss 0.1704, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 15001, loss 0.1977, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 15501, loss 0.1794, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 16001, loss 0.2044, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 16501, loss 0.1935, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 17001, loss 0.1738, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 17501, loss 0.1766, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 18001, loss 0.1217, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 18501, loss 0.1139, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 19001, loss 0.1234, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19501, loss 0.2052, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 20001, loss 0.1612, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 20501, loss 0.1031, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 21001, loss 0.1264, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 21501, loss 0.0723, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 22001, loss 0.1682, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 22501, loss 0.0695, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 23001, loss 0.0986, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 23501, loss 0.1871, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 24001, loss 0.1498, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 24501, loss 0.0576, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
running_time is 25.412225215
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.13
normal_0.5
./test_glass0/model_MLP_25000_0.13/record_1/MLP_25000_0.13_4
./test_glass0/result_MLP_25000_0.13_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.13/record_1/MLP_25000_0.13_5
----------------------



epoch 1, loss 0.6431, train acc 65.12%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4828
epoch 501, loss 0.5031, train acc 79.65%, f1 0.7445, precision 0.6296, recall 0.9107, auc 0.8260
epoch 1001, loss 0.3538, train acc 81.40%, f1 0.7612, precision 0.6538, recall 0.9107, auc 0.8390
epoch 1501, loss 0.3319, train acc 83.72%, f1 0.7812, precision 0.6944, recall 0.8929, auc 0.8516
epoch 2001, loss 0.3827, train acc 84.30%, f1 0.7939, precision 0.6933, recall 0.9286, auc 0.8651
epoch 2501, loss 0.3413, train acc 84.88%, f1 0.8030, precision 0.6974, recall 0.9464, auc 0.8741
epoch 3001, loss 0.2786, train acc 87.21%, f1 0.8308, precision 0.7297, recall 0.9643, auc 0.8959
epoch 3501, loss 0.2413, train acc 87.79%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.9002
epoch 4001, loss 0.2211, train acc 87.79%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.9002
epoch 4501, loss 0.2380, train acc 88.95%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9089
epoch 5001, loss 0.2099, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 5501, loss 0.1770, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 6001, loss 0.1759, train acc 90.70%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9264
epoch 6501, loss 0.2401, train acc 91.28%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9307
epoch 7001, loss 0.1946, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 7501, loss 0.1870, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 8001, loss 0.1004, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 8501, loss 0.2005, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 9001, loss 0.1164, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 9501, loss 0.2103, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 10001, loss 0.2096, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 10501, loss 0.1501, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 11001, loss 0.1080, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 11501, loss 0.1469, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 12001, loss 0.1323, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 12501, loss 0.1179, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 13001, loss 0.2278, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 13501, loss 0.1221, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 14001, loss 0.2083, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 14501, loss 0.1913, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 15001, loss 0.0763, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 15501, loss 0.1267, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 16001, loss 0.1116, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 16501, loss 0.0756, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 17001, loss 0.1569, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 17501, loss 0.1527, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 18001, loss 0.0926, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 18501, loss 0.1445, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 19001, loss 0.1263, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 19501, loss 0.1352, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 20001, loss 0.0778, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 20501, loss 0.1312, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 21001, loss 0.0636, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 21501, loss 0.0910, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 22001, loss 0.1490, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 22501, loss 0.1136, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 23001, loss 0.0666, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 23501, loss 0.1129, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 24001, loss 0.1405, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 24501, loss 0.0711, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
running_time is 25.306234023
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.13
normal_0.5
./test_glass0/model_MLP_25000_0.13/record_1/MLP_25000_0.13_5
./test_glass0/result_MLP_25000_0.13_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.14/record_1/MLP_25000_0.14_1
----------------------



epoch 1, loss 0.6188, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3560, train acc 81.87%, f1 0.7704, precision 0.6582, recall 0.9286, auc 0.8469
epoch 1001, loss 0.3557, train acc 82.46%, f1 0.7761, precision 0.6667, recall 0.9286, auc 0.8512
epoch 1501, loss 0.3263, train acc 84.80%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8686
epoch 2001, loss 0.3338, train acc 84.80%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8686
epoch 2501, loss 0.2597, train acc 84.21%, f1 0.7939, precision 0.6933, recall 0.9286, auc 0.8643
epoch 3001, loss 0.3782, train acc 85.38%, f1 0.8092, precision 0.7067, recall 0.9464, auc 0.8776
epoch 3501, loss 0.2654, train acc 85.96%, f1 0.8154, precision 0.7162, recall 0.9464, auc 0.8819
epoch 4001, loss 0.2513, train acc 86.55%, f1 0.8217, precision 0.7260, recall 0.9464, auc 0.8863
epoch 4501, loss 0.1718, train acc 86.55%, f1 0.8217, precision 0.7260, recall 0.9464, auc 0.8863
epoch 5001, loss 0.2632, train acc 87.13%, f1 0.8281, precision 0.7361, recall 0.9464, auc 0.8906
epoch 5501, loss 0.1834, train acc 87.72%, f1 0.8346, precision 0.7465, recall 0.9464, auc 0.8950
epoch 6001, loss 0.2471, train acc 88.30%, f1 0.8413, precision 0.7571, recall 0.9464, auc 0.8993
epoch 6501, loss 0.2603, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 7001, loss 0.2048, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 7501, loss 0.2116, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 8001, loss 0.2051, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8501, loss 0.1105, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 9001, loss 0.1613, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 9501, loss 0.1913, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 10001, loss 0.1509, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 10501, loss 0.1879, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 11001, loss 0.1010, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 11501, loss 0.1226, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12001, loss 0.1392, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 12501, loss 0.1791, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13001, loss 0.1080, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13501, loss 0.0773, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14001, loss 0.1172, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14501, loss 0.0740, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15001, loss 0.0859, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15501, loss 0.0539, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16001, loss 0.0607, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 16501, loss 0.1171, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17001, loss 0.0975, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 17501, loss 0.1142, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18001, loss 0.0479, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18501, loss 0.1146, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19001, loss 0.0849, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19501, loss 0.0822, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 20001, loss 0.0704, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 20501, loss 0.0377, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 21001, loss 0.0503, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 21501, loss 0.0355, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 22001, loss 0.0503, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 22501, loss 0.0406, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 23001, loss 0.0859, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 23501, loss 0.0838, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24001, loss 0.0523, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24501, loss 0.0599, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 25.422240847
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.14
normal_0.5
./test_glass0/model_MLP_25000_0.14/record_1/MLP_25000_0.14_1
./test_glass0/result_MLP_25000_0.14_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.14/record_1/MLP_25000_0.14_2
----------------------



epoch 1, loss 0.6071, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3955, train acc 81.87%, f1 0.7634, precision 0.6667, recall 0.8929, auc 0.8377
epoch 1001, loss 0.2519, train acc 84.21%, f1 0.7939, precision 0.6933, recall 0.9286, auc 0.8643
epoch 1501, loss 0.3400, train acc 87.13%, f1 0.8308, precision 0.7297, recall 0.9643, auc 0.8952
epoch 2001, loss 0.2089, train acc 88.89%, f1 0.8480, precision 0.7681, recall 0.9464, auc 0.9036
epoch 2501, loss 0.3231, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 3001, loss 0.2500, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 3501, loss 0.1430, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 4001, loss 0.1137, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 4501, loss 0.2441, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 5001, loss 0.1594, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 5501, loss 0.1828, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 6001, loss 0.1665, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 6501, loss 0.2256, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 7001, loss 0.1667, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 7501, loss 0.1171, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 8001, loss 0.1291, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 8501, loss 0.1342, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 9001, loss 0.1632, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 9501, loss 0.1782, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 10001, loss 0.1197, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10501, loss 0.1039, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 11001, loss 0.1214, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 11501, loss 0.0837, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12001, loss 0.1148, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 12501, loss 0.1195, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 13001, loss 0.1542, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 13501, loss 0.0901, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 14001, loss 0.1562, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14501, loss 0.0957, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 15001, loss 0.0826, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15501, loss 0.0774, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 16001, loss 0.1073, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.0716, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17001, loss 0.0993, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17501, loss 0.0743, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18001, loss 0.1250, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18501, loss 0.0820, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 19001, loss 0.1426, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19501, loss 0.0428, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 20001, loss 0.1075, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20501, loss 0.1084, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 21001, loss 0.0604, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21501, loss 0.0960, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 22001, loss 0.0719, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22501, loss 0.1092, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 23001, loss 0.1212, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
epoch 23501, loss 0.0849, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24001, loss 0.0975, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24501, loss 0.1338, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 25.364970334000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.14
normal_0.5
./test_glass0/model_MLP_25000_0.14/record_1/MLP_25000_0.14_2
./test_glass0/result_MLP_25000_0.14_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.14/record_1/MLP_25000_0.14_3
----------------------



epoch 1, loss 0.6303, train acc 66.67%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4957
epoch 501, loss 0.4052, train acc 77.78%, f1 0.7246, precision 0.6098, recall 0.8929, auc 0.8073
epoch 1001, loss 0.4211, train acc 83.63%, f1 0.7778, precision 0.7000, recall 0.8750, auc 0.8462
epoch 1501, loss 0.2782, train acc 84.21%, f1 0.7840, precision 0.7101, recall 0.8750, auc 0.8505
epoch 2001, loss 0.3713, train acc 88.30%, f1 0.8387, precision 0.7647, recall 0.9286, auc 0.8947
epoch 2501, loss 0.3344, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 3001, loss 0.1891, train acc 88.89%, f1 0.8480, precision 0.7681, recall 0.9464, auc 0.9036
epoch 3501, loss 0.2493, train acc 89.47%, f1 0.8548, precision 0.7794, recall 0.9464, auc 0.9080
epoch 4001, loss 0.1740, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 4501, loss 0.2673, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 5001, loss 0.1736, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 5501, loss 0.1842, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 6001, loss 0.1528, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 6501, loss 0.1447, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 7001, loss 0.1537, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 7501, loss 0.0825, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 8001, loss 0.1515, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8501, loss 0.1409, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 9001, loss 0.1447, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 9501, loss 0.1208, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 10001, loss 0.1740, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 10501, loss 0.1282, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11001, loss 0.1320, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11501, loss 0.1479, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 12001, loss 0.1421, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 12501, loss 0.1332, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13001, loss 0.1014, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 13501, loss 0.0858, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14001, loss 0.0611, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.1261, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15001, loss 0.1094, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.1106, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16001, loss 0.0801, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 16501, loss 0.1234, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.1197, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.0990, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18001, loss 0.1037, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.0867, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.0850, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.0821, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20001, loss 0.0883, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20501, loss 0.0799, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21001, loss 0.0769, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21501, loss 0.1196, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22001, loss 0.0971, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22501, loss 0.0985, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23001, loss 0.1355, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23501, loss 0.1045, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24001, loss 0.0640, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 24501, loss 0.0788, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 25.464104084
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.14
normal_0.5
./test_glass0/model_MLP_25000_0.14/record_1/MLP_25000_0.14_3
./test_glass0/result_MLP_25000_0.14_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.14/record_1/MLP_25000_0.14_4
----------------------



epoch 1, loss 0.5843, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3370, train acc 80.12%, f1 0.7463, precision 0.6410, recall 0.8929, auc 0.8247
epoch 1001, loss 0.3601, train acc 81.87%, f1 0.7669, precision 0.6623, recall 0.9107, auc 0.8423
epoch 1501, loss 0.3315, train acc 83.04%, f1 0.7752, precision 0.6849, recall 0.8929, auc 0.8464
epoch 2001, loss 0.3290, train acc 84.80%, f1 0.7937, precision 0.7143, recall 0.8929, auc 0.8595
epoch 2501, loss 0.3503, train acc 87.72%, f1 0.8293, precision 0.7612, recall 0.9107, auc 0.8858
epoch 3001, loss 0.3182, train acc 88.30%, f1 0.8361, precision 0.7727, recall 0.9107, auc 0.8901
epoch 3501, loss 0.3253, train acc 88.30%, f1 0.8361, precision 0.7727, recall 0.9107, auc 0.8901
epoch 4001, loss 0.2106, train acc 89.47%, f1 0.8525, precision 0.7879, recall 0.9286, auc 0.9034
epoch 4501, loss 0.2651, train acc 90.06%, f1 0.8618, precision 0.7910, recall 0.9464, auc 0.9123
epoch 5001, loss 0.2572, train acc 90.64%, f1 0.8689, precision 0.8030, recall 0.9464, auc 0.9167
epoch 5501, loss 0.2178, train acc 90.64%, f1 0.8689, precision 0.8030, recall 0.9464, auc 0.9167
epoch 6001, loss 0.2521, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 6501, loss 0.1936, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 7001, loss 0.2625, train acc 91.23%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9210
epoch 7501, loss 0.2110, train acc 91.23%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9210
epoch 8001, loss 0.1499, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 8501, loss 0.1777, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 9001, loss 0.1234, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 9501, loss 0.1835, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 10001, loss 0.1738, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 10501, loss 0.1162, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 11001, loss 0.1787, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 11501, loss 0.1071, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 12001, loss 0.2284, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 12501, loss 0.1707, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 13001, loss 0.1112, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 13501, loss 0.1371, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 14001, loss 0.0963, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 14501, loss 0.1768, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 15001, loss 0.0848, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 15501, loss 0.1541, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 16001, loss 0.1696, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 16501, loss 0.0818, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 17001, loss 0.1553, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 17501, loss 0.0973, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 18001, loss 0.1622, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 18501, loss 0.1513, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 19001, loss 0.1197, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 19501, loss 0.1500, train acc 95.91%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9604
epoch 20001, loss 0.0984, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 20501, loss 0.0792, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 21001, loss 0.1018, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 21501, loss 0.1324, train acc 96.49%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9648
epoch 22001, loss 0.1395, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 22501, loss 0.0741, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 23001, loss 0.1277, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 23501, loss 0.1184, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 24001, loss 0.0888, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 24501, loss 0.0802, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
running_time is 25.413108508
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.14
normal_0.5
./test_glass0/model_MLP_25000_0.14/record_1/MLP_25000_0.14_4
./test_glass0/result_MLP_25000_0.14_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.14/record_1/MLP_25000_0.14_5
----------------------



epoch 1, loss 0.6536, train acc 51.74%, f1 0.2095, precision 0.2245, recall 0.1964, auc 0.4344
epoch 501, loss 0.3900, train acc 77.33%, f1 0.7194, precision 0.6024, recall 0.8929, auc 0.8042
epoch 1001, loss 0.3718, train acc 80.23%, f1 0.7500, precision 0.6375, recall 0.9107, auc 0.8304
epoch 1501, loss 0.3566, train acc 81.40%, f1 0.7647, precision 0.6500, recall 0.9286, auc 0.8436
epoch 2001, loss 0.2833, train acc 83.72%, f1 0.7879, precision 0.6842, recall 0.9286, auc 0.8608
epoch 2501, loss 0.4291, train acc 83.14%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8565
epoch 3001, loss 0.3054, train acc 83.14%, f1 0.7852, precision 0.6709, recall 0.9464, auc 0.8611
epoch 3501, loss 0.2809, train acc 83.72%, f1 0.7910, precision 0.6795, recall 0.9464, auc 0.8655
epoch 4001, loss 0.3573, train acc 86.05%, f1 0.8125, precision 0.7222, recall 0.9286, auc 0.8781
epoch 4501, loss 0.2599, train acc 87.21%, f1 0.8281, precision 0.7361, recall 0.9464, auc 0.8913
epoch 5001, loss 0.2279, train acc 87.21%, f1 0.8281, precision 0.7361, recall 0.9464, auc 0.8913
epoch 5501, loss 0.3074, train acc 89.53%, f1 0.8548, precision 0.7794, recall 0.9464, auc 0.9086
epoch 6001, loss 0.2640, train acc 89.53%, f1 0.8548, precision 0.7794, recall 0.9464, auc 0.9086
epoch 6501, loss 0.1910, train acc 90.12%, f1 0.8618, precision 0.7910, recall 0.9464, auc 0.9129
epoch 7001, loss 0.2093, train acc 90.70%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9218
epoch 7501, loss 0.2114, train acc 91.28%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9261
epoch 8001, loss 0.1565, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 8501, loss 0.1912, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 9001, loss 0.1986, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 9501, loss 0.1781, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 10001, loss 0.1833, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 10501, loss 0.1898, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 11001, loss 0.2295, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 11501, loss 0.2055, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 12001, loss 0.1859, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 12501, loss 0.1329, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 13001, loss 0.1505, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 13501, loss 0.1583, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 14001, loss 0.1994, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 14501, loss 0.1875, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 15001, loss 0.1249, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 15501, loss 0.1483, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 16001, loss 0.1072, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 16501, loss 0.2049, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 17001, loss 0.0887, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 17501, loss 0.1566, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 18001, loss 0.0816, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 18501, loss 0.1339, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 19001, loss 0.1183, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 19501, loss 0.1151, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 20001, loss 0.1267, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 20501, loss 0.0913, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 21001, loss 0.1133, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 21501, loss 0.1314, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 22001, loss 0.1537, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 22501, loss 0.1613, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 23001, loss 0.1209, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 23501, loss 0.1354, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 24001, loss 0.1586, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 24501, loss 0.0833, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
running_time is 25.494695459
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.14
normal_0.5
./test_glass0/model_MLP_25000_0.14/record_1/MLP_25000_0.14_5
./test_glass0/result_MLP_25000_0.14_normal_0.5/record_1/
----------------------



the AUC is 0.5535714285714285

the Fscore is 0.23529411764705882

the precision is 0.6666666666666666

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.15/record_1/MLP_25000_0.15_1
----------------------



epoch 1, loss 0.5546, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.2962, train acc 83.04%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8556
epoch 1001, loss 0.3551, train acc 82.46%, f1 0.7761, precision 0.6667, recall 0.9286, auc 0.8512
epoch 1501, loss 0.4817, train acc 84.21%, f1 0.7939, precision 0.6933, recall 0.9286, auc 0.8643
epoch 2001, loss 0.2430, train acc 84.21%, f1 0.7939, precision 0.6933, recall 0.9286, auc 0.8643
epoch 2501, loss 0.2765, train acc 84.21%, f1 0.7939, precision 0.6933, recall 0.9286, auc 0.8643
epoch 3001, loss 0.2718, train acc 86.55%, f1 0.8189, precision 0.7324, recall 0.9286, auc 0.8817
epoch 3501, loss 0.1809, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 4001, loss 0.2075, train acc 88.89%, f1 0.8480, precision 0.7681, recall 0.9464, auc 0.9036
epoch 4501, loss 0.2416, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 5001, loss 0.2490, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 5501, loss 0.1886, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 6001, loss 0.1229, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 6501, loss 0.2129, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 7001, loss 0.2032, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 7501, loss 0.1012, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 8001, loss 0.1241, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 8501, loss 0.1763, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9001, loss 0.1646, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9501, loss 0.1558, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10001, loss 0.1330, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10501, loss 0.1491, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11001, loss 0.1260, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11501, loss 0.1583, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12001, loss 0.0817, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12501, loss 0.0727, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13001, loss 0.0954, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13501, loss 0.1044, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14001, loss 0.0908, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.0794, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15001, loss 0.1032, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.0635, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.0922, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.0995, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.1036, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.0755, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.1115, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.1063, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.0819, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.0947, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20001, loss 0.0584, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20501, loss 0.1458, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21001, loss 0.0898, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21501, loss 0.0679, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22001, loss 0.1163, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22501, loss 0.1035, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23001, loss 0.0576, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23501, loss 0.0816, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24001, loss 0.0581, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24501, loss 0.1491, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 25.437554849
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.15
normal_0.5
./test_glass0/model_MLP_25000_0.15/record_1/MLP_25000_0.15_1
./test_glass0/result_MLP_25000_0.15_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.15/record_1/MLP_25000_0.15_2
----------------------



epoch 1, loss 0.6536, train acc 53.22%, f1 0.5789, precision 0.4104, recall 0.9821, auc 0.6476
epoch 501, loss 0.3672, train acc 79.53%, f1 0.7368, precision 0.6364, recall 0.8750, auc 0.8158
epoch 1001, loss 0.2949, train acc 83.04%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8556
epoch 1501, loss 0.2887, train acc 86.55%, f1 0.8217, precision 0.7260, recall 0.9464, auc 0.8863
epoch 2001, loss 0.3128, train acc 89.47%, f1 0.8548, precision 0.7794, recall 0.9464, auc 0.9080
epoch 2501, loss 0.2711, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 3001, loss 0.1891, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 3501, loss 0.1973, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 4001, loss 0.1957, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 4501, loss 0.1695, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 5001, loss 0.1034, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 5501, loss 0.1517, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 6001, loss 0.1518, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 6501, loss 0.2244, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 7001, loss 0.1125, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 7501, loss 0.1494, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 8001, loss 0.1330, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8501, loss 0.1084, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 9001, loss 0.1258, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9501, loss 0.1589, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 10001, loss 0.1273, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10501, loss 0.1449, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11001, loss 0.0708, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 11501, loss 0.0618, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 12001, loss 0.1478, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 12501, loss 0.1110, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 13001, loss 0.2211, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 13501, loss 0.0691, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 14001, loss 0.1043, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 14501, loss 0.1357, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 15001, loss 0.1044, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 15501, loss 0.1318, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 16001, loss 0.1244, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 16501, loss 0.0548, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 17001, loss 0.0980, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 17501, loss 0.0527, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18001, loss 0.0506, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18501, loss 0.1518, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 19001, loss 0.0811, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 19501, loss 0.1963, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 20001, loss 0.0990, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 20501, loss 0.0599, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 21001, loss 0.0653, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 21501, loss 0.1210, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22001, loss 0.0726, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 22501, loss 0.0703, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23001, loss 0.1055, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23501, loss 0.1029, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 24001, loss 0.0837, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24501, loss 0.1431, train acc 98.25%, f1 0.9735, precision 0.9649, recall 0.9821, auc 0.9824
running_time is 25.403417122
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.15
normal_0.5
./test_glass0/model_MLP_25000_0.15/record_1/MLP_25000_0.15_2
./test_glass0/result_MLP_25000_0.15_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.15/record_1/MLP_25000_0.15_3
----------------------



epoch 1, loss 0.6784, train acc 43.86%, f1 0.5340, precision 0.3667, recall 0.9821, auc 0.5780
epoch 501, loss 0.3370, train acc 78.36%, f1 0.7176, precision 0.6267, recall 0.8393, auc 0.7979
epoch 1001, loss 0.4128, train acc 80.70%, f1 0.7481, precision 0.6533, recall 0.8750, auc 0.8245
epoch 1501, loss 0.3107, train acc 83.04%, f1 0.7752, precision 0.6849, recall 0.8929, auc 0.8464
epoch 2001, loss 0.2674, train acc 87.72%, f1 0.8320, precision 0.7536, recall 0.9286, auc 0.8904
epoch 2501, loss 0.1654, train acc 88.30%, f1 0.8413, precision 0.7571, recall 0.9464, auc 0.8993
epoch 3001, loss 0.1756, train acc 88.89%, f1 0.8480, precision 0.7681, recall 0.9464, auc 0.9036
epoch 3501, loss 0.2195, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 4001, loss 0.1833, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 4501, loss 0.1813, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 5001, loss 0.1473, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 5501, loss 0.1148, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 6001, loss 0.1047, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 6501, loss 0.1594, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 7001, loss 0.1014, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 7501, loss 0.1501, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 8001, loss 0.1251, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 8501, loss 0.1378, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 9001, loss 0.1078, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 9501, loss 0.0992, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 10001, loss 0.1507, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 10501, loss 0.1102, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 11001, loss 0.0747, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 11501, loss 0.1032, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 12001, loss 0.0674, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 12501, loss 0.0688, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 13001, loss 0.0941, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 13501, loss 0.0665, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 14001, loss 0.0718, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 14501, loss 0.0676, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15001, loss 0.0560, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 15501, loss 0.0628, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16001, loss 0.0884, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 16501, loss 0.0788, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17001, loss 0.0813, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 17501, loss 0.0777, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18001, loss 0.0798, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 18501, loss 0.0413, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19001, loss 0.0532, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 19501, loss 0.0702, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 20001, loss 0.0635, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 20501, loss 0.0375, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 21001, loss 0.0603, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 21501, loss 0.0565, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 22001, loss 0.0462, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 22501, loss 0.0506, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 23001, loss 0.0579, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 23501, loss 0.0513, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 24001, loss 0.0514, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 24501, loss 0.0638, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
running_time is 25.389237546
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.15
normal_0.5
./test_glass0/model_MLP_25000_0.15/record_1/MLP_25000_0.15_3
./test_glass0/result_MLP_25000_0.15_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.15/record_1/MLP_25000_0.15_4
----------------------



epoch 1, loss 0.6536, train acc 47.95%, f1 0.5189, precision 0.3721, recall 0.8571, auc 0.5764
epoch 501, loss 0.3446, train acc 78.95%, f1 0.7391, precision 0.6220, recall 0.9107, auc 0.8206
epoch 1001, loss 0.4101, train acc 81.29%, f1 0.7612, precision 0.6538, recall 0.9107, auc 0.8380
epoch 1501, loss 0.3544, train acc 83.04%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8556
epoch 2001, loss 0.2726, train acc 85.96%, f1 0.8125, precision 0.7222, recall 0.9286, auc 0.8773
epoch 2501, loss 0.2422, train acc 87.72%, f1 0.8320, precision 0.7536, recall 0.9286, auc 0.8904
epoch 3001, loss 0.2327, train acc 89.47%, f1 0.8525, precision 0.7879, recall 0.9286, auc 0.9034
epoch 3501, loss 0.1777, train acc 90.64%, f1 0.8667, precision 0.8125, recall 0.9286, auc 0.9121
epoch 4001, loss 0.1597, train acc 91.23%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9210
epoch 4501, loss 0.0989, train acc 91.23%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9210
epoch 5001, loss 0.1869, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 5501, loss 0.1545, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 6001, loss 0.1875, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 6501, loss 0.2394, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 7001, loss 0.2667, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 7501, loss 0.2405, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 8001, loss 0.1752, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 8501, loss 0.1980, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 9001, loss 0.1752, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 9501, loss 0.1928, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 10001, loss 0.1870, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 10501, loss 0.2300, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 11001, loss 0.2141, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 11501, loss 0.1118, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 12001, loss 0.1110, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 12501, loss 0.1699, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 13001, loss 0.1810, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 13501, loss 0.1610, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 14001, loss 0.1911, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 14501, loss 0.1611, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 15001, loss 0.1890, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 15501, loss 0.1247, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 16001, loss 0.1790, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 16501, loss 0.2451, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 17001, loss 0.0976, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 17501, loss 0.1384, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 18001, loss 0.2280, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 18501, loss 0.1122, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 19001, loss 0.1210, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 19501, loss 0.1412, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 20001, loss 0.1216, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 20501, loss 0.1670, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 21001, loss 0.1718, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 21501, loss 0.2024, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 22001, loss 0.2313, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 22501, loss 0.1519, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 23001, loss 0.2287, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 23501, loss 0.1314, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 24001, loss 0.1068, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 24501, loss 0.1663, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
running_time is 25.530544030999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.15
normal_0.5
./test_glass0/model_MLP_25000_0.15/record_1/MLP_25000_0.15_4
./test_glass0/result_MLP_25000_0.15_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.15/record_1/MLP_25000_0.15_5
----------------------



epoch 1, loss 0.6536, train acc 48.26%, f1 0.5572, precision 0.3862, recall 1.0000, auc 0.6164
epoch 501, loss 0.3445, train acc 79.07%, f1 0.7391, precision 0.6220, recall 0.9107, auc 0.8217
epoch 1001, loss 0.4015, train acc 80.81%, f1 0.7591, precision 0.6420, recall 0.9286, auc 0.8393
epoch 1501, loss 0.3011, train acc 83.14%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8565
epoch 2001, loss 0.3914, train acc 83.72%, f1 0.7879, precision 0.6842, recall 0.9286, auc 0.8608
epoch 2501, loss 0.2978, train acc 84.88%, f1 0.8000, precision 0.7027, recall 0.9286, auc 0.8695
epoch 3001, loss 0.2923, train acc 86.63%, f1 0.8217, precision 0.7260, recall 0.9464, auc 0.8870
epoch 3501, loss 0.1797, train acc 87.21%, f1 0.8308, precision 0.7297, recall 0.9643, auc 0.8959
epoch 4001, loss 0.2644, train acc 88.37%, f1 0.8437, precision 0.7500, recall 0.9643, auc 0.9046
epoch 4501, loss 0.2316, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 5001, loss 0.2075, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 5501, loss 0.1731, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 6001, loss 0.2098, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 6501, loss 0.1952, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 7001, loss 0.1714, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 7501, loss 0.2316, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 8001, loss 0.1434, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 8501, loss 0.1232, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 9001, loss 0.1712, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 9501, loss 0.2236, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 10001, loss 0.1271, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 10501, loss 0.1491, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 11001, loss 0.0844, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 11501, loss 0.1926, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 12001, loss 0.1571, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 12501, loss 0.1007, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 13001, loss 0.1381, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 13501, loss 0.2408, train acc 94.77%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9520
epoch 14001, loss 0.0934, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 14501, loss 0.0910, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 15001, loss 0.1485, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 15501, loss 0.1835, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 16001, loss 0.1339, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 16501, loss 0.1282, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 17001, loss 0.2102, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 17501, loss 0.0824, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 18001, loss 0.0692, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 18501, loss 0.0978, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 19001, loss 0.1105, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 19501, loss 0.1322, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 20001, loss 0.1244, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 20501, loss 0.1118, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 21001, loss 0.1066, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 21501, loss 0.1645, train acc 95.35%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9563
epoch 22001, loss 0.2093, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 22501, loss 0.1109, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 23001, loss 0.0987, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 23501, loss 0.1377, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 24001, loss 0.0957, train acc 96.51%, f1 0.9474, precision 0.9310, recall 0.9643, auc 0.9649
epoch 24501, loss 0.0712, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
running_time is 25.372230162999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.15
normal_0.5
./test_glass0/model_MLP_25000_0.15/record_1/MLP_25000_0.15_5
./test_glass0/result_MLP_25000_0.15_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.16/record_1/MLP_25000_0.16_1
----------------------



epoch 1, loss 0.6279, train acc 60.23%, f1 0.2766, precision 0.3421, recall 0.2321, auc 0.5074
epoch 501, loss 0.4488, train acc 81.29%, f1 0.7647, precision 0.6500, recall 0.9286, auc 0.8425
epoch 1001, loss 0.4091, train acc 81.29%, f1 0.7647, precision 0.6500, recall 0.9286, auc 0.8425
epoch 1501, loss 0.2424, train acc 83.04%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8556
epoch 2001, loss 0.3457, train acc 84.21%, f1 0.7939, precision 0.6933, recall 0.9286, auc 0.8643
epoch 2501, loss 0.2261, train acc 84.21%, f1 0.7939, precision 0.6933, recall 0.9286, auc 0.8643
epoch 3001, loss 0.3367, train acc 84.80%, f1 0.8030, precision 0.6974, recall 0.9464, auc 0.8732
epoch 3501, loss 0.2851, train acc 85.96%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8865
epoch 4001, loss 0.3321, train acc 88.30%, f1 0.8413, precision 0.7571, recall 0.9464, auc 0.8993
epoch 4501, loss 0.2604, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 5001, loss 0.3028, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 5501, loss 0.1900, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 6001, loss 0.1871, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 6501, loss 0.1380, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 7001, loss 0.2015, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 7501, loss 0.2088, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 8001, loss 0.0986, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 8501, loss 0.1873, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 9001, loss 0.1305, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 9501, loss 0.1078, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 10001, loss 0.1132, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 10501, loss 0.1624, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 11001, loss 0.1707, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 11501, loss 0.0851, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12001, loss 0.2271, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12501, loss 0.1195, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13001, loss 0.1566, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13501, loss 0.1646, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14001, loss 0.1137, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14501, loss 0.1275, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.1210, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15501, loss 0.0640, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16001, loss 0.0594, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.0690, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17001, loss 0.0979, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17501, loss 0.1326, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18001, loss 0.1277, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.1292, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19001, loss 0.1434, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19501, loss 0.1201, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20001, loss 0.1549, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20501, loss 0.0798, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21001, loss 0.0516, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21501, loss 0.1088, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 22001, loss 0.1994, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 22501, loss 0.1000, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 23001, loss 0.1597, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 23501, loss 0.1265, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 24001, loss 0.0676, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 24501, loss 0.1474, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
running_time is 25.453620993
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.16
normal_0.5
./test_glass0/model_MLP_25000_0.16/record_1/MLP_25000_0.16_1
./test_glass0/result_MLP_25000_0.16_normal_0.5/record_1/
----------------------



the AUC is 0.5431034482758621

the Fscore is 0.4242424242424242

the precision is 0.3684210526315789

the recall is 0.5

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.16/record_1/MLP_25000_0.16_2
----------------------



epoch 1, loss 0.6279, train acc 63.16%, f1 0.0308, precision 0.1111, recall 0.0179, auc 0.4741
epoch 501, loss 0.3025, train acc 82.46%, f1 0.7761, precision 0.6667, recall 0.9286, auc 0.8512
epoch 1001, loss 0.3307, train acc 83.04%, f1 0.7852, precision 0.6709, recall 0.9464, auc 0.8602
epoch 1501, loss 0.2430, train acc 84.21%, f1 0.7970, precision 0.6883, recall 0.9464, auc 0.8689
epoch 2001, loss 0.2653, train acc 88.30%, f1 0.8437, precision 0.7500, recall 0.9643, auc 0.9039
epoch 2501, loss 0.3166, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 3001, loss 0.2784, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 3501, loss 0.2875, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 4001, loss 0.1594, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 4501, loss 0.1250, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 5001, loss 0.1988, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 5501, loss 0.1884, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 6001, loss 0.1138, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 6501, loss 0.1558, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 7001, loss 0.1315, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 7501, loss 0.1792, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8001, loss 0.1977, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8501, loss 0.1555, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9001, loss 0.1317, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 9501, loss 0.1197, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 10001, loss 0.0688, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 10501, loss 0.1660, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11001, loss 0.0966, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11501, loss 0.1245, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12001, loss 0.1059, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 12501, loss 0.1046, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13001, loss 0.0691, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 13501, loss 0.1545, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14001, loss 0.0769, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 14501, loss 0.0747, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 15001, loss 0.0864, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 15501, loss 0.0574, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 16001, loss 0.1157, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 16501, loss 0.1108, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.0601, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 17501, loss 0.0888, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.1134, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.1371, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19001, loss 0.1019, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19501, loss 0.0687, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 20001, loss 0.0873, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 20501, loss 0.1312, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21001, loss 0.0936, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21501, loss 0.1578, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22001, loss 0.0957, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22501, loss 0.1540, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23001, loss 0.0852, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 23501, loss 0.0669, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24001, loss 0.0708, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 24501, loss 0.1340, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 25.433135326
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.16
normal_0.5
./test_glass0/model_MLP_25000_0.16/record_1/MLP_25000_0.16_2
./test_glass0/result_MLP_25000_0.16_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.16/record_1/MLP_25000_0.16_3
----------------------



epoch 1, loss 0.6676, train acc 38.01%, f1 0.5138, precision 0.3457, recall 1.0000, auc 0.5391
epoch 501, loss 0.3891, train acc 78.36%, f1 0.7259, precision 0.6203, recall 0.8750, auc 0.8071
epoch 1001, loss 0.3104, train acc 80.70%, f1 0.7481, precision 0.6533, recall 0.8750, auc 0.8245
epoch 1501, loss 0.3236, train acc 83.04%, f1 0.7717, precision 0.6901, recall 0.8750, auc 0.8418
epoch 2001, loss 0.4044, train acc 85.96%, f1 0.8065, precision 0.7353, recall 0.8929, auc 0.8682
epoch 2501, loss 0.2729, train acc 87.13%, f1 0.8197, precision 0.7576, recall 0.8929, auc 0.8769
epoch 3001, loss 0.1648, train acc 88.89%, f1 0.8480, precision 0.7681, recall 0.9464, auc 0.9036
epoch 3501, loss 0.1745, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 4001, loss 0.1702, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 4501, loss 0.1913, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 5001, loss 0.1525, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 5501, loss 0.1176, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 6001, loss 0.1610, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 6501, loss 0.1390, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 7001, loss 0.1623, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 7501, loss 0.1227, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 8001, loss 0.1095, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 8501, loss 0.1713, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 9001, loss 0.1407, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 9501, loss 0.1456, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 10001, loss 0.1281, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10501, loss 0.1113, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 11001, loss 0.1005, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11501, loss 0.1332, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12001, loss 0.1333, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 12501, loss 0.1192, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13001, loss 0.1187, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13501, loss 0.1142, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14001, loss 0.0902, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 14501, loss 0.0832, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 15001, loss 0.0709, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 15501, loss 0.0843, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 16001, loss 0.1001, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 16501, loss 0.0780, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 17001, loss 0.0633, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 17501, loss 0.0681, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18001, loss 0.0781, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18501, loss 0.0782, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19001, loss 0.0727, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19501, loss 0.0737, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 20001, loss 0.0726, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 20501, loss 0.0709, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 21001, loss 0.0965, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 21501, loss 0.0638, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 22001, loss 0.0626, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 22501, loss 0.0721, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 23001, loss 0.0730, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 23501, loss 0.0725, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 24001, loss 0.0901, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
epoch 24501, loss 0.0682, train acc 100.00%, f1 1.0000, precision 1.0000, recall 1.0000, auc 1.0000
running_time is 25.418814415
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.16
normal_0.5
./test_glass0/model_MLP_25000_0.16/record_1/MLP_25000_0.16_3
./test_glass0/result_MLP_25000_0.16_normal_0.5/record_1/
----------------------



the AUC is 0.625615763546798

the Fscore is 0.4210526315789473

the precision is 0.8

the recall is 0.2857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.16/record_1/MLP_25000_0.16_4
----------------------



epoch 1, loss 0.6410, train acc 46.20%, f1 0.5490, precision 0.3784, recall 1.0000, auc 0.6000
epoch 501, loss 0.4016, train acc 80.12%, f1 0.7606, precision 0.6279, recall 0.9643, auc 0.8430
epoch 1001, loss 0.3953, train acc 78.95%, f1 0.7391, precision 0.6220, recall 0.9107, auc 0.8206
epoch 1501, loss 0.4066, train acc 79.53%, f1 0.7482, precision 0.6265, recall 0.9286, auc 0.8295
epoch 2001, loss 0.3397, train acc 83.04%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8556
epoch 2501, loss 0.2726, train acc 85.96%, f1 0.8095, precision 0.7286, recall 0.9107, auc 0.8727
epoch 3001, loss 0.2329, train acc 89.47%, f1 0.8548, precision 0.7794, recall 0.9464, auc 0.9080
epoch 3501, loss 0.2903, train acc 90.06%, f1 0.8618, precision 0.7910, recall 0.9464, auc 0.9123
epoch 4001, loss 0.2529, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 4501, loss 0.2901, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 5001, loss 0.2510, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 5501, loss 0.2153, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 6001, loss 0.1855, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 6501, loss 0.1865, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 7001, loss 0.1832, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 7501, loss 0.1515, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 8001, loss 0.1824, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 8501, loss 0.1687, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 9001, loss 0.1502, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 9501, loss 0.2108, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 10001, loss 0.1991, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 10501, loss 0.1771, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 11001, loss 0.2106, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 11501, loss 0.1763, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 12001, loss 0.2240, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 12501, loss 0.0922, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 13001, loss 0.2002, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 13501, loss 0.2010, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 14001, loss 0.1309, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 14501, loss 0.1856, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 15001, loss 0.1495, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 15501, loss 0.0971, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 16001, loss 0.1890, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 16501, loss 0.1578, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 17001, loss 0.2068, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 17501, loss 0.1159, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 18001, loss 0.1391, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 18501, loss 0.2424, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 19001, loss 0.1077, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 19501, loss 0.0961, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 20001, loss 0.1223, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 20501, loss 0.1365, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 21001, loss 0.1532, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 21501, loss 0.1155, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 22001, loss 0.1015, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 22501, loss 0.1905, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 23001, loss 0.1424, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 23501, loss 0.1661, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 24001, loss 0.0775, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 24501, loss 0.1582, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
running_time is 25.533237604
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.16
normal_0.5
./test_glass0/model_MLP_25000_0.16/record_1/MLP_25000_0.16_4
./test_glass0/result_MLP_25000_0.16_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.16/record_1/MLP_25000_0.16_5
----------------------



epoch 1, loss 0.6012, train acc 65.12%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4828
epoch 501, loss 0.4051, train acc 78.49%, f1 0.7338, precision 0.6145, recall 0.9107, auc 0.8174
epoch 1001, loss 0.3046, train acc 81.40%, f1 0.7647, precision 0.6500, recall 0.9286, auc 0.8436
epoch 1501, loss 0.2558, train acc 81.98%, f1 0.7704, precision 0.6582, recall 0.9286, auc 0.8479
epoch 2001, loss 0.3113, train acc 83.14%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8565
epoch 2501, loss 0.2705, train acc 84.88%, f1 0.8030, precision 0.6974, recall 0.9464, auc 0.8741
epoch 3001, loss 0.2975, train acc 84.88%, f1 0.8030, precision 0.6974, recall 0.9464, auc 0.8741
epoch 3501, loss 0.2382, train acc 84.88%, f1 0.8030, precision 0.6974, recall 0.9464, auc 0.8741
epoch 4001, loss 0.2681, train acc 85.47%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8830
epoch 4501, loss 0.2360, train acc 86.05%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8873
epoch 5001, loss 0.2411, train acc 86.63%, f1 0.8244, precision 0.7200, recall 0.9643, auc 0.8916
epoch 5501, loss 0.2481, train acc 88.37%, f1 0.8437, precision 0.7500, recall 0.9643, auc 0.9046
epoch 6001, loss 0.2142, train acc 88.95%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9089
epoch 6501, loss 0.1649, train acc 91.28%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9307
epoch 7001, loss 0.2167, train acc 91.28%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9307
epoch 7501, loss 0.2516, train acc 91.28%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9261
epoch 8001, loss 0.1480, train acc 91.28%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9261
epoch 8501, loss 0.2653, train acc 91.28%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9261
epoch 9001, loss 0.1486, train acc 91.28%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9261
epoch 9501, loss 0.1688, train acc 91.86%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9304
epoch 10001, loss 0.1902, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 10501, loss 0.1503, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 11001, loss 0.2140, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 11501, loss 0.1545, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 12001, loss 0.1337, train acc 92.44%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9347
epoch 12501, loss 0.1936, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 13001, loss 0.1939, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 13501, loss 0.1677, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 14001, loss 0.1381, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 14501, loss 0.1283, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 15001, loss 0.1207, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 15501, loss 0.1240, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 16001, loss 0.1667, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 16501, loss 0.1120, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 17001, loss 0.2206, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 17501, loss 0.1717, train acc 94.19%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9477
epoch 18001, loss 0.1354, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 18501, loss 0.1462, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 19001, loss 0.1615, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 19501, loss 0.1487, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 20001, loss 0.1440, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 20501, loss 0.1338, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 21001, loss 0.1789, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 21501, loss 0.1635, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 22001, loss 0.1201, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 22501, loss 0.1139, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 23001, loss 0.1005, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 23501, loss 0.1464, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 24001, loss 0.1110, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 24501, loss 0.1454, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
running_time is 25.475516287999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.16
normal_0.5
./test_glass0/model_MLP_25000_0.16/record_1/MLP_25000_0.16_5
./test_glass0/result_MLP_25000_0.16_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.17/record_1/MLP_25000_0.17_1
----------------------



epoch 1, loss 0.6697, train acc 36.26%, f1 0.5023, precision 0.3374, recall 0.9821, auc 0.5215
epoch 501, loss 0.3878, train acc 81.29%, f1 0.7647, precision 0.6500, recall 0.9286, auc 0.8425
epoch 1001, loss 0.3428, train acc 82.46%, f1 0.7794, precision 0.6625, recall 0.9464, auc 0.8558
epoch 1501, loss 0.3575, train acc 83.63%, f1 0.7910, precision 0.6795, recall 0.9464, auc 0.8645
epoch 2001, loss 0.1835, train acc 85.96%, f1 0.8154, precision 0.7162, recall 0.9464, auc 0.8819
epoch 2501, loss 0.2105, train acc 87.13%, f1 0.8281, precision 0.7361, recall 0.9464, auc 0.8906
epoch 3001, loss 0.2198, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 3501, loss 0.2881, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 4001, loss 0.2198, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 4501, loss 0.2563, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 5001, loss 0.1735, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 5501, loss 0.2365, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 6001, loss 0.1706, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 6501, loss 0.1921, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 7001, loss 0.1187, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 7501, loss 0.1121, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 8001, loss 0.2189, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 8501, loss 0.0814, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 9001, loss 0.1509, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 9501, loss 0.1474, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10001, loss 0.1176, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10501, loss 0.1158, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11001, loss 0.1000, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11501, loss 0.1249, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12001, loss 0.0980, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12501, loss 0.1348, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13001, loss 0.1477, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 13501, loss 0.1201, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14001, loss 0.1507, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.0856, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15001, loss 0.1431, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.0826, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.0999, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.0546, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.0962, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.1560, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.0647, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.0616, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.1116, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.1065, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20001, loss 0.0925, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20501, loss 0.0563, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21001, loss 0.0768, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21501, loss 0.1307, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22001, loss 0.0452, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22501, loss 0.1271, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23001, loss 0.0994, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23501, loss 0.1308, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24001, loss 0.0991, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24501, loss 0.0414, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 25.455599718
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.17
normal_0.5
./test_glass0/model_MLP_25000_0.17/record_1/MLP_25000_0.17_1
./test_glass0/result_MLP_25000_0.17_normal_0.5/record_1/
----------------------



the AUC is 0.4507389162561576

the Fscore is 0.17391304347826086

the precision is 0.2222222222222222

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.17/record_1/MLP_25000_0.17_2
----------------------



epoch 1, loss 0.6696, train acc 32.75%, f1 0.4934, precision 0.3275, recall 1.0000, auc 0.5000
epoch 501, loss 0.4097, train acc 79.53%, f1 0.7445, precision 0.6296, recall 0.9107, auc 0.8249
epoch 1001, loss 0.3716, train acc 83.63%, f1 0.7910, precision 0.6795, recall 0.9464, auc 0.8645
epoch 1501, loss 0.3167, train acc 85.96%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8865
epoch 2001, loss 0.2814, train acc 87.72%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.8995
epoch 2501, loss 0.2495, train acc 89.47%, f1 0.8548, precision 0.7794, recall 0.9464, auc 0.9080
epoch 3001, loss 0.1941, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 3501, loss 0.1854, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 4001, loss 0.1839, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 4501, loss 0.2596, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 5001, loss 0.1790, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 5501, loss 0.0996, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 6001, loss 0.1802, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 6501, loss 0.1084, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 7001, loss 0.1684, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 7501, loss 0.1480, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 8001, loss 0.1105, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 8501, loss 0.1701, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 9001, loss 0.1509, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 9501, loss 0.1221, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10001, loss 0.1250, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 10501, loss 0.1003, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 11001, loss 0.1104, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 11501, loss 0.1042, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 12001, loss 0.0970, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 12501, loss 0.1073, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 13001, loss 0.0842, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 13501, loss 0.0720, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14001, loss 0.1312, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 14501, loss 0.1328, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15001, loss 0.0739, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15501, loss 0.0826, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16001, loss 0.1465, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16501, loss 0.0741, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.0862, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.1703, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.0953, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.0729, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.1046, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.0930, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20001, loss 0.1272, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20501, loss 0.1276, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21001, loss 0.0882, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21501, loss 0.1465, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22001, loss 0.1233, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22501, loss 0.0930, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23001, loss 0.1421, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23501, loss 0.0993, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24001, loss 0.0640, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24501, loss 0.1270, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 25.294018191
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.17
normal_0.5
./test_glass0/model_MLP_25000_0.17/record_1/MLP_25000_0.17_2
./test_glass0/result_MLP_25000_0.17_normal_0.5/record_1/
----------------------



the AUC is 0.5197044334975369

the Fscore is 0.21052631578947364

the precision is 0.4

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.17/record_1/MLP_25000_0.17_3
----------------------



epoch 1, loss 0.6555, train acc 32.75%, f1 0.4934, precision 0.3275, recall 1.0000, auc 0.5000
epoch 501, loss 0.4050, train acc 78.95%, f1 0.7391, precision 0.6220, recall 0.9107, auc 0.8206
epoch 1001, loss 0.3831, train acc 78.95%, f1 0.7353, precision 0.6250, recall 0.8929, auc 0.8160
epoch 1501, loss 0.3702, train acc 81.29%, f1 0.7576, precision 0.6579, recall 0.8929, auc 0.8334
epoch 2001, loss 0.3585, train acc 83.63%, f1 0.7846, precision 0.6892, recall 0.9107, auc 0.8554
epoch 2501, loss 0.2826, train acc 86.55%, f1 0.8189, precision 0.7324, recall 0.9286, auc 0.8817
epoch 3001, loss 0.1945, train acc 87.72%, f1 0.8320, precision 0.7536, recall 0.9286, auc 0.8904
epoch 3501, loss 0.2052, train acc 88.89%, f1 0.8480, precision 0.7681, recall 0.9464, auc 0.9036
epoch 4001, loss 0.2694, train acc 88.89%, f1 0.8480, precision 0.7681, recall 0.9464, auc 0.9036
epoch 4501, loss 0.1965, train acc 91.23%, f1 0.8760, precision 0.8154, recall 0.9464, auc 0.9210
epoch 5001, loss 0.2246, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 5501, loss 0.2252, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 6001, loss 0.1220, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 6501, loss 0.0920, train acc 92.40%, f1 0.8889, precision 0.8525, recall 0.9286, auc 0.9252
epoch 7001, loss 0.1418, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 7501, loss 0.1323, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 8001, loss 0.2133, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 8501, loss 0.1616, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 9001, loss 0.1554, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 9501, loss 0.1718, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 10001, loss 0.1434, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 10501, loss 0.1815, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 11001, loss 0.1620, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 11501, loss 0.1527, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 12001, loss 0.1457, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 12501, loss 0.0986, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 13001, loss 0.1671, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 13501, loss 0.1476, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 14001, loss 0.1449, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 14501, loss 0.1101, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 15001, loss 0.1426, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 15501, loss 0.2138, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 16001, loss 0.1388, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 16501, loss 0.1190, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 17001, loss 0.0908, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 17501, loss 0.1306, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 18001, loss 0.0853, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 18501, loss 0.0800, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 19001, loss 0.1618, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 19501, loss 0.1844, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 20001, loss 0.1513, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 20501, loss 0.0997, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 21001, loss 0.1561, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 21501, loss 0.1083, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 22001, loss 0.1680, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 22501, loss 0.1156, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 23001, loss 0.1401, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 23501, loss 0.1614, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 24001, loss 0.0974, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 24501, loss 0.1516, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
running_time is 25.47814725
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.17
normal_0.5
./test_glass0/model_MLP_25000_0.17/record_1/MLP_25000_0.17_3
./test_glass0/result_MLP_25000_0.17_normal_0.5/record_1/
----------------------



the AUC is 0.7549261083743841

the Fscore is 0.6666666666666667

the precision is 0.5789473684210527

the recall is 0.7857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.17/record_1/MLP_25000_0.17_4
----------------------



epoch 1, loss 0.6270, train acc 67.84%, f1 0.1538, precision 0.5556, recall 0.0893, auc 0.5273
epoch 501, loss 0.3244, train acc 78.36%, f1 0.7338, precision 0.6145, recall 0.9107, auc 0.8162
epoch 1001, loss 0.4459, train acc 80.12%, f1 0.7536, precision 0.6341, recall 0.9286, auc 0.8339
epoch 1501, loss 0.3476, train acc 83.04%, f1 0.7852, precision 0.6709, recall 0.9464, auc 0.8602
epoch 2001, loss 0.2481, train acc 83.04%, f1 0.7852, precision 0.6709, recall 0.9464, auc 0.8602
epoch 2501, loss 0.2853, train acc 85.96%, f1 0.8154, precision 0.7162, recall 0.9464, auc 0.8819
epoch 3001, loss 0.3124, train acc 87.72%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.8995
epoch 3501, loss 0.2673, train acc 87.72%, f1 0.8320, precision 0.7536, recall 0.9286, auc 0.8904
epoch 4001, loss 0.2665, train acc 87.72%, f1 0.8346, precision 0.7465, recall 0.9464, auc 0.8950
epoch 4501, loss 0.1880, train acc 88.89%, f1 0.8455, precision 0.7761, recall 0.9286, auc 0.8991
epoch 5001, loss 0.2421, train acc 88.89%, f1 0.8455, precision 0.7761, recall 0.9286, auc 0.8991
epoch 5501, loss 0.1614, train acc 88.89%, f1 0.8480, precision 0.7681, recall 0.9464, auc 0.9036
epoch 6001, loss 0.1905, train acc 90.64%, f1 0.8689, precision 0.8030, recall 0.9464, auc 0.9167
epoch 6501, loss 0.2830, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 7001, loss 0.1231, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 7501, loss 0.2065, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 8001, loss 0.1315, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 8501, loss 0.1803, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 9001, loss 0.1461, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 9501, loss 0.1824, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 10001, loss 0.1415, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 10501, loss 0.2463, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 11001, loss 0.2072, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 11501, loss 0.2389, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 12001, loss 0.1736, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 12501, loss 0.2305, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 13001, loss 0.1969, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 13501, loss 0.1083, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 14001, loss 0.2139, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 14501, loss 0.1216, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 15001, loss 0.1731, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 15501, loss 0.1821, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 16001, loss 0.1989, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 16501, loss 0.1474, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 17001, loss 0.2419, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 17501, loss 0.1352, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 18001, loss 0.1887, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 18501, loss 0.1132, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 19001, loss 0.1427, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 19501, loss 0.1299, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 20001, loss 0.1650, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 20501, loss 0.1976, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 21001, loss 0.1560, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 21501, loss 0.1346, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 22001, loss 0.1014, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 22501, loss 0.1843, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 23001, loss 0.0893, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 23501, loss 0.1327, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 24001, loss 0.2037, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 24501, loss 0.1459, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
running_time is 25.501767944999997
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.17
normal_0.5
./test_glass0/model_MLP_25000_0.17/record_1/MLP_25000_0.17_4
./test_glass0/result_MLP_25000_0.17_normal_0.5/record_1/
----------------------



the AUC is 0.5726600985221675

the Fscore is 0.3157894736842105

the precision is 0.6

the recall is 0.21428571428571427

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.17/record_1/MLP_25000_0.17_5
----------------------



epoch 1, loss 0.6270, train acc 67.44%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.4049, train acc 77.91%, f1 0.7361, precision 0.6023, recall 0.9464, auc 0.8224
epoch 1001, loss 0.3672, train acc 80.23%, f1 0.7571, precision 0.6310, recall 0.9464, auc 0.8396
epoch 1501, loss 0.3367, train acc 82.56%, f1 0.7794, precision 0.6625, recall 0.9464, auc 0.8568
epoch 2001, loss 0.3253, train acc 84.30%, f1 0.7939, precision 0.6933, recall 0.9286, auc 0.8651
epoch 2501, loss 0.2144, train acc 86.05%, f1 0.8125, precision 0.7222, recall 0.9286, auc 0.8781
epoch 3001, loss 0.2911, train acc 87.21%, f1 0.8308, precision 0.7297, recall 0.9643, auc 0.8959
epoch 3501, loss 0.2279, train acc 87.21%, f1 0.8308, precision 0.7297, recall 0.9643, auc 0.8959
epoch 4001, loss 0.2263, train acc 87.79%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9049
epoch 4501, loss 0.1977, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 5001, loss 0.1959, train acc 90.12%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9221
epoch 5501, loss 0.2383, train acc 90.70%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9264
epoch 6001, loss 0.2013, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 6501, loss 0.1533, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 7001, loss 0.2201, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 7501, loss 0.2169, train acc 92.44%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9393
epoch 8001, loss 0.1483, train acc 92.44%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9393
epoch 8501, loss 0.2055, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 9001, loss 0.1458, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 9501, loss 0.1889, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 10001, loss 0.1728, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 10501, loss 0.2262, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 11001, loss 0.1938, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 11501, loss 0.1465, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 12001, loss 0.1070, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 12501, loss 0.2490, train acc 95.93%, f1 0.9391, precision 0.9153, recall 0.9643, auc 0.9606
epoch 13001, loss 0.1921, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 13501, loss 0.1247, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 14001, loss 0.1539, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 14501, loss 0.1450, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 15001, loss 0.1139, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 15501, loss 0.1210, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 16001, loss 0.0799, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 16501, loss 0.1137, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 17001, loss 0.1258, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 17501, loss 0.1222, train acc 96.51%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9695
epoch 18001, loss 0.1750, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 18501, loss 0.1709, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 19001, loss 0.0796, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 19501, loss 0.0881, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 20001, loss 0.1245, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 20501, loss 0.0615, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 21001, loss 0.1090, train acc 97.09%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9738
epoch 21501, loss 0.1007, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 22001, loss 0.0971, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 22501, loss 0.0792, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 23001, loss 0.1393, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 23501, loss 0.0690, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 24001, loss 0.0910, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 24501, loss 0.0854, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
running_time is 25.245230254
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.17
normal_0.5
./test_glass0/model_MLP_25000_0.17/record_1/MLP_25000_0.17_5
./test_glass0/result_MLP_25000_0.17_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.18/record_1/MLP_25000_0.18_1
----------------------



epoch 1, loss 0.6570, train acc 36.84%, f1 0.5000, precision 0.3375, recall 0.9643, auc 0.5213
epoch 501, loss 0.3747, train acc 79.53%, f1 0.7482, precision 0.6265, recall 0.9286, auc 0.8295
epoch 1001, loss 0.3771, train acc 80.70%, f1 0.7626, precision 0.6386, recall 0.9464, auc 0.8428
epoch 1501, loss 0.3029, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 2001, loss 0.2668, train acc 84.21%, f1 0.8000, precision 0.6835, recall 0.9643, auc 0.8734
epoch 2501, loss 0.2351, train acc 85.38%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8821
epoch 3001, loss 0.3026, train acc 85.38%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8821
epoch 3501, loss 0.2965, train acc 87.72%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9041
epoch 4001, loss 0.1578, train acc 88.30%, f1 0.8437, precision 0.7500, recall 0.9643, auc 0.9039
epoch 4501, loss 0.2053, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 5001, loss 0.2154, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 5501, loss 0.2371, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 6001, loss 0.2667, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 6501, loss 0.2292, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 7001, loss 0.1444, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 7501, loss 0.1289, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 8001, loss 0.1118, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 8501, loss 0.1122, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9001, loss 0.1422, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9501, loss 0.1280, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10001, loss 0.0906, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10501, loss 0.1777, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11001, loss 0.1737, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11501, loss 0.0745, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12001, loss 0.1572, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12501, loss 0.1325, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13001, loss 0.1260, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13501, loss 0.1308, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14001, loss 0.1196, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14501, loss 0.0698, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15001, loss 0.1021, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.1792, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.0841, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.0654, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.1430, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.1458, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.1112, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.0564, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.1280, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.1262, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20001, loss 0.0866, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20501, loss 0.1062, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21001, loss 0.0906, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21501, loss 0.1441, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22001, loss 0.0872, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22501, loss 0.1019, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23001, loss 0.1405, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23501, loss 0.1285, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24001, loss 0.0981, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24501, loss 0.0902, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 25.525200025
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.18
normal_0.5
./test_glass0/model_MLP_25000_0.18/record_1/MLP_25000_0.18_1
./test_glass0/result_MLP_25000_0.18_normal_0.5/record_1/
----------------------



the AUC is 0.5394088669950738

the Fscore is 0.3333333333333333

the precision is 0.4

the recall is 0.2857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.18/record_1/MLP_25000_0.18_2
----------------------



epoch 1, loss 0.5819, train acc 54.97%, f1 0.1348, precision 0.1818, recall 0.1071, auc 0.4362
epoch 501, loss 0.2846, train acc 80.70%, f1 0.7556, precision 0.6456, recall 0.9107, auc 0.8336
epoch 1001, loss 0.3402, train acc 83.63%, f1 0.7910, precision 0.6795, recall 0.9464, auc 0.8645
epoch 1501, loss 0.3064, train acc 85.96%, f1 0.8154, precision 0.7162, recall 0.9464, auc 0.8819
epoch 2001, loss 0.3629, train acc 87.13%, f1 0.8308, precision 0.7297, recall 0.9643, auc 0.8952
epoch 2501, loss 0.2162, train acc 88.30%, f1 0.8437, precision 0.7500, recall 0.9643, auc 0.9039
epoch 3001, loss 0.2653, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 3501, loss 0.1436, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 4001, loss 0.2474, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 4501, loss 0.2535, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 5001, loss 0.1721, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 5501, loss 0.1499, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 6001, loss 0.2052, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 6501, loss 0.2069, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 7001, loss 0.1832, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 7501, loss 0.2024, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8001, loss 0.1594, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8501, loss 0.1121, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 9001, loss 0.1406, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9501, loss 0.1413, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10001, loss 0.1605, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10501, loss 0.1455, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11001, loss 0.1850, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11501, loss 0.1042, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12001, loss 0.1350, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12501, loss 0.1223, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13001, loss 0.0758, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13501, loss 0.0874, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14001, loss 0.0941, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14501, loss 0.1423, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 15001, loss 0.1032, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 15501, loss 0.1082, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 16001, loss 0.1298, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 16501, loss 0.1301, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17001, loss 0.1822, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17501, loss 0.1102, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.1341, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.1350, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.1263, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.1298, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20001, loss 0.1084, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20501, loss 0.0843, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21001, loss 0.0761, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21501, loss 0.0615, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22001, loss 0.1133, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22501, loss 0.1065, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23001, loss 0.0766, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23501, loss 0.0900, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24001, loss 0.1480, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24501, loss 0.0740, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 25.511586236
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.18
normal_0.5
./test_glass0/model_MLP_25000_0.18/record_1/MLP_25000_0.18_2
./test_glass0/result_MLP_25000_0.18_normal_0.5/record_1/
----------------------



the AUC is 0.6280788177339902

the Fscore is 0.4799999999999999

the precision is 0.5454545454545454

the recall is 0.42857142857142855

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.18/record_1/MLP_25000_0.18_3
----------------------



epoch 1, loss 0.5817, train acc 65.50%, f1 0.0328, precision 0.2000, recall 0.0179, auc 0.4915
epoch 501, loss 0.3869, train acc 77.78%, f1 0.7246, precision 0.6098, recall 0.8929, auc 0.8073
epoch 1001, loss 0.3041, train acc 80.12%, f1 0.7500, precision 0.6375, recall 0.9107, auc 0.8293
epoch 1501, loss 0.2688, train acc 79.53%, f1 0.7445, precision 0.6296, recall 0.9107, auc 0.8249
epoch 2001, loss 0.3161, train acc 80.70%, f1 0.7591, precision 0.6420, recall 0.9286, auc 0.8382
epoch 2501, loss 0.2511, train acc 84.80%, f1 0.8030, precision 0.6974, recall 0.9464, auc 0.8732
epoch 3001, loss 0.2878, train acc 84.80%, f1 0.8030, precision 0.6974, recall 0.9464, auc 0.8732
epoch 3501, loss 0.2769, train acc 86.55%, f1 0.8244, precision 0.7200, recall 0.9643, auc 0.8908
epoch 4001, loss 0.2197, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 4501, loss 0.2201, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 5001, loss 0.1819, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 5501, loss 0.2098, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 6001, loss 0.1644, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 6501, loss 0.1574, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 7001, loss 0.1917, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 7501, loss 0.1931, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8001, loss 0.1564, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8501, loss 0.2022, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 9001, loss 0.1248, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9501, loss 0.1354, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10001, loss 0.1284, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 10501, loss 0.1439, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11001, loss 0.1296, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 11501, loss 0.1355, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12001, loss 0.0699, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12501, loss 0.0857, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 13001, loss 0.1119, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13501, loss 0.1005, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14001, loss 0.1256, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14501, loss 0.0761, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.0804, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.1153, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16001, loss 0.1066, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.1292, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 17001, loss 0.0782, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.0752, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.0799, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.0765, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.0985, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 19501, loss 0.0826, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 20001, loss 0.0776, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20501, loss 0.1145, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 21001, loss 0.1369, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 21501, loss 0.0853, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 22001, loss 0.0799, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 22501, loss 0.1355, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23001, loss 0.0784, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23501, loss 0.0935, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 24001, loss 0.1499, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 24501, loss 0.0688, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 25.382410907
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.18
normal_0.5
./test_glass0/model_MLP_25000_0.18/record_1/MLP_25000_0.18_3
./test_glass0/result_MLP_25000_0.18_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.18/record_1/MLP_25000_0.18_4
----------------------



epoch 1, loss 0.6570, train acc 38.60%, f1 0.5116, precision 0.3459, recall 0.9821, auc 0.5389
epoch 501, loss 0.3819, train acc 77.19%, f1 0.7273, precision 0.5977, recall 0.9286, auc 0.8121
epoch 1001, loss 0.2983, train acc 78.95%, f1 0.7429, precision 0.6190, recall 0.9286, auc 0.8252
epoch 1501, loss 0.3202, train acc 83.63%, f1 0.7910, precision 0.6795, recall 0.9464, auc 0.8645
epoch 2001, loss 0.2256, train acc 86.55%, f1 0.8217, precision 0.7260, recall 0.9464, auc 0.8863
epoch 2501, loss 0.3019, train acc 85.96%, f1 0.8125, precision 0.7222, recall 0.9286, auc 0.8773
epoch 3001, loss 0.2938, train acc 86.55%, f1 0.8217, precision 0.7260, recall 0.9464, auc 0.8863
epoch 3501, loss 0.2636, train acc 88.30%, f1 0.8437, precision 0.7500, recall 0.9643, auc 0.9039
epoch 4001, loss 0.2989, train acc 87.72%, f1 0.8346, precision 0.7465, recall 0.9464, auc 0.8950
epoch 4501, loss 0.1988, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 5001, loss 0.2567, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 5501, loss 0.1908, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 6001, loss 0.1153, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 6501, loss 0.2342, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 7001, loss 0.2007, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 7501, loss 0.2003, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 8001, loss 0.1471, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 8501, loss 0.2234, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 9001, loss 0.2024, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 9501, loss 0.1506, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 10001, loss 0.1199, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 10501, loss 0.1682, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 11001, loss 0.1599, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 11501, loss 0.2050, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 12001, loss 0.1074, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 12501, loss 0.0857, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 13001, loss 0.1107, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 13501, loss 0.1150, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14001, loss 0.2536, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14501, loss 0.2260, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15001, loss 0.2056, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15501, loss 0.1688, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16001, loss 0.1038, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16501, loss 0.1530, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 17001, loss 0.1153, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 17501, loss 0.0593, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 18001, loss 0.0977, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 18501, loss 0.1839, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19001, loss 0.1451, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19501, loss 0.1525, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 20001, loss 0.1019, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 20501, loss 0.1538, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 21001, loss 0.0692, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 21501, loss 0.1396, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 22001, loss 0.1246, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 22501, loss 0.0623, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 23001, loss 0.1321, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 23501, loss 0.1854, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 24001, loss 0.1092, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 24501, loss 0.1383, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
running_time is 25.504750955
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.18
normal_0.5
./test_glass0/model_MLP_25000_0.18/record_1/MLP_25000_0.18_4
./test_glass0/result_MLP_25000_0.18_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.18/record_1/MLP_25000_0.18_5
----------------------



epoch 1, loss 0.5966, train acc 65.12%, f1 0.2308, precision 0.4091, recall 0.1607, auc 0.5243
epoch 501, loss 0.4481, train acc 78.49%, f1 0.7376, precision 0.6118, recall 0.9286, auc 0.8220
epoch 1001, loss 0.2889, train acc 81.98%, f1 0.7704, precision 0.6582, recall 0.9286, auc 0.8479
epoch 1501, loss 0.3573, train acc 81.40%, f1 0.7647, precision 0.6500, recall 0.9286, auc 0.8436
epoch 2001, loss 0.3148, train acc 82.56%, f1 0.7761, precision 0.6667, recall 0.9286, auc 0.8522
epoch 2501, loss 0.3220, train acc 83.14%, f1 0.7820, precision 0.6753, recall 0.9286, auc 0.8565
epoch 3001, loss 0.3398, train acc 83.72%, f1 0.7910, precision 0.6795, recall 0.9464, auc 0.8655
epoch 3501, loss 0.2737, train acc 84.30%, f1 0.8000, precision 0.6835, recall 0.9643, auc 0.8744
epoch 4001, loss 0.3326, train acc 86.63%, f1 0.8217, precision 0.7260, recall 0.9464, auc 0.8870
epoch 4501, loss 0.2076, train acc 87.21%, f1 0.8308, precision 0.7297, recall 0.9643, auc 0.8959
epoch 5001, loss 0.3084, train acc 88.95%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9135
epoch 5501, loss 0.3380, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 6001, loss 0.2332, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 6501, loss 0.1966, train acc 90.12%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9221
epoch 7001, loss 0.2087, train acc 91.28%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9261
epoch 7501, loss 0.2348, train acc 91.28%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9261
epoch 8001, loss 0.1326, train acc 92.44%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9393
epoch 8501, loss 0.2016, train acc 91.86%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9304
epoch 9001, loss 0.2388, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 9501, loss 0.2172, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 10001, loss 0.1442, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 10501, loss 0.2149, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 11001, loss 0.2485, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 11501, loss 0.1529, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 12001, loss 0.1576, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 12501, loss 0.1365, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 13001, loss 0.2165, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 13501, loss 0.1918, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 14001, loss 0.1761, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 14501, loss 0.2515, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 15001, loss 0.1670, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 15501, loss 0.1564, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 16001, loss 0.1579, train acc 93.02%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9390
epoch 16501, loss 0.0848, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 17001, loss 0.1765, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 17501, loss 0.1422, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 18001, loss 0.1455, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 18501, loss 0.2281, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 19001, loss 0.1152, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 19501, loss 0.1701, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 20001, loss 0.1267, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 20501, loss 0.1198, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 21001, loss 0.1530, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 21501, loss 0.1641, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 22001, loss 0.1317, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 22501, loss 0.1333, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 23001, loss 0.1801, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 23501, loss 0.1306, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 24001, loss 0.1630, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 24501, loss 0.0913, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
running_time is 25.458045693
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.18
normal_0.5
./test_glass0/model_MLP_25000_0.18/record_1/MLP_25000_0.18_5
./test_glass0/result_MLP_25000_0.18_normal_0.5/record_1/
----------------------



the AUC is 0.6785714285714285

the Fscore is 0.5714285714285714

the precision is 0.5714285714285714

the recall is 0.5714285714285714

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.19/record_1/MLP_25000_0.19_1
----------------------



epoch 1, loss 0.6432, train acc 33.33%, f1 0.4956, precision 0.3294, recall 1.0000, auc 0.5043
epoch 501, loss 0.3581, train acc 80.12%, f1 0.7536, precision 0.6341, recall 0.9286, auc 0.8339
epoch 1001, loss 0.3578, train acc 80.70%, f1 0.7626, precision 0.6386, recall 0.9464, auc 0.8428
epoch 1501, loss 0.2954, train acc 80.70%, f1 0.7591, precision 0.6420, recall 0.9286, auc 0.8382
epoch 2001, loss 0.2798, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 2501, loss 0.2472, train acc 84.80%, f1 0.8060, precision 0.6923, recall 0.9643, auc 0.8778
epoch 3001, loss 0.3061, train acc 85.38%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8821
epoch 3501, loss 0.2519, train acc 87.13%, f1 0.8308, precision 0.7297, recall 0.9643, auc 0.8952
epoch 4001, loss 0.3435, train acc 87.72%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9041
epoch 4501, loss 0.2785, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 5001, loss 0.2375, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 5501, loss 0.2712, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 6001, loss 0.1970, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 6501, loss 0.2228, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 7001, loss 0.1522, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 7501, loss 0.1608, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 8001, loss 0.2193, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 8501, loss 0.1364, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9001, loss 0.1106, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9501, loss 0.1671, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10001, loss 0.1204, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10501, loss 0.1665, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 11001, loss 0.1131, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11501, loss 0.0937, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12001, loss 0.1563, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12501, loss 0.1511, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13001, loss 0.0822, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13501, loss 0.0926, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14001, loss 0.1426, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14501, loss 0.1173, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.1311, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15501, loss 0.1306, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.0846, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.0753, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.1412, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.1920, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.1489, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.1361, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.1175, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.0941, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20001, loss 0.0803, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20501, loss 0.1061, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21001, loss 0.1057, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21501, loss 0.0725, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22001, loss 0.0976, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22501, loss 0.1257, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23001, loss 0.0938, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23501, loss 0.1795, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24001, loss 0.0720, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24501, loss 0.1187, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 25.39324486
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.19
normal_0.5
./test_glass0/model_MLP_25000_0.19/record_1/MLP_25000_0.19_1
./test_glass0/result_MLP_25000_0.19_normal_0.5/record_1/
----------------------



the AUC is 0.5800492610837438

the Fscore is 0.4864864864864865

the precision is 0.391304347826087

the recall is 0.6428571428571429

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.19/record_1/MLP_25000_0.19_2
----------------------



epoch 1, loss 0.6432, train acc 35.09%, f1 0.4977, precision 0.3333, recall 0.9821, auc 0.5128
epoch 501, loss 0.4037, train acc 80.12%, f1 0.7571, precision 0.6310, recall 0.9464, auc 0.8384
epoch 1001, loss 0.3274, train acc 83.04%, f1 0.7852, precision 0.6709, recall 0.9464, auc 0.8602
epoch 1501, loss 0.2171, train acc 84.80%, f1 0.8060, precision 0.6923, recall 0.9643, auc 0.8778
epoch 2001, loss 0.2589, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 2501, loss 0.2251, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 3001, loss 0.2345, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 3501, loss 0.1678, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 4001, loss 0.1815, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 4501, loss 0.1813, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 5001, loss 0.1574, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 5501, loss 0.1134, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 6001, loss 0.1426, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 6501, loss 0.1729, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 7001, loss 0.1450, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 7501, loss 0.1169, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8001, loss 0.1126, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 8501, loss 0.1297, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 9001, loss 0.0974, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 9501, loss 0.0849, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10001, loss 0.1592, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 10501, loss 0.1786, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 11001, loss 0.1637, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 11501, loss 0.1387, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12001, loss 0.1038, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12501, loss 0.0902, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13001, loss 0.1422, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 13501, loss 0.0868, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14001, loss 0.1177, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.0897, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15001, loss 0.1282, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15501, loss 0.1449, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 16001, loss 0.1068, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.1445, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17001, loss 0.2031, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.0872, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18001, loss 0.1174, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.0878, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19001, loss 0.0789, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 19501, loss 0.1367, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 20001, loss 0.1072, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20501, loss 0.1190, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21001, loss 0.1230, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 21501, loss 0.0805, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22001, loss 0.0917, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22501, loss 0.1229, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23001, loss 0.0685, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 23501, loss 0.0698, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24001, loss 0.1210, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24501, loss 0.0842, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 25.420606565
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.19
normal_0.5
./test_glass0/model_MLP_25000_0.19/record_1/MLP_25000_0.19_2
./test_glass0/result_MLP_25000_0.19_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.19/record_1/MLP_25000_0.19_3
----------------------



epoch 1, loss 0.5471, train acc 65.50%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4870
epoch 501, loss 0.3935, train acc 77.19%, f1 0.7273, precision 0.5977, recall 0.9286, auc 0.8121
epoch 1001, loss 0.3826, train acc 78.95%, f1 0.7391, precision 0.6220, recall 0.9107, auc 0.8206
epoch 1501, loss 0.2643, train acc 82.46%, f1 0.7727, precision 0.6711, recall 0.9107, auc 0.8467
epoch 2001, loss 0.3838, train acc 86.55%, f1 0.8217, precision 0.7260, recall 0.9464, auc 0.8863
epoch 2501, loss 0.3255, train acc 85.96%, f1 0.8154, precision 0.7162, recall 0.9464, auc 0.8819
epoch 3001, loss 0.2925, train acc 87.72%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.8995
epoch 3501, loss 0.2294, train acc 88.30%, f1 0.8413, precision 0.7571, recall 0.9464, auc 0.8993
epoch 4001, loss 0.2021, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 4501, loss 0.2430, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 5001, loss 0.0979, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 5501, loss 0.1550, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 6001, loss 0.1617, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 6501, loss 0.1682, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 7001, loss 0.1067, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 7501, loss 0.1990, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 8001, loss 0.1711, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 8501, loss 0.1834, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9001, loss 0.1985, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9501, loss 0.1214, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 10001, loss 0.0988, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10501, loss 0.1115, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 11001, loss 0.1889, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11501, loss 0.1368, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12001, loss 0.1335, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12501, loss 0.1280, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13001, loss 0.1579, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13501, loss 0.1660, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14001, loss 0.1524, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14501, loss 0.1372, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 15001, loss 0.1053, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15501, loss 0.1302, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 16001, loss 0.1181, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16501, loss 0.1421, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17001, loss 0.1065, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17501, loss 0.1252, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 18001, loss 0.1473, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.1209, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 19001, loss 0.1356, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19501, loss 0.0969, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20001, loss 0.1000, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20501, loss 0.1345, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21001, loss 0.1517, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21501, loss 0.1260, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22001, loss 0.0975, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 22501, loss 0.0744, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 23001, loss 0.0944, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23501, loss 0.1393, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24001, loss 0.0917, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24501, loss 0.1288, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 25.317241205000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.19
normal_0.5
./test_glass0/model_MLP_25000_0.19/record_1/MLP_25000_0.19_3
./test_glass0/result_MLP_25000_0.19_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.19/record_1/MLP_25000_0.19_4
----------------------



epoch 1, loss 0.5631, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.3815, train acc 77.78%, f1 0.7361, precision 0.6023, recall 0.9464, auc 0.8210
epoch 1001, loss 0.3158, train acc 81.87%, f1 0.7737, precision 0.6543, recall 0.9464, auc 0.8515
epoch 1501, loss 0.3568, train acc 84.21%, f1 0.7970, precision 0.6883, recall 0.9464, auc 0.8689
epoch 2001, loss 0.3286, train acc 85.96%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8865
epoch 2501, loss 0.2067, train acc 87.13%, f1 0.8308, precision 0.7297, recall 0.9643, auc 0.8952
epoch 3001, loss 0.2767, train acc 87.13%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.8998
epoch 3501, loss 0.2573, train acc 85.96%, f1 0.8125, precision 0.7222, recall 0.9286, auc 0.8773
epoch 4001, loss 0.2533, train acc 87.72%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.8995
epoch 4501, loss 0.2140, train acc 88.30%, f1 0.8437, precision 0.7500, recall 0.9643, auc 0.9039
epoch 5001, loss 0.1791, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 5501, loss 0.2648, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 6001, loss 0.2851, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 6501, loss 0.2391, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 7001, loss 0.1925, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 7501, loss 0.1412, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 8001, loss 0.1652, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 8501, loss 0.1698, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 9001, loss 0.1545, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 9501, loss 0.1667, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 10001, loss 0.1580, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 10501, loss 0.1925, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 11001, loss 0.2127, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 11501, loss 0.1924, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 12001, loss 0.2158, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 12501, loss 0.1114, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 13001, loss 0.1902, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 13501, loss 0.1663, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 14001, loss 0.1066, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14501, loss 0.1994, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 15001, loss 0.1883, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15501, loss 0.1320, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16001, loss 0.1769, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 16501, loss 0.1017, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 17001, loss 0.1560, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 17501, loss 0.1503, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 18001, loss 0.1781, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 18501, loss 0.1187, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 19001, loss 0.1370, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 19501, loss 0.1576, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 20001, loss 0.1535, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 20501, loss 0.1488, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 21001, loss 0.1965, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 21501, loss 0.1590, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 22001, loss 0.2080, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 22501, loss 0.1502, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 23001, loss 0.1036, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 23501, loss 0.0955, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 24001, loss 0.1282, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 24501, loss 0.1259, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
running_time is 25.57929222
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.19
normal_0.5
./test_glass0/model_MLP_25000_0.19/record_1/MLP_25000_0.19_4
./test_glass0/result_MLP_25000_0.19_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.19/record_1/MLP_25000_0.19_5
----------------------



epoch 1, loss 0.6911, train acc 43.02%, f1 0.5333, precision 0.3636, recall 1.0000, auc 0.5776
epoch 501, loss 0.3768, train acc 79.07%, f1 0.7429, precision 0.6190, recall 0.9286, auc 0.8264
epoch 1001, loss 0.3490, train acc 81.40%, f1 0.7647, precision 0.6500, recall 0.9286, auc 0.8436
epoch 1501, loss 0.2814, train acc 80.81%, f1 0.7626, precision 0.6386, recall 0.9464, auc 0.8439
epoch 2001, loss 0.2773, train acc 83.14%, f1 0.7852, precision 0.6709, recall 0.9464, auc 0.8611
epoch 2501, loss 0.3359, train acc 84.88%, f1 0.8030, precision 0.6974, recall 0.9464, auc 0.8741
epoch 3001, loss 0.4027, train acc 85.47%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8830
epoch 3501, loss 0.3127, train acc 86.63%, f1 0.8244, precision 0.7200, recall 0.9643, auc 0.8916
epoch 4001, loss 0.3265, train acc 87.21%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.9006
epoch 4501, loss 0.2715, train acc 87.79%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.9002
epoch 5001, loss 0.1770, train acc 88.37%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9092
epoch 5501, loss 0.2636, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 6001, loss 0.2114, train acc 90.70%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9264
epoch 6501, loss 0.2252, train acc 90.70%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9264
epoch 7001, loss 0.1402, train acc 91.28%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9307
epoch 7501, loss 0.1881, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 8001, loss 0.2584, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 8501, loss 0.1749, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 9001, loss 0.1672, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 9501, loss 0.1668, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 10001, loss 0.1256, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 10501, loss 0.2200, train acc 93.60%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9433
epoch 11001, loss 0.1718, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 11501, loss 0.1833, train acc 92.44%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9393
epoch 12001, loss 0.1630, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 12501, loss 0.1742, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 13001, loss 0.1452, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 13501, loss 0.1441, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 14001, loss 0.0973, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 14501, loss 0.1136, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 15001, loss 0.1540, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 15501, loss 0.1368, train acc 95.35%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9609
epoch 16001, loss 0.1783, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 16501, loss 0.1567, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 17001, loss 0.1409, train acc 95.93%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9652
epoch 17501, loss 0.1319, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 18001, loss 0.1881, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 18501, loss 0.2202, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 19001, loss 0.1499, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 19501, loss 0.1426, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 20001, loss 0.1187, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 20501, loss 0.0938, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 21001, loss 0.1392, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 21501, loss 0.1262, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 22001, loss 0.1313, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 22501, loss 0.1198, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 23001, loss 0.1456, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 23501, loss 0.1039, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 24001, loss 0.1249, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 24501, loss 0.1184, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
running_time is 25.451373533
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.19
normal_0.5
./test_glass0/model_MLP_25000_0.19/record_1/MLP_25000_0.19_5
./test_glass0/result_MLP_25000_0.19_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.2/record_1/MLP_25000_0.2_1
----------------------



epoch 1, loss 0.5771, train acc 56.73%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4217
epoch 501, loss 0.3649, train acc 80.12%, f1 0.7536, precision 0.6341, recall 0.9286, auc 0.8339
epoch 1001, loss 0.4047, train acc 81.29%, f1 0.7681, precision 0.6463, recall 0.9464, auc 0.8471
epoch 1501, loss 0.4221, train acc 83.63%, f1 0.7910, precision 0.6795, recall 0.9464, auc 0.8645
epoch 2001, loss 0.3241, train acc 84.21%, f1 0.7970, precision 0.6883, recall 0.9464, auc 0.8689
epoch 2501, loss 0.2981, train acc 85.96%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8865
epoch 3001, loss 0.3384, train acc 85.96%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8865
epoch 3501, loss 0.2857, train acc 87.13%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.8998
epoch 4001, loss 0.3058, train acc 87.13%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.8998
epoch 4501, loss 0.2939, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 5001, loss 0.2198, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 5501, loss 0.1505, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 6001, loss 0.2066, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 6501, loss 0.2792, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 7001, loss 0.1933, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 7501, loss 0.1709, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8001, loss 0.1545, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 8501, loss 0.2725, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 9001, loss 0.1222, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9501, loss 0.1607, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10001, loss 0.1215, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10501, loss 0.1604, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 11001, loss 0.1436, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11501, loss 0.1740, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12001, loss 0.1343, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12501, loss 0.0700, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13001, loss 0.1197, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13501, loss 0.1563, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14001, loss 0.1311, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14501, loss 0.2122, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 15001, loss 0.0994, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15501, loss 0.1663, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16001, loss 0.0665, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.0600, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17001, loss 0.1299, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17501, loss 0.0769, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18001, loss 0.1752, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.0776, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19001, loss 0.1367, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19501, loss 0.1827, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20001, loss 0.1148, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20501, loss 0.1179, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21001, loss 0.1282, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21501, loss 0.1726, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 22001, loss 0.0773, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 22501, loss 0.1685, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 23001, loss 0.0861, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 23501, loss 0.0952, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 24001, loss 0.1453, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 24501, loss 0.0762, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
running_time is 25.358644442
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.2
normal_0.5
./test_glass0/model_MLP_25000_0.2/record_1/MLP_25000_0.2_1
./test_glass0/result_MLP_25000_0.2_normal_0.5/record_1/
----------------------



the AUC is 0.5061576354679803

the Fscore is 0.3448275862068965

the precision is 0.3333333333333333

the recall is 0.35714285714285715

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.2/record_1/MLP_25000_0.2_2
----------------------



epoch 1, loss 0.6618, train acc 44.44%, f1 0.5411, precision 0.3709, recall 1.0000, auc 0.5870
epoch 501, loss 0.4810, train acc 81.29%, f1 0.7681, precision 0.6463, recall 0.9464, auc 0.8471
epoch 1001, loss 0.3729, train acc 81.29%, f1 0.7681, precision 0.6463, recall 0.9464, auc 0.8471
epoch 1501, loss 0.3300, train acc 84.21%, f1 0.8000, precision 0.6835, recall 0.9643, auc 0.8734
epoch 2001, loss 0.2906, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 2501, loss 0.2530, train acc 87.72%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9041
epoch 3001, loss 0.2782, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 3501, loss 0.2696, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 4001, loss 0.2498, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 4501, loss 0.1663, train acc 92.40%, f1 0.8926, precision 0.8308, recall 0.9643, auc 0.9343
epoch 5001, loss 0.1946, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 5501, loss 0.1116, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 6001, loss 0.1476, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 6501, loss 0.1657, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 7001, loss 0.1118, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 7501, loss 0.1230, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8001, loss 0.1388, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 8501, loss 0.1558, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9001, loss 0.1355, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 9501, loss 0.0991, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 10001, loss 0.1879, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 10501, loss 0.1491, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 11001, loss 0.1440, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 11501, loss 0.1282, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 12001, loss 0.1074, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 12501, loss 0.0928, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 13001, loss 0.0941, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13501, loss 0.1144, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 14001, loss 0.1057, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 14501, loss 0.1363, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.0917, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15501, loss 0.1050, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 16001, loss 0.0937, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 16501, loss 0.1281, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.0955, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.0980, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.1323, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 18501, loss 0.1063, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.1385, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.1155, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20001, loss 0.0916, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20501, loss 0.1078, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21001, loss 0.1256, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21501, loss 0.0967, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22001, loss 0.0968, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22501, loss 0.0866, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23001, loss 0.0666, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23501, loss 0.1491, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24001, loss 0.1112, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24501, loss 0.1267, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 25.38201456
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.2
normal_0.5
./test_glass0/model_MLP_25000_0.2/record_1/MLP_25000_0.2_2
./test_glass0/result_MLP_25000_0.2_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.2/record_1/MLP_25000_0.2_3
----------------------



epoch 1, loss 0.6110, train acc 61.99%, f1 0.0299, precision 0.0909, recall 0.0179, auc 0.4655
epoch 501, loss 0.3826, train acc 71.93%, f1 0.6800, precision 0.5426, recall 0.9107, auc 0.7684
epoch 1001, loss 0.2727, train acc 78.36%, f1 0.7376, precision 0.6118, recall 0.9286, auc 0.8208
epoch 1501, loss 0.3283, train acc 81.29%, f1 0.7612, precision 0.6538, recall 0.9107, auc 0.8380
epoch 2001, loss 0.2786, train acc 84.21%, f1 0.7970, precision 0.6883, recall 0.9464, auc 0.8689
epoch 2501, loss 0.2509, train acc 87.13%, f1 0.8281, precision 0.7361, recall 0.9464, auc 0.8906
epoch 3001, loss 0.3312, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 3501, loss 0.2102, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 4001, loss 0.2538, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 4501, loss 0.2718, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 5001, loss 0.2330, train acc 91.81%, f1 0.8833, precision 0.8281, recall 0.9464, auc 0.9254
epoch 5501, loss 0.2492, train acc 92.40%, f1 0.8908, precision 0.8413, recall 0.9464, auc 0.9297
epoch 6001, loss 0.2535, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 6501, loss 0.2403, train acc 92.98%, f1 0.9000, precision 0.8438, recall 0.9643, auc 0.9387
epoch 7001, loss 0.1446, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 7501, loss 0.2273, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 8001, loss 0.1246, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 8501, loss 0.1262, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 9001, loss 0.2570, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 9501, loss 0.2009, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 10001, loss 0.2145, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 10501, loss 0.1499, train acc 94.15%, f1 0.9153, precision 0.8710, recall 0.9643, auc 0.9474
epoch 11001, loss 0.1464, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 11501, loss 0.1304, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 12001, loss 0.1126, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 12501, loss 0.1462, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 13001, loss 0.1694, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 13501, loss 0.1442, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 14001, loss 0.1377, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 14501, loss 0.1529, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 15001, loss 0.0872, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15501, loss 0.1981, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16001, loss 0.1322, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 16501, loss 0.1036, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17001, loss 0.1464, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17501, loss 0.0751, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18001, loss 0.2419, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 18501, loss 0.1459, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 19001, loss 0.1467, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19501, loss 0.1962, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20001, loss 0.0505, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20501, loss 0.1297, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21001, loss 0.1083, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21501, loss 0.0873, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 22001, loss 0.1462, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22501, loss 0.1012, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23001, loss 0.1108, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23501, loss 0.1780, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24001, loss 0.1095, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24501, loss 0.0971, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 25.397702545
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.2
normal_0.5
./test_glass0/model_MLP_25000_0.2/record_1/MLP_25000_0.2_3
./test_glass0/result_MLP_25000_0.2_normal_0.5/record_1/
----------------------



the AUC is 0.7339901477832512

the Fscore is 0.64

the precision is 0.7272727272727273

the recall is 0.5714285714285714

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.2/record_1/MLP_25000_0.2_4
----------------------



epoch 1, loss 0.6617, train acc 39.77%, f1 0.5164, precision 0.3503, recall 0.9821, auc 0.5476
epoch 501, loss 0.4195, train acc 74.85%, f1 0.7114, precision 0.5699, recall 0.9464, auc 0.7993
epoch 1001, loss 0.4214, train acc 80.12%, f1 0.7571, precision 0.6310, recall 0.9464, auc 0.8384
epoch 1501, loss 0.3410, train acc 80.12%, f1 0.7571, precision 0.6310, recall 0.9464, auc 0.8384
epoch 2001, loss 0.3167, train acc 81.87%, f1 0.7737, precision 0.6543, recall 0.9464, auc 0.8515
epoch 2501, loss 0.2722, train acc 85.96%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8865
epoch 3001, loss 0.2367, train acc 85.96%, f1 0.8154, precision 0.7162, recall 0.9464, auc 0.8819
epoch 3501, loss 0.2060, train acc 89.47%, f1 0.8548, precision 0.7794, recall 0.9464, auc 0.9080
epoch 4001, loss 0.2680, train acc 88.89%, f1 0.8480, precision 0.7681, recall 0.9464, auc 0.9036
epoch 4501, loss 0.2074, train acc 90.64%, f1 0.8689, precision 0.8030, recall 0.9464, auc 0.9167
epoch 5001, loss 0.2102, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 5501, loss 0.2212, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 6001, loss 0.2199, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 6501, loss 0.2373, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 7001, loss 0.1644, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 7501, loss 0.2275, train acc 93.57%, f1 0.9076, precision 0.8571, recall 0.9643, auc 0.9430
epoch 8001, loss 0.1500, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 8501, loss 0.2875, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 9001, loss 0.1966, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 9501, loss 0.1761, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 10001, loss 0.1751, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 10501, loss 0.2135, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 11001, loss 0.1287, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 11501, loss 0.1412, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 12001, loss 0.1109, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 12501, loss 0.1763, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 13001, loss 0.1959, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 13501, loss 0.0956, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 14001, loss 0.1685, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 14501, loss 0.1243, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 15001, loss 0.1732, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 15501, loss 0.1315, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 16001, loss 0.1381, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 16501, loss 0.1440, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 17001, loss 0.1218, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 17501, loss 0.1253, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 18001, loss 0.1205, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 18501, loss 0.1974, train acc 95.32%, f1 0.9310, precision 0.9000, recall 0.9643, auc 0.9561
epoch 19001, loss 0.1312, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19501, loss 0.1627, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 20001, loss 0.1492, train acc 94.74%, f1 0.9231, precision 0.8852, recall 0.9643, auc 0.9517
epoch 20501, loss 0.1357, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 21001, loss 0.1236, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 21501, loss 0.1632, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 22001, loss 0.1186, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 22501, loss 0.1921, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 23001, loss 0.1762, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 23501, loss 0.1879, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 24001, loss 0.1973, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 24501, loss 0.0998, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
running_time is 25.466709947
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.2
normal_0.5
./test_glass0/model_MLP_25000_0.2/record_1/MLP_25000_0.2_4
./test_glass0/result_MLP_25000_0.2_normal_0.5/record_1/
----------------------



the AUC is 0.5369458128078817

the Fscore is 0.22222222222222224

the precision is 0.5

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.2/record_1/MLP_25000_0.2_5
----------------------



epoch 1, loss 0.6448, train acc 34.30%, f1 0.4978, precision 0.3314, recall 1.0000, auc 0.5129
epoch 501, loss 0.4325, train acc 78.49%, f1 0.7448, precision 0.6067, recall 0.9643, auc 0.8313
epoch 1001, loss 0.3491, train acc 81.98%, f1 0.7770, precision 0.6506, recall 0.9643, auc 0.8571
epoch 1501, loss 0.3315, train acc 81.98%, f1 0.7737, precision 0.6543, recall 0.9464, auc 0.8525
epoch 2001, loss 0.2631, train acc 83.72%, f1 0.7910, precision 0.6795, recall 0.9464, auc 0.8655
epoch 2501, loss 0.2318, train acc 84.88%, f1 0.8030, precision 0.6974, recall 0.9464, auc 0.8741
epoch 3001, loss 0.2827, train acc 86.05%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8873
epoch 3501, loss 0.3240, train acc 87.79%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.9002
epoch 4001, loss 0.2810, train acc 87.79%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9049
epoch 4501, loss 0.2787, train acc 88.95%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9135
epoch 5001, loss 0.2413, train acc 88.95%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9135
epoch 5501, loss 0.2676, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 6001, loss 0.2116, train acc 90.12%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9221
epoch 6501, loss 0.2368, train acc 91.28%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9307
epoch 7001, loss 0.1946, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 7501, loss 0.2132, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 8001, loss 0.1699, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 8501, loss 0.1565, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 9001, loss 0.1826, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 9501, loss 0.1796, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 10001, loss 0.1436, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 10501, loss 0.1428, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 11001, loss 0.1498, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 11501, loss 0.1410, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 12001, loss 0.1541, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 12501, loss 0.1868, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 13001, loss 0.1806, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 13501, loss 0.1421, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 14001, loss 0.1305, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 14501, loss 0.1447, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 15001, loss 0.1477, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 15501, loss 0.0985, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 16001, loss 0.1367, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 16501, loss 0.1530, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 17001, loss 0.1224, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 17501, loss 0.1297, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 18001, loss 0.1171, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 18501, loss 0.1184, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 19001, loss 0.1002, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 19501, loss 0.0731, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 20001, loss 0.0637, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 20501, loss 0.1126, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 21001, loss 0.1278, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 21501, loss 0.0588, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 22001, loss 0.1187, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 22501, loss 0.1438, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 23001, loss 0.1232, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 23501, loss 0.1289, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 24001, loss 0.1184, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 24501, loss 0.0774, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
running_time is 25.439377985
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.2
normal_0.5
./test_glass0/model_MLP_25000_0.2/record_1/MLP_25000_0.2_5
./test_glass0/result_MLP_25000_0.2_normal_0.5/record_1/
----------------------



the AUC is 0.5892857142857143

the Fscore is 0.3333333333333333

the precision is 0.75

the recall is 0.21428571428571427

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.21/record_1/MLP_25000_0.21_1
----------------------



epoch 1, loss 0.6649, train acc 46.78%, f1 0.5081, precision 0.3643, recall 0.8393, auc 0.5631
epoch 501, loss 0.3998, train acc 78.95%, f1 0.7465, precision 0.6163, recall 0.9464, auc 0.8297
epoch 1001, loss 0.3265, train acc 81.29%, f1 0.7714, precision 0.6429, recall 0.9643, auc 0.8517
epoch 1501, loss 0.2810, train acc 83.04%, f1 0.7883, precision 0.6667, recall 0.9643, auc 0.8648
epoch 2001, loss 0.3655, train acc 84.21%, f1 0.8000, precision 0.6835, recall 0.9643, auc 0.8734
epoch 2501, loss 0.2727, train acc 84.21%, f1 0.7970, precision 0.6883, recall 0.9464, auc 0.8689
epoch 3001, loss 0.1918, train acc 84.80%, f1 0.8060, precision 0.6923, recall 0.9643, auc 0.8778
epoch 3501, loss 0.2971, train acc 86.55%, f1 0.8244, precision 0.7200, recall 0.9643, auc 0.8908
epoch 4001, loss 0.1850, train acc 87.72%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.8995
epoch 4501, loss 0.2571, train acc 87.72%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.8995
epoch 5001, loss 0.1589, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 5501, loss 0.2553, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 6001, loss 0.2937, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 6501, loss 0.2565, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 7001, loss 0.2050, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 7501, loss 0.1886, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 8001, loss 0.1786, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 8501, loss 0.2327, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 9001, loss 0.2711, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9501, loss 0.2354, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 10001, loss 0.2861, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 10501, loss 0.1869, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 11001, loss 0.1963, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 11501, loss 0.1524, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 12001, loss 0.1649, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 12501, loss 0.1097, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 13001, loss 0.1324, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 13501, loss 0.1195, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 14001, loss 0.3333, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 14501, loss 0.1690, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 15001, loss 0.1912, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 15501, loss 0.1896, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 16001, loss 0.1748, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 16501, loss 0.0982, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 17001, loss 0.1959, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 17501, loss 0.1583, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 18001, loss 0.1918, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 18501, loss 0.1355, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 19001, loss 0.1588, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 19501, loss 0.1490, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 20001, loss 0.0787, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 20501, loss 0.1763, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 21001, loss 0.2146, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 21501, loss 0.1711, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 22001, loss 0.0939, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 22501, loss 0.1059, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 23001, loss 0.1936, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 23501, loss 0.0793, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 24001, loss 0.2198, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 24501, loss 0.2194, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
running_time is 25.261065441
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.21
normal_0.5
./test_glass0/model_MLP_25000_0.21/record_1/MLP_25000_0.21_1
./test_glass0/result_MLP_25000_0.21_normal_0.5/record_1/
----------------------



the AUC is 0.5209359605911329

the Fscore is 0.2727272727272727

the precision is 0.375

the recall is 0.21428571428571427

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.21/record_1/MLP_25000_0.21_2
----------------------



epoch 1, loss 0.5574, train acc 63.16%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.4696
epoch 501, loss 0.4203, train acc 78.95%, f1 0.7429, precision 0.6190, recall 0.9286, auc 0.8252
epoch 1001, loss 0.3278, train acc 83.04%, f1 0.7852, precision 0.6709, recall 0.9464, auc 0.8602
epoch 1501, loss 0.4268, train acc 85.96%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8865
epoch 2001, loss 0.2263, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 2501, loss 0.1838, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 3001, loss 0.2744, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 3501, loss 0.2588, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 4001, loss 0.1684, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 4501, loss 0.2117, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 5001, loss 0.2332, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 5501, loss 0.2330, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 6001, loss 0.1940, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 6501, loss 0.1512, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 7001, loss 0.2294, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 7501, loss 0.1147, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 8001, loss 0.1492, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 8501, loss 0.1622, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9001, loss 0.1583, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9501, loss 0.2182, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10001, loss 0.1799, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10501, loss 0.1140, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11001, loss 0.1125, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11501, loss 0.1614, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12001, loss 0.1639, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12501, loss 0.1420, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13001, loss 0.1191, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13501, loss 0.1765, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14001, loss 0.1318, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14501, loss 0.1482, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.0676, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15501, loss 0.1957, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16001, loss 0.1383, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.1205, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17001, loss 0.1547, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17501, loss 0.1950, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18001, loss 0.0981, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.0985, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19001, loss 0.1058, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19501, loss 0.1128, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20001, loss 0.1394, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20501, loss 0.0764, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21001, loss 0.0768, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21501, loss 0.0718, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22001, loss 0.0873, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22501, loss 0.1339, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23001, loss 0.1074, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23501, loss 0.0847, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24001, loss 0.0783, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24501, loss 0.0678, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 25.464211441
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.21
normal_0.5
./test_glass0/model_MLP_25000_0.21/record_1/MLP_25000_0.21_2
./test_glass0/result_MLP_25000_0.21_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.21/record_1/MLP_25000_0.21_3
----------------------



epoch 1, loss 0.6110, train acc 42.69%, f1 0.5333, precision 0.3636, recall 1.0000, auc 0.5739
epoch 501, loss 0.4733, train acc 76.02%, f1 0.7172, precision 0.5843, recall 0.9286, auc 0.8034
epoch 1001, loss 0.4333, train acc 79.53%, f1 0.7445, precision 0.6296, recall 0.9107, auc 0.8249
epoch 1501, loss 0.3738, train acc 79.53%, f1 0.7482, precision 0.6265, recall 0.9286, auc 0.8295
epoch 2001, loss 0.3187, train acc 83.63%, f1 0.7879, precision 0.6842, recall 0.9286, auc 0.8599
epoch 2501, loss 0.2275, train acc 86.55%, f1 0.8244, precision 0.7200, recall 0.9643, auc 0.8908
epoch 3001, loss 0.3491, train acc 87.72%, f1 0.8372, precision 0.7397, recall 0.9643, auc 0.8995
epoch 3501, loss 0.2265, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 4001, loss 0.2317, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 4501, loss 0.2139, train acc 91.81%, f1 0.8852, precision 0.8182, recall 0.9643, auc 0.9300
epoch 5001, loss 0.1325, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 5501, loss 0.1584, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 6001, loss 0.1597, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 6501, loss 0.1952, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 7001, loss 0.1570, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 7501, loss 0.2052, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 8001, loss 0.1959, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 8501, loss 0.2343, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 9001, loss 0.1449, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 9501, loss 0.1996, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 10001, loss 0.1522, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 10501, loss 0.1610, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 11001, loss 0.1279, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 11501, loss 0.1287, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 12001, loss 0.1462, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 12501, loss 0.1869, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 13001, loss 0.1477, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 13501, loss 0.2160, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 14001, loss 0.1014, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 14501, loss 0.1575, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 15001, loss 0.1234, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15501, loss 0.1759, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16001, loss 0.1629, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16501, loss 0.1198, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17001, loss 0.1279, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17501, loss 0.1296, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 18001, loss 0.1329, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.1274, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19001, loss 0.1249, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19501, loss 0.1652, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20001, loss 0.1532, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20501, loss 0.1336, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 21001, loss 0.1357, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21501, loss 0.1567, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 22001, loss 0.1391, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22501, loss 0.1232, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23001, loss 0.1066, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23501, loss 0.1394, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24001, loss 0.1402, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24501, loss 0.0658, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 25.27868298
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.21
normal_0.5
./test_glass0/model_MLP_25000_0.21/record_1/MLP_25000_0.21_3
./test_glass0/result_MLP_25000_0.21_normal_0.5/record_1/
----------------------



the AUC is 0.518472906403941

the Fscore is 0.125

the precision is 0.5

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.21/record_1/MLP_25000_0.21_4
----------------------



epoch 1, loss 0.6469, train acc 51.46%, f1 0.5608, precision 0.3985, recall 0.9464, auc 0.6254
epoch 501, loss 0.3534, train acc 77.19%, f1 0.7310, precision 0.5955, recall 0.9464, auc 0.8167
epoch 1001, loss 0.4455, train acc 77.78%, f1 0.7361, precision 0.6023, recall 0.9464, auc 0.8210
epoch 1501, loss 0.3910, train acc 80.70%, f1 0.7626, precision 0.6386, recall 0.9464, auc 0.8428
epoch 2001, loss 0.3674, train acc 82.46%, f1 0.7794, precision 0.6625, recall 0.9464, auc 0.8558
epoch 2501, loss 0.2635, train acc 83.04%, f1 0.7852, precision 0.6709, recall 0.9464, auc 0.8602
epoch 3001, loss 0.2469, train acc 85.96%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8865
epoch 3501, loss 0.2728, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 4001, loss 0.3183, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 4501, loss 0.2591, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 5001, loss 0.1841, train acc 91.23%, f1 0.8780, precision 0.8060, recall 0.9643, auc 0.9256
epoch 5501, loss 0.2067, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 6001, loss 0.2302, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 6501, loss 0.2051, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 7001, loss 0.1815, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 7501, loss 0.1729, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 8001, loss 0.1677, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 8501, loss 0.2100, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 9001, loss 0.2201, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 9501, loss 0.2181, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 10001, loss 0.1509, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 10501, loss 0.1607, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 11001, loss 0.2087, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 11501, loss 0.2224, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 12001, loss 0.1850, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 12501, loss 0.2194, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 13001, loss 0.1957, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 13501, loss 0.1794, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 14001, loss 0.1468, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 14501, loss 0.2149, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 15001, loss 0.1790, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 15501, loss 0.1981, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 16001, loss 0.1407, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 16501, loss 0.1674, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 17001, loss 0.1116, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 17501, loss 0.1744, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 18001, loss 0.1714, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 18501, loss 0.1268, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 19001, loss 0.1961, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 19501, loss 0.1743, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 20001, loss 0.1484, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 20501, loss 0.1427, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 21001, loss 0.1182, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 21501, loss 0.1424, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 22001, loss 0.1464, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 22501, loss 0.2318, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 23001, loss 0.1464, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 23501, loss 0.1402, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 24001, loss 0.1332, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 24501, loss 0.1279, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
running_time is 25.470319834
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.21
normal_0.5
./test_glass0/model_MLP_25000_0.21/record_1/MLP_25000_0.21_4
./test_glass0/result_MLP_25000_0.21_normal_0.5/record_1/
----------------------



the AUC is 0.5554187192118226

the Fscore is 0.3

the precision is 0.5

the recall is 0.21428571428571427

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.21/record_1/MLP_25000_0.21_5
----------------------



epoch 1, loss 0.6290, train acc 47.67%, f1 0.5545, precision 0.3836, recall 1.0000, auc 0.6121
epoch 501, loss 0.3598, train acc 78.49%, f1 0.7448, precision 0.6067, recall 0.9643, auc 0.8313
epoch 1001, loss 0.3295, train acc 80.81%, f1 0.7626, precision 0.6386, recall 0.9464, auc 0.8439
epoch 1501, loss 0.2906, train acc 80.81%, f1 0.7626, precision 0.6386, recall 0.9464, auc 0.8439
epoch 2001, loss 0.2758, train acc 81.98%, f1 0.7737, precision 0.6543, recall 0.9464, auc 0.8525
epoch 2501, loss 0.2330, train acc 84.88%, f1 0.8060, precision 0.6923, recall 0.9643, auc 0.8787
epoch 3001, loss 0.2599, train acc 86.05%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8919
epoch 3501, loss 0.2858, train acc 87.21%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.9006
epoch 4001, loss 0.2836, train acc 87.21%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.9006
epoch 4501, loss 0.1433, train acc 87.79%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9049
epoch 5001, loss 0.2756, train acc 88.37%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9092
epoch 5501, loss 0.2078, train acc 90.70%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9264
epoch 6001, loss 0.2071, train acc 92.44%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9393
epoch 6501, loss 0.1529, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 7001, loss 0.1702, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 7501, loss 0.1602, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 8001, loss 0.1956, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 8501, loss 0.1977, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 9001, loss 0.1611, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 9501, loss 0.1305, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 10001, loss 0.1452, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 10501, loss 0.1583, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 11001, loss 0.1462, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 11501, loss 0.1799, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 12001, loss 0.2023, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 12501, loss 0.1051, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 13001, loss 0.0582, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 13501, loss 0.1900, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 14001, loss 0.0909, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 14501, loss 0.1383, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 15001, loss 0.1259, train acc 97.09%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9784
epoch 15501, loss 0.0911, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 16001, loss 0.1247, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 16501, loss 0.1076, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 17001, loss 0.1384, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 17501, loss 0.0983, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 18001, loss 0.1258, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 18501, loss 0.1359, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 19001, loss 0.1065, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 19501, loss 0.1064, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 20001, loss 0.1534, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 20501, loss 0.0851, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 21001, loss 0.0999, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 21501, loss 0.1421, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 22001, loss 0.1162, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 22501, loss 0.1745, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 23001, loss 0.0664, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 23501, loss 0.0942, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 24001, loss 0.1335, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
epoch 24501, loss 0.1302, train acc 97.67%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9828
running_time is 25.548807528
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.21
normal_0.5
./test_glass0/model_MLP_25000_0.21/record_1/MLP_25000_0.21_5
./test_glass0/result_MLP_25000_0.21_normal_0.5/record_1/
----------------------



the AUC is 0.5178571428571429

the Fscore is 0.125

the precision is 0.5

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.22/record_1/MLP_25000_0.22_1
----------------------



epoch 1, loss 0.6685, train acc 45.61%, f1 0.5463, precision 0.3758, recall 1.0000, auc 0.5957
epoch 501, loss 0.3958, train acc 80.12%, f1 0.7606, precision 0.6279, recall 0.9643, auc 0.8430
epoch 1001, loss 0.2819, train acc 81.87%, f1 0.7770, precision 0.6506, recall 0.9643, auc 0.8561
epoch 1501, loss 0.2935, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 2001, loss 0.3648, train acc 84.80%, f1 0.8060, precision 0.6923, recall 0.9643, auc 0.8778
epoch 2501, loss 0.3316, train acc 85.96%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8911
epoch 3001, loss 0.2442, train acc 87.13%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.8998
epoch 3501, loss 0.2983, train acc 87.13%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.8998
epoch 4001, loss 0.2424, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 4501, loss 0.1756, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 5001, loss 0.1963, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 5501, loss 0.2804, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 6001, loss 0.2193, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 6501, loss 0.1676, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 7001, loss 0.1409, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 7501, loss 0.2464, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 8001, loss 0.1395, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 8501, loss 0.2365, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 9001, loss 0.1536, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9501, loss 0.2577, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 10001, loss 0.1534, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 10501, loss 0.1381, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 11001, loss 0.1913, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 11501, loss 0.1768, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12001, loss 0.1734, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12501, loss 0.1403, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13001, loss 0.1756, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13501, loss 0.1295, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14001, loss 0.1095, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14501, loss 0.1639, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.1654, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15501, loss 0.1661, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16001, loss 0.1064, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.0978, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17001, loss 0.1080, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17501, loss 0.1690, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18001, loss 0.1542, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.1668, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19001, loss 0.0960, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19501, loss 0.1368, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20001, loss 0.0870, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20501, loss 0.1119, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21001, loss 0.1361, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21501, loss 0.1314, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 22001, loss 0.1213, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 22501, loss 0.1153, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 23001, loss 0.1505, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 23501, loss 0.1156, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 24001, loss 0.1103, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 24501, loss 0.1206, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
running_time is 25.623617601
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.22
normal_0.5
./test_glass0/model_MLP_25000_0.22/record_1/MLP_25000_0.22_1
./test_glass0/result_MLP_25000_0.22_normal_0.5/record_1/
----------------------



the AUC is 0.518472906403941

the Fscore is 0.125

the precision is 0.5

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.22/record_1/MLP_25000_0.22_2
----------------------



epoch 1, loss 0.5930, train acc 63.74%, f1 0.6173, precision 0.4717, recall 0.8929, auc 0.7030
epoch 501, loss 0.3734, train acc 76.61%, f1 0.7297, precision 0.5870, recall 0.9643, auc 0.8169
epoch 1001, loss 0.4600, train acc 80.12%, f1 0.7606, precision 0.6279, recall 0.9643, auc 0.8430
epoch 1501, loss 0.2607, train acc 83.04%, f1 0.7852, precision 0.6709, recall 0.9464, auc 0.8602
epoch 2001, loss 0.3103, train acc 84.21%, f1 0.8029, precision 0.6790, recall 0.9821, auc 0.8780
epoch 2501, loss 0.2591, train acc 87.13%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.8998
epoch 3001, loss 0.2374, train acc 87.13%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.8998
epoch 3501, loss 0.2070, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 4001, loss 0.1689, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 4501, loss 0.2314, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 5001, loss 0.2173, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 5501, loss 0.1904, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 6001, loss 0.1780, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 6501, loss 0.1345, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 7001, loss 0.1913, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 7501, loss 0.1827, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 8001, loss 0.1394, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 8501, loss 0.1852, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9001, loss 0.2143, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9501, loss 0.1538, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 10001, loss 0.1612, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10501, loss 0.1921, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11001, loss 0.1740, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11501, loss 0.1917, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12001, loss 0.1086, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12501, loss 0.1787, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13001, loss 0.0971, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13501, loss 0.1141, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14001, loss 0.0868, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14501, loss 0.1480, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.1907, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15501, loss 0.1078, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16001, loss 0.1865, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 16501, loss 0.1203, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17001, loss 0.0974, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17501, loss 0.1164, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18001, loss 0.1190, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.2093, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19001, loss 0.1707, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 19501, loss 0.1498, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20001, loss 0.1654, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20501, loss 0.1296, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21001, loss 0.1452, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21501, loss 0.1576, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 22001, loss 0.1393, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 22501, loss 0.1246, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 23001, loss 0.1347, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 23501, loss 0.1191, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 24001, loss 0.1493, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 24501, loss 0.1255, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
running_time is 25.346763483
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.22
normal_0.5
./test_glass0/model_MLP_25000_0.22/record_1/MLP_25000_0.22_2
./test_glass0/result_MLP_25000_0.22_normal_0.5/record_1/
----------------------



the AUC is 0.5935960591133005

the Fscore is 0.4444444444444445

the precision is 0.46153846153846156

the recall is 0.42857142857142855

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.22/record_1/MLP_25000_0.22_3
----------------------



epoch 1, loss 0.6496, train acc 43.27%, f1 0.5359, precision 0.3660, recall 1.0000, auc 0.5783
epoch 501, loss 0.3486, train acc 73.68%, f1 0.7020, precision 0.5579, recall 0.9464, auc 0.7906
epoch 1001, loss 0.3920, train acc 76.61%, f1 0.7260, precision 0.5889, recall 0.9464, auc 0.8123
epoch 1501, loss 0.4320, train acc 78.36%, f1 0.7413, precision 0.6092, recall 0.9464, auc 0.8254
epoch 2001, loss 0.4024, train acc 79.53%, f1 0.7518, precision 0.6235, recall 0.9464, auc 0.8341
epoch 2501, loss 0.3103, train acc 82.46%, f1 0.7794, precision 0.6625, recall 0.9464, auc 0.8558
epoch 3001, loss 0.3604, train acc 84.21%, f1 0.8000, precision 0.6835, recall 0.9643, auc 0.8734
epoch 3501, loss 0.2549, train acc 85.38%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8821
epoch 4001, loss 0.2894, train acc 86.55%, f1 0.8244, precision 0.7200, recall 0.9643, auc 0.8908
epoch 4501, loss 0.2042, train acc 86.55%, f1 0.8217, precision 0.7260, recall 0.9464, auc 0.8863
epoch 5001, loss 0.2885, train acc 88.30%, f1 0.8413, precision 0.7571, recall 0.9464, auc 0.8993
epoch 5501, loss 0.2470, train acc 88.30%, f1 0.8413, precision 0.7571, recall 0.9464, auc 0.8993
epoch 6001, loss 0.1596, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 6501, loss 0.2254, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 7001, loss 0.1865, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 7501, loss 0.1911, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 8001, loss 0.0917, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 8501, loss 0.1513, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 9001, loss 0.1648, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 9501, loss 0.1874, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 10001, loss 0.2216, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 10501, loss 0.1265, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 11001, loss 0.1582, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 11501, loss 0.1529, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 12001, loss 0.1657, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 12501, loss 0.1561, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 13001, loss 0.1319, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 13501, loss 0.1625, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 14001, loss 0.1416, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 14501, loss 0.1680, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 15001, loss 0.1179, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 15501, loss 0.1363, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 16001, loss 0.2319, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 16501, loss 0.1414, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 17001, loss 0.0849, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 17501, loss 0.1561, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 18001, loss 0.1504, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 18501, loss 0.1643, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 19001, loss 0.1452, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 19501, loss 0.1019, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 20001, loss 0.1849, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 20501, loss 0.2002, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 21001, loss 0.1625, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 21501, loss 0.1175, train acc 97.08%, f1 0.9565, precision 0.9322, recall 0.9821, auc 0.9737
epoch 22001, loss 0.1445, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 22501, loss 0.1559, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 23001, loss 0.0997, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 23501, loss 0.1873, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 24001, loss 0.1287, train acc 96.49%, f1 0.9483, precision 0.9167, recall 0.9821, auc 0.9693
epoch 24501, loss 0.1254, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
running_time is 25.35027332
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.22
normal_0.5
./test_glass0/model_MLP_25000_0.22/record_1/MLP_25000_0.22_3
./test_glass0/result_MLP_25000_0.22_normal_0.5/record_1/
----------------------



the AUC is 0.625615763546798

the Fscore is 0.4210526315789473

the precision is 0.8

the recall is 0.2857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.22/record_1/MLP_25000_0.22_4
----------------------



epoch 1, loss 0.6685, train acc 38.01%, f1 0.5138, precision 0.3457, recall 1.0000, auc 0.5391
epoch 501, loss 0.4172, train acc 76.02%, f1 0.7248, precision 0.5806, recall 0.9643, auc 0.8126
epoch 1001, loss 0.4270, train acc 77.78%, f1 0.7361, precision 0.6023, recall 0.9464, auc 0.8210
epoch 1501, loss 0.3809, train acc 79.53%, f1 0.7518, precision 0.6235, recall 0.9464, auc 0.8341
epoch 2001, loss 0.4476, train acc 80.70%, f1 0.7626, precision 0.6386, recall 0.9464, auc 0.8428
epoch 2501, loss 0.2999, train acc 83.04%, f1 0.7852, precision 0.6709, recall 0.9464, auc 0.8602
epoch 3001, loss 0.2161, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 3501, loss 0.3554, train acc 86.55%, f1 0.8217, precision 0.7260, recall 0.9464, auc 0.8863
epoch 4001, loss 0.3033, train acc 87.13%, f1 0.8308, precision 0.7297, recall 0.9643, auc 0.8952
epoch 4501, loss 0.2389, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 5001, loss 0.2699, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 5501, loss 0.2223, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 6001, loss 0.2330, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 6501, loss 0.2435, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 7001, loss 0.2348, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 7501, loss 0.1758, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 8001, loss 0.1635, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 8501, loss 0.2157, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 9001, loss 0.2392, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 9501, loss 0.2787, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 10001, loss 0.2059, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 10501, loss 0.1533, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 11001, loss 0.1743, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 11501, loss 0.1211, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 12001, loss 0.2115, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 12501, loss 0.1508, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 13001, loss 0.1476, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 13501, loss 0.2123, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 14001, loss 0.1374, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 14501, loss 0.2043, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 15001, loss 0.1300, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 15501, loss 0.1347, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 16001, loss 0.1284, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 16501, loss 0.2007, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 17001, loss 0.1895, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 17501, loss 0.1396, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 18001, loss 0.1443, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 18501, loss 0.1717, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 19001, loss 0.1627, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 19501, loss 0.1316, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 20001, loss 0.1724, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 20501, loss 0.2293, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 21001, loss 0.1718, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 21501, loss 0.1997, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 22001, loss 0.1922, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 22501, loss 0.1454, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 23001, loss 0.1633, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 23501, loss 0.1632, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 24001, loss 0.2288, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 24501, loss 0.1701, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
running_time is 25.431866599
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.22
normal_0.5
./test_glass0/model_MLP_25000_0.22/record_1/MLP_25000_0.22_4
./test_glass0/result_MLP_25000_0.22_normal_0.5/record_1/
----------------------



the AUC is 0.7573891625615764

the Fscore is 0.6666666666666666

the precision is 0.52

the recall is 0.9285714285714286

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.22/record_1/MLP_25000_0.22_5
----------------------



epoch 1, loss 0.6496, train acc 35.47%, f1 0.5022, precision 0.3353, recall 1.0000, auc 0.5216
epoch 501, loss 0.4897, train acc 76.74%, f1 0.7297, precision 0.5870, recall 0.9643, auc 0.8183
epoch 1001, loss 0.3785, train acc 80.23%, f1 0.7606, precision 0.6279, recall 0.9643, auc 0.8442
epoch 1501, loss 0.2507, train acc 81.40%, f1 0.7714, precision 0.6429, recall 0.9643, auc 0.8528
epoch 2001, loss 0.3649, train acc 81.98%, f1 0.7737, precision 0.6543, recall 0.9464, auc 0.8525
epoch 2501, loss 0.2916, train acc 83.72%, f1 0.7910, precision 0.6795, recall 0.9464, auc 0.8655
epoch 3001, loss 0.3142, train acc 84.30%, f1 0.8000, precision 0.6835, recall 0.9643, auc 0.8744
epoch 3501, loss 0.3236, train acc 84.30%, f1 0.8000, precision 0.6835, recall 0.9643, auc 0.8744
epoch 4001, loss 0.2981, train acc 85.47%, f1 0.8148, precision 0.6962, recall 0.9821, auc 0.8876
epoch 4501, loss 0.2154, train acc 86.63%, f1 0.8244, precision 0.7200, recall 0.9643, auc 0.8916
epoch 5001, loss 0.2593, train acc 86.63%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8962
epoch 5501, loss 0.2562, train acc 88.37%, f1 0.8437, precision 0.7500, recall 0.9643, auc 0.9046
epoch 6001, loss 0.2366, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 6501, loss 0.2323, train acc 90.12%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9221
epoch 7001, loss 0.2244, train acc 91.28%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9307
epoch 7501, loss 0.1959, train acc 91.28%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9307
epoch 8001, loss 0.1986, train acc 91.28%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9307
epoch 8501, loss 0.2911, train acc 91.28%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9307
epoch 9001, loss 0.3173, train acc 91.28%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9307
epoch 9501, loss 0.1769, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 10001, loss 0.2000, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 10501, loss 0.2191, train acc 92.44%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9393
epoch 11001, loss 0.2362, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 11501, loss 0.1821, train acc 92.44%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9393
epoch 12001, loss 0.1989, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 12501, loss 0.1932, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 13001, loss 0.1746, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 13501, loss 0.2022, train acc 92.44%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9393
epoch 14001, loss 0.1196, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 14501, loss 0.2042, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 15001, loss 0.2091, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 15501, loss 0.2125, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 16001, loss 0.1745, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 16501, loss 0.1427, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 17001, loss 0.1131, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 17501, loss 0.1631, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 18001, loss 0.1532, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 18501, loss 0.1500, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 19001, loss 0.1168, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 19501, loss 0.1486, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 20001, loss 0.2125, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 20501, loss 0.1645, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 21001, loss 0.1448, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 21501, loss 0.1986, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 22001, loss 0.1680, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 22501, loss 0.1225, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 23001, loss 0.1273, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 23501, loss 0.1790, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 24001, loss 0.1400, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 24501, loss 0.1316, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
running_time is 25.376353502
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.22
normal_0.5
./test_glass0/model_MLP_25000_0.22/record_1/MLP_25000_0.22_5
./test_glass0/result_MLP_25000_0.22_normal_0.5/record_1/
----------------------



the AUC is 0.6785714285714286

the Fscore is 0.56

the precision is 0.6363636363636364

the recall is 0.5

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.23/record_1/MLP_25000_0.23_1
----------------------



epoch 1, loss 0.6926, train acc 46.78%, f1 0.5517, precision 0.3810, recall 1.0000, auc 0.6043
epoch 501, loss 0.4070, train acc 80.70%, f1 0.7660, precision 0.6353, recall 0.9643, auc 0.8474
epoch 1001, loss 0.3190, train acc 81.29%, f1 0.7714, precision 0.6429, recall 0.9643, auc 0.8517
epoch 1501, loss 0.3762, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 2001, loss 0.3196, train acc 82.46%, f1 0.7826, precision 0.6585, recall 0.9643, auc 0.8604
epoch 2501, loss 0.3988, train acc 83.63%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8737
epoch 3001, loss 0.2156, train acc 85.38%, f1 0.8148, precision 0.6962, recall 0.9821, auc 0.8867
epoch 3501, loss 0.2517, train acc 85.96%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8911
epoch 4001, loss 0.2766, train acc 87.13%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.8998
epoch 4501, loss 0.2234, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 5001, loss 0.2856, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 5501, loss 0.2198, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 6001, loss 0.1676, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 6501, loss 0.2080, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 7001, loss 0.2224, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 7501, loss 0.2170, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 8001, loss 0.2372, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 8501, loss 0.1581, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 9001, loss 0.2042, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 9501, loss 0.1685, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10001, loss 0.2680, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10501, loss 0.2288, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11001, loss 0.2094, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11501, loss 0.1655, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 12001, loss 0.1359, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 12501, loss 0.2273, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 13001, loss 0.1165, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 13501, loss 0.1967, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 14001, loss 0.2228, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 14501, loss 0.2059, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 15001, loss 0.2070, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 15501, loss 0.1749, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 16001, loss 0.1830, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 16501, loss 0.1676, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 17001, loss 0.1854, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 17501, loss 0.1359, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18001, loss 0.1625, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18501, loss 0.2754, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19001, loss 0.2052, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19501, loss 0.1895, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 20001, loss 0.2419, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 20501, loss 0.2116, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 21001, loss 0.1337, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 21501, loss 0.2082, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 22001, loss 0.1087, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 22501, loss 0.1738, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 23001, loss 0.1443, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 23501, loss 0.1776, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24001, loss 0.1586, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24501, loss 0.2017, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
running_time is 25.38077689
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.23
normal_0.5
./test_glass0/model_MLP_25000_0.23/record_1/MLP_25000_0.23_1
./test_glass0/result_MLP_25000_0.23_normal_0.5/record_1/
----------------------



the AUC is 0.5738916256157635

the Fscore is 0.36363636363636365

the precision is 0.5

the recall is 0.2857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.23/record_1/MLP_25000_0.23_2
----------------------



epoch 1, loss 0.6330, train acc 43.27%, f1 0.5359, precision 0.3660, recall 1.0000, auc 0.5783
epoch 501, loss 0.4559, train acc 76.61%, f1 0.7297, precision 0.5870, recall 0.9643, auc 0.8169
epoch 1001, loss 0.3085, train acc 82.46%, f1 0.7794, precision 0.6625, recall 0.9464, auc 0.8558
epoch 1501, loss 0.4139, train acc 84.80%, f1 0.8088, precision 0.6875, recall 0.9821, auc 0.8824
epoch 2001, loss 0.3166, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 2501, loss 0.2510, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 3001, loss 0.2842, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 3501, loss 0.2264, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 4001, loss 0.2014, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 4501, loss 0.1998, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 5001, loss 0.2182, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 5501, loss 0.2126, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 6001, loss 0.1814, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 6501, loss 0.1341, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 7001, loss 0.1575, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 7501, loss 0.1682, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 8001, loss 0.1784, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 8501, loss 0.1123, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 9001, loss 0.1912, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 9501, loss 0.1212, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10001, loss 0.1618, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10501, loss 0.0968, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11001, loss 0.1288, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 11501, loss 0.1899, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12001, loss 0.1628, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12501, loss 0.1235, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13001, loss 0.0903, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13501, loss 0.1631, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14001, loss 0.1636, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14501, loss 0.1541, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 15001, loss 0.1527, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 15501, loss 0.1363, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 16001, loss 0.1839, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.1301, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.1441, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 17501, loss 0.0810, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 18001, loss 0.1037, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.1050, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.1169, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.1652, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20001, loss 0.2048, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20501, loss 0.1654, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21001, loss 0.1089, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21501, loss 0.1685, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22001, loss 0.1636, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22501, loss 0.0856, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23001, loss 0.1153, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23501, loss 0.0696, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24001, loss 0.1354, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24501, loss 0.0933, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 25.453671951
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.23
normal_0.5
./test_glass0/model_MLP_25000_0.23/record_1/MLP_25000_0.23_2
./test_glass0/result_MLP_25000_0.23_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.23/record_1/MLP_25000_0.23_3
----------------------



epoch 1, loss 0.6728, train acc 53.80%, f1 0.5325, precision 0.3982, recall 0.8036, auc 0.6061
epoch 501, loss 0.4158, train acc 76.02%, f1 0.7172, precision 0.5843, recall 0.9286, auc 0.8034
epoch 1001, loss 0.3594, train acc 79.53%, f1 0.7445, precision 0.6296, recall 0.9107, auc 0.8249
epoch 1501, loss 0.3491, train acc 80.12%, f1 0.7571, precision 0.6310, recall 0.9464, auc 0.8384
epoch 2001, loss 0.2778, train acc 83.04%, f1 0.7883, precision 0.6667, recall 0.9643, auc 0.8648
epoch 2501, loss 0.3189, train acc 85.38%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8821
epoch 3001, loss 0.2485, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 3501, loss 0.1952, train acc 87.72%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9041
epoch 4001, loss 0.2700, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 4501, loss 0.1758, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 5001, loss 0.2274, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 5501, loss 0.1521, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 6001, loss 0.1934, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 6501, loss 0.1668, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 7001, loss 0.1726, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 7501, loss 0.1797, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 8001, loss 0.1898, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 8501, loss 0.1660, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9001, loss 0.1683, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9501, loss 0.1382, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10001, loss 0.1319, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10501, loss 0.1367, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11001, loss 0.1442, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 11501, loss 0.1898, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12001, loss 0.1748, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12501, loss 0.1384, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13001, loss 0.1213, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13501, loss 0.1312, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14001, loss 0.1144, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14501, loss 0.1405, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15001, loss 0.1340, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 15501, loss 0.1201, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.1210, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.1210, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.1048, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.1489, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.1115, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 18501, loss 0.0885, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19001, loss 0.1309, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 19501, loss 0.0953, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 20001, loss 0.1140, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 20501, loss 0.1018, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 21001, loss 0.0742, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 21501, loss 0.1244, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 22001, loss 0.0971, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 22501, loss 0.1030, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 23001, loss 0.0948, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 23501, loss 0.1144, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24001, loss 0.1046, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
epoch 24501, loss 0.0864, train acc 99.42%, f1 0.9912, precision 0.9825, recall 1.0000, auc 0.9957
running_time is 25.392287893
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.23
normal_0.5
./test_glass0/model_MLP_25000_0.23/record_1/MLP_25000_0.23_3
./test_glass0/result_MLP_25000_0.23_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.23/record_1/MLP_25000_0.23_4
----------------------



epoch 1, loss 0.5732, train acc 67.25%, f1 0.0000, precision 0.0000, recall 0.0000, auc 0.5000
epoch 501, loss 0.4390, train acc 76.02%, f1 0.7248, precision 0.5806, recall 0.9643, auc 0.8126
epoch 1001, loss 0.3065, train acc 77.19%, f1 0.7310, precision 0.5955, recall 0.9464, auc 0.8167
epoch 1501, loss 0.3956, train acc 80.70%, f1 0.7626, precision 0.6386, recall 0.9464, auc 0.8428
epoch 2001, loss 0.3994, train acc 81.29%, f1 0.7681, precision 0.6463, recall 0.9464, auc 0.8471
epoch 2501, loss 0.3557, train acc 83.04%, f1 0.7883, precision 0.6667, recall 0.9643, auc 0.8648
epoch 3001, loss 0.2385, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 3501, loss 0.2851, train acc 87.13%, f1 0.8308, precision 0.7297, recall 0.9643, auc 0.8952
epoch 4001, loss 0.2146, train acc 88.30%, f1 0.8437, precision 0.7500, recall 0.9643, auc 0.9039
epoch 4501, loss 0.2692, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 5001, loss 0.2447, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 5501, loss 0.2157, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 6001, loss 0.2423, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 6501, loss 0.1195, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 7001, loss 0.1752, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 7501, loss 0.1333, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 8001, loss 0.2540, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 8501, loss 0.2416, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 9001, loss 0.1028, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 9501, loss 0.2166, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 10001, loss 0.1945, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 10501, loss 0.1791, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 11001, loss 0.1425, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 11501, loss 0.2199, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 12001, loss 0.1312, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 12501, loss 0.1839, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 13001, loss 0.1898, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13501, loss 0.1408, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14001, loss 0.1719, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 14501, loss 0.1856, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15001, loss 0.1507, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15501, loss 0.1691, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16001, loss 0.1149, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16501, loss 0.2003, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17001, loss 0.1744, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 17501, loss 0.1995, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18001, loss 0.1478, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18501, loss 0.1786, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 19001, loss 0.1276, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19501, loss 0.1402, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 20001, loss 0.1255, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 20501, loss 0.2010, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 21001, loss 0.1535, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 21501, loss 0.1752, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 22001, loss 0.1621, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 22501, loss 0.1575, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 23001, loss 0.1584, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 23501, loss 0.0901, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 24001, loss 0.1333, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 24501, loss 0.1742, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
running_time is 25.658963198000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.23
normal_0.5
./test_glass0/model_MLP_25000_0.23/record_1/MLP_25000_0.23_4
./test_glass0/result_MLP_25000_0.23_normal_0.5/record_1/
----------------------



the AUC is 0.518472906403941

the Fscore is 0.125

the precision is 0.5

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.23/record_1/MLP_25000_0.23_5
----------------------



epoch 1, loss 0.6529, train acc 45.35%, f1 0.5437, precision 0.3733, recall 1.0000, auc 0.5948
epoch 501, loss 0.3872, train acc 78.49%, f1 0.7448, precision 0.6067, recall 0.9643, auc 0.8313
epoch 1001, loss 0.4336, train acc 79.65%, f1 0.7552, precision 0.6207, recall 0.9643, auc 0.8399
epoch 1501, loss 0.4296, train acc 79.65%, f1 0.7518, precision 0.6235, recall 0.9464, auc 0.8353
epoch 2001, loss 0.2533, train acc 81.98%, f1 0.7737, precision 0.6543, recall 0.9464, auc 0.8525
epoch 2501, loss 0.2864, train acc 84.30%, f1 0.7970, precision 0.6883, recall 0.9464, auc 0.8698
epoch 3001, loss 0.2858, train acc 85.47%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8830
epoch 3501, loss 0.2375, train acc 86.05%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8873
epoch 4001, loss 0.2717, train acc 86.63%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8962
epoch 4501, loss 0.2643, train acc 87.21%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.9006
epoch 5001, loss 0.2471, train acc 88.37%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9092
epoch 5501, loss 0.3107, train acc 88.95%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9135
epoch 6001, loss 0.1994, train acc 88.95%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9135
epoch 6501, loss 0.2006, train acc 90.12%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9221
epoch 7001, loss 0.1712, train acc 90.70%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9310
epoch 7501, loss 0.2347, train acc 92.44%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9393
epoch 8001, loss 0.1947, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 8501, loss 0.2483, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 9001, loss 0.1655, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 9501, loss 0.1966, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 10001, loss 0.1648, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 10501, loss 0.1431, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 11001, loss 0.1831, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 11501, loss 0.1728, train acc 93.60%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9480
epoch 12001, loss 0.2076, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 12501, loss 0.2062, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 13001, loss 0.2360, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 13501, loss 0.1408, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 14001, loss 0.1640, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 14501, loss 0.1854, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 15001, loss 0.1260, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 15501, loss 0.2546, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 16001, loss 0.1312, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 16501, loss 0.1282, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 17001, loss 0.2009, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 17501, loss 0.1996, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 18001, loss 0.2008, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 18501, loss 0.1497, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 19001, loss 0.1139, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 19501, loss 0.1430, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 20001, loss 0.1526, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 20501, loss 0.1811, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 21001, loss 0.1295, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 21501, loss 0.1532, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 22001, loss 0.1575, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 22501, loss 0.1209, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 23001, loss 0.1391, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
epoch 23501, loss 0.0962, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 24001, loss 0.1361, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 24501, loss 0.1141, train acc 96.51%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9741
running_time is 25.431717738
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.23
normal_0.5
./test_glass0/model_MLP_25000_0.23/record_1/MLP_25000_0.23_5
./test_glass0/result_MLP_25000_0.23_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.24/record_1/MLP_25000_0.24_1
----------------------



epoch 1, loss 0.6357, train acc 33.33%, f1 0.4956, precision 0.3294, recall 1.0000, auc 0.5043
epoch 501, loss 0.3141, train acc 76.02%, f1 0.7248, precision 0.5806, recall 0.9643, auc 0.8126
epoch 1001, loss 0.3348, train acc 80.70%, f1 0.7660, precision 0.6353, recall 0.9643, auc 0.8474
epoch 1501, loss 0.3211, train acc 81.87%, f1 0.7770, precision 0.6506, recall 0.9643, auc 0.8561
epoch 2001, loss 0.3273, train acc 82.46%, f1 0.7826, precision 0.6585, recall 0.9643, auc 0.8604
epoch 2501, loss 0.2799, train acc 84.80%, f1 0.8060, precision 0.6923, recall 0.9643, auc 0.8778
epoch 3001, loss 0.2481, train acc 85.38%, f1 0.8120, precision 0.7013, recall 0.9643, auc 0.8821
epoch 3501, loss 0.3079, train acc 85.96%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8911
epoch 4001, loss 0.2560, train acc 87.72%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9041
epoch 4501, loss 0.2660, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 5001, loss 0.2672, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 5501, loss 0.2509, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 6001, loss 0.2276, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 6501, loss 0.2155, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 7001, loss 0.1940, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 7501, loss 0.1229, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 8001, loss 0.1756, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 8501, loss 0.1558, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9001, loss 0.1548, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9501, loss 0.1148, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10001, loss 0.1044, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10501, loss 0.1616, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 11001, loss 0.1663, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 11501, loss 0.1278, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12001, loss 0.1383, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12501, loss 0.1701, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13001, loss 0.1598, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13501, loss 0.1461, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14001, loss 0.1254, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14501, loss 0.1720, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15001, loss 0.1429, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15501, loss 0.1566, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16001, loss 0.1745, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 16501, loss 0.2089, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17001, loss 0.1062, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17501, loss 0.2385, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 18001, loss 0.2046, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 18501, loss 0.1421, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 19001, loss 0.1741, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 19501, loss 0.1290, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 20001, loss 0.1520, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20501, loss 0.1453, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21001, loss 0.1174, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21501, loss 0.1547, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 22001, loss 0.1076, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 22501, loss 0.1128, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 23001, loss 0.1283, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 23501, loss 0.1294, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 24001, loss 0.1326, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 24501, loss 0.1401, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
running_time is 25.617811898
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.24
normal_0.5
./test_glass0/model_MLP_25000_0.24/record_1/MLP_25000_0.24_1
./test_glass0/result_MLP_25000_0.24_normal_0.5/record_1/
----------------------



the AUC is 0.5073891625615764

the Fscore is 0.375

the precision is 0.3333333333333333

the recall is 0.42857142857142855

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.24/record_1/MLP_25000_0.24_2
----------------------



epoch 1, loss 0.5940, train acc 34.50%, f1 0.4955, precision 0.3313, recall 0.9821, auc 0.5085
epoch 501, loss 0.3541, train acc 75.44%, f1 0.7162, precision 0.5761, recall 0.9464, auc 0.8036
epoch 1001, loss 0.3428, train acc 81.29%, f1 0.7681, precision 0.6463, recall 0.9464, auc 0.8471
epoch 1501, loss 0.2959, train acc 82.46%, f1 0.7826, precision 0.6585, recall 0.9643, auc 0.8604
epoch 2001, loss 0.2773, train acc 84.80%, f1 0.8088, precision 0.6875, recall 0.9821, auc 0.8824
epoch 2501, loss 0.3352, train acc 85.96%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8911
epoch 3001, loss 0.3133, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 3501, loss 0.2035, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 4001, loss 0.2536, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 4501, loss 0.2517, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 5001, loss 0.2316, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 5501, loss 0.2094, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 6001, loss 0.2418, train acc 90.64%, f1 0.8710, precision 0.7941, recall 0.9643, auc 0.9213
epoch 6501, loss 0.2539, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 7001, loss 0.1528, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 7501, loss 0.1631, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8001, loss 0.1673, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 8501, loss 0.1993, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 9001, loss 0.1465, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 9501, loss 0.2130, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 10001, loss 0.2195, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10501, loss 0.1605, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 11001, loss 0.1677, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 11501, loss 0.1206, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 12001, loss 0.2411, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 12501, loss 0.1761, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13001, loss 0.1267, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 13501, loss 0.1436, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 14001, loss 0.1242, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 14501, loss 0.1693, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15001, loss 0.1483, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 15501, loss 0.1710, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 16001, loss 0.1337, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 16501, loss 0.1524, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 17001, loss 0.1904, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17501, loss 0.1563, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 18001, loss 0.1087, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 18501, loss 0.1605, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 19001, loss 0.1507, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 19501, loss 0.1267, train acc 97.66%, f1 0.9649, precision 0.9483, recall 0.9821, auc 0.9780
epoch 20001, loss 0.1555, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20501, loss 0.1379, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21001, loss 0.1134, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 21501, loss 0.1001, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22001, loss 0.0702, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 22501, loss 0.0946, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23001, loss 0.1124, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 23501, loss 0.1593, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 24001, loss 0.1079, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 24501, loss 0.1109, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 25.369563161000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.24
normal_0.5
./test_glass0/model_MLP_25000_0.24/record_1/MLP_25000_0.24_2
./test_glass0/result_MLP_25000_0.24_normal_0.5/record_1/
----------------------



the AUC is 0.5554187192118226

the Fscore is 0.3

the precision is 0.5

the recall is 0.21428571428571427

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.24/record_1/MLP_25000_0.24_3
----------------------



epoch 1, loss 0.6984, train acc 49.12%, f1 0.5584, precision 0.3901, recall 0.9821, auc 0.6172
epoch 501, loss 0.3223, train acc 73.10%, f1 0.7013, precision 0.5510, recall 0.9643, auc 0.7908
epoch 1001, loss 0.3416, train acc 76.61%, f1 0.7183, precision 0.5930, recall 0.9107, auc 0.8032
epoch 1501, loss 0.2812, train acc 79.53%, f1 0.7482, precision 0.6265, recall 0.9286, auc 0.8295
epoch 2001, loss 0.3049, train acc 81.29%, f1 0.7681, precision 0.6463, recall 0.9464, auc 0.8471
epoch 2501, loss 0.2347, train acc 84.80%, f1 0.8060, precision 0.6923, recall 0.9643, auc 0.8778
epoch 3001, loss 0.2957, train acc 87.13%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.8998
epoch 3501, loss 0.2928, train acc 87.72%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9041
epoch 4001, loss 0.2192, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 4501, loss 0.2745, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 5001, loss 0.3218, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 5501, loss 0.1595, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 6001, loss 0.1734, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 6501, loss 0.2309, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 7001, loss 0.2182, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 7501, loss 0.1630, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 8001, loss 0.2521, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 8501, loss 0.1779, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 9001, loss 0.1718, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 9501, loss 0.2041, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 10001, loss 0.1842, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10501, loss 0.1301, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 11001, loss 0.1190, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 11501, loss 0.1586, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 12001, loss 0.1786, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 12501, loss 0.1707, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13001, loss 0.2031, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 13501, loss 0.1654, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14001, loss 0.1645, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14501, loss 0.1176, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15001, loss 0.1331, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 15501, loss 0.1208, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 16001, loss 0.1549, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 16501, loss 0.1287, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17001, loss 0.1830, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 17501, loss 0.1411, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 18001, loss 0.1236, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 18501, loss 0.1300, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 19001, loss 0.1109, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 19501, loss 0.1254, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 20001, loss 0.1512, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 20501, loss 0.1165, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21001, loss 0.1600, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 21501, loss 0.2551, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 22001, loss 0.1325, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22501, loss 0.1277, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23001, loss 0.2010, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23501, loss 0.1563, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24001, loss 0.1576, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24501, loss 0.1259, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 25.35040532
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.24
normal_0.5
./test_glass0/model_MLP_25000_0.24/record_1/MLP_25000_0.24_3
./test_glass0/result_MLP_25000_0.24_normal_0.5/record_1/
----------------------



the AUC is 0.6206896551724138

the Fscore is 0.56

the precision is 0.3888888888888889

the recall is 1.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.24/record_1/MLP_25000_0.24_4
----------------------



epoch 1, loss 0.6357, train acc 49.71%, f1 0.5612, precision 0.3929, recall 0.9821, auc 0.6215
epoch 501, loss 0.4757, train acc 74.27%, f1 0.7105, precision 0.5625, recall 0.9643, auc 0.7995
epoch 1001, loss 0.4157, train acc 76.61%, f1 0.7260, precision 0.5889, recall 0.9464, auc 0.8123
epoch 1501, loss 0.3823, train acc 78.95%, f1 0.7465, precision 0.6163, recall 0.9464, auc 0.8297
epoch 2001, loss 0.3131, train acc 80.12%, f1 0.7571, precision 0.6310, recall 0.9464, auc 0.8384
epoch 2501, loss 0.3438, train acc 82.46%, f1 0.7794, precision 0.6625, recall 0.9464, auc 0.8558
epoch 3001, loss 0.2496, train acc 84.80%, f1 0.8060, precision 0.6923, recall 0.9643, auc 0.8778
epoch 3501, loss 0.3744, train acc 85.38%, f1 0.8148, precision 0.6962, recall 0.9821, auc 0.8867
epoch 4001, loss 0.3178, train acc 84.80%, f1 0.8088, precision 0.6875, recall 0.9821, auc 0.8824
epoch 4501, loss 0.2979, train acc 87.13%, f1 0.8308, precision 0.7297, recall 0.9643, auc 0.8952
epoch 5001, loss 0.2322, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 5501, loss 0.2514, train acc 88.89%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9082
epoch 6001, loss 0.2411, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 6501, loss 0.2605, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 7001, loss 0.2061, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 7501, loss 0.1988, train acc 90.06%, f1 0.8640, precision 0.7826, recall 0.9643, auc 0.9169
epoch 8001, loss 0.1792, train acc 90.64%, f1 0.8730, precision 0.7857, recall 0.9821, auc 0.9259
epoch 8501, loss 0.1825, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 9001, loss 0.1868, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 9501, loss 0.2820, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 10001, loss 0.1563, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 10501, loss 0.2267, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11001, loss 0.1657, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 11501, loss 0.2152, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 12001, loss 0.1805, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 12501, loss 0.1786, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 13001, loss 0.1197, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 13501, loss 0.1702, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 14001, loss 0.1733, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 14501, loss 0.1629, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 15001, loss 0.1737, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 15501, loss 0.2179, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 16001, loss 0.1653, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 16501, loss 0.1978, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 17001, loss 0.2031, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 17501, loss 0.1401, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 18001, loss 0.1721, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 18501, loss 0.2028, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 19001, loss 0.1660, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 19501, loss 0.1733, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 20001, loss 0.1535, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 20501, loss 0.1597, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 21001, loss 0.1630, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 21501, loss 0.1946, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 22001, loss 0.1573, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 22501, loss 0.1059, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 23001, loss 0.2015, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 23501, loss 0.2128, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 24001, loss 0.1935, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 24501, loss 0.1736, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
running_time is 25.444821067
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.24
normal_0.5
./test_glass0/model_MLP_25000_0.24/record_1/MLP_25000_0.24_4
./test_glass0/result_MLP_25000_0.24_normal_0.5/record_1/
----------------------



the AUC is 0.5541871921182266

the Fscore is 0.23529411764705882

the precision is 0.6666666666666666

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.24/record_1/MLP_25000_0.24_5
----------------------



epoch 1, loss 0.7405, train acc 32.56%, f1 0.4912, precision 0.3256, recall 1.0000, auc 0.5000
epoch 501, loss 0.3959, train acc 78.49%, f1 0.7448, precision 0.6067, recall 0.9643, auc 0.8313
epoch 1001, loss 0.3464, train acc 79.07%, f1 0.7500, precision 0.6136, recall 0.9643, auc 0.8356
epoch 1501, loss 0.3145, train acc 80.23%, f1 0.7606, precision 0.6279, recall 0.9643, auc 0.8442
epoch 2001, loss 0.4339, train acc 81.40%, f1 0.7681, precision 0.6463, recall 0.9464, auc 0.8482
epoch 2501, loss 0.3596, train acc 83.14%, f1 0.7883, precision 0.6667, recall 0.9643, auc 0.8658
epoch 3001, loss 0.3220, train acc 84.30%, f1 0.8000, precision 0.6835, recall 0.9643, auc 0.8744
epoch 3501, loss 0.2864, train acc 84.88%, f1 0.8088, precision 0.6875, recall 0.9821, auc 0.8833
epoch 4001, loss 0.2984, train acc 86.05%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8919
epoch 4501, loss 0.2779, train acc 86.05%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8919
epoch 5001, loss 0.2607, train acc 87.21%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.9006
epoch 5501, loss 0.2865, train acc 88.95%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9181
epoch 6001, loss 0.3263, train acc 87.79%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9049
epoch 6501, loss 0.2555, train acc 88.95%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9135
epoch 7001, loss 0.1980, train acc 90.70%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9310
epoch 7501, loss 0.1656, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 8001, loss 0.1494, train acc 92.44%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9393
epoch 8501, loss 0.2072, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 9001, loss 0.2106, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 9501, loss 0.1918, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 10001, loss 0.1347, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 10501, loss 0.2499, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 11001, loss 0.2187, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 11501, loss 0.2106, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 12001, loss 0.1453, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 12501, loss 0.2274, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 13001, loss 0.1763, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 13501, loss 0.1570, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 14001, loss 0.2011, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 14501, loss 0.2228, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 15001, loss 0.1900, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 15501, loss 0.1304, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 16001, loss 0.1204, train acc 94.77%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9566
epoch 16501, loss 0.2013, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 17001, loss 0.1463, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 17501, loss 0.1671, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 18001, loss 0.1453, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 18501, loss 0.1422, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 19001, loss 0.1797, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 19501, loss 0.0879, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 20001, loss 0.2041, train acc 95.35%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9655
epoch 20501, loss 0.1690, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 21001, loss 0.1503, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 21501, loss 0.1064, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 22001, loss 0.1238, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 22501, loss 0.2133, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 23001, loss 0.1468, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 23501, loss 0.2032, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 24001, loss 0.1547, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
epoch 24501, loss 0.1072, train acc 95.93%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9698
running_time is 25.366144823
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.24
normal_0.5
./test_glass0/model_MLP_25000_0.24/record_1/MLP_25000_0.24_5
./test_glass0/result_MLP_25000_0.24_normal_0.5/record_1/
----------------------



the AUC is 0.6964285714285714

the Fscore is 0.5925925925925927

the precision is 0.6153846153846154

the recall is 0.5714285714285714

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.25/record_1/MLP_25000_0.25_1
----------------------



epoch 1, loss 0.6611, train acc 34.50%, f1 0.5000, precision 0.3333, recall 1.0000, auc 0.5130
epoch 501, loss 0.4247, train acc 76.61%, f1 0.7297, precision 0.5870, recall 0.9643, auc 0.8169
epoch 1001, loss 0.2830, train acc 80.12%, f1 0.7571, precision 0.6310, recall 0.9464, auc 0.8384
epoch 1501, loss 0.2831, train acc 82.46%, f1 0.7826, precision 0.6585, recall 0.9643, auc 0.8604
epoch 2001, loss 0.2960, train acc 83.04%, f1 0.7883, precision 0.6667, recall 0.9643, auc 0.8648
epoch 2501, loss 0.2638, train acc 84.80%, f1 0.8060, precision 0.6923, recall 0.9643, auc 0.8778
epoch 3001, loss 0.2614, train acc 85.96%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8911
epoch 3501, loss 0.2617, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 4001, loss 0.2445, train acc 87.13%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.8998
epoch 4501, loss 0.2425, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 5001, loss 0.2683, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 5501, loss 0.2739, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 6001, loss 0.1849, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 6501, loss 0.2153, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 7001, loss 0.1315, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 7501, loss 0.2343, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8001, loss 0.2545, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8501, loss 0.2543, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9001, loss 0.1849, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9501, loss 0.2168, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 10001, loss 0.2032, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 10501, loss 0.2139, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 11001, loss 0.2633, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 11501, loss 0.1689, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 12001, loss 0.2167, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 12501, loss 0.2554, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 13001, loss 0.2136, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 13501, loss 0.1319, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 14001, loss 0.2318, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 14501, loss 0.1554, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 15001, loss 0.2285, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 15501, loss 0.2152, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 16001, loss 0.3036, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 16501, loss 0.2077, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 17001, loss 0.2187, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 17501, loss 0.2329, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18001, loss 0.2937, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18501, loss 0.1874, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19001, loss 0.2423, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19501, loss 0.2038, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 20001, loss 0.2091, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 20501, loss 0.2382, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 21001, loss 0.2034, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 21501, loss 0.1933, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 22001, loss 0.1361, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 22501, loss 0.1480, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 23001, loss 0.1028, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 23501, loss 0.1374, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24001, loss 0.2480, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24501, loss 0.1375, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
running_time is 25.411605264000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.25
normal_0.5
./test_glass0/model_MLP_25000_0.25/record_1/MLP_25000_0.25_1
./test_glass0/result_MLP_25000_0.25_normal_0.5/record_1/
----------------------



the AUC is 0.541871921182266

the Fscore is 0.39999999999999997

the precision is 0.375

the recall is 0.42857142857142855

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.25/record_1/MLP_25000_0.25_2
----------------------



epoch 1, loss 0.5514, train acc 62.57%, f1 0.1111, precision 0.2500, recall 0.0714, auc 0.4835
epoch 501, loss 0.3659, train acc 74.85%, f1 0.7114, precision 0.5699, recall 0.9464, auc 0.7993
epoch 1001, loss 0.3669, train acc 80.70%, f1 0.7660, precision 0.6353, recall 0.9643, auc 0.8474
epoch 1501, loss 0.3199, train acc 83.04%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8693
epoch 2001, loss 0.2832, train acc 84.21%, f1 0.8029, precision 0.6790, recall 0.9821, auc 0.8780
epoch 2501, loss 0.3281, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 3001, loss 0.2309, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 3501, loss 0.1600, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 4001, loss 0.1770, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 4501, loss 0.2246, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 5001, loss 0.1865, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 5501, loss 0.1996, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 6001, loss 0.2506, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 6501, loss 0.1996, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 7001, loss 0.2107, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 7501, loss 0.1977, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8001, loss 0.2091, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 8501, loss 0.1915, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9001, loss 0.1423, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 9501, loss 0.2016, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10001, loss 0.1744, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10501, loss 0.2050, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11001, loss 0.1151, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 11501, loss 0.1671, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12001, loss 0.1451, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12501, loss 0.1148, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13001, loss 0.1255, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13501, loss 0.1152, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14001, loss 0.1424, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14501, loss 0.1560, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15001, loss 0.1417, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.1757, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.1555, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.1412, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.1142, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.1228, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.1445, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.1592, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.2016, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.1176, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20001, loss 0.0966, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 20501, loss 0.1674, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21001, loss 0.1332, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 21501, loss 0.1196, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22001, loss 0.1757, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 22501, loss 0.1282, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23001, loss 0.1387, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 23501, loss 0.1153, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24001, loss 0.1841, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 24501, loss 0.1034, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
running_time is 25.719608352999998
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.25
normal_0.5
./test_glass0/model_MLP_25000_0.25/record_1/MLP_25000_0.25_2
./test_glass0/result_MLP_25000_0.25_normal_0.5/record_1/
----------------------



the AUC is 0.4827586206896552

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.25/record_1/MLP_25000_0.25_3
----------------------



epoch 1, loss 0.7051, train acc 34.50%, f1 0.5000, precision 0.3333, recall 1.0000, auc 0.5130
epoch 501, loss 0.4044, train acc 71.93%, f1 0.6842, precision 0.5417, recall 0.9286, auc 0.7730
epoch 1001, loss 0.4106, train acc 74.27%, f1 0.7067, precision 0.5638, recall 0.9464, auc 0.7950
epoch 1501, loss 0.4101, train acc 77.19%, f1 0.7273, precision 0.5977, recall 0.9286, auc 0.8121
epoch 2001, loss 0.3446, train acc 80.70%, f1 0.7626, precision 0.6386, recall 0.9464, auc 0.8428
epoch 2501, loss 0.2401, train acc 85.96%, f1 0.8182, precision 0.7105, recall 0.9643, auc 0.8865
epoch 3001, loss 0.3044, train acc 87.72%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9041
epoch 3501, loss 0.2564, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 4001, loss 0.2027, train acc 87.72%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9041
epoch 4501, loss 0.2512, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 5001, loss 0.2513, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 5501, loss 0.2266, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 6001, loss 0.2944, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 6501, loss 0.1853, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 7001, loss 0.2052, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 7501, loss 0.2227, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 8001, loss 0.1742, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 8501, loss 0.2022, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 9001, loss 0.1645, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 9501, loss 0.1756, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 10001, loss 0.1224, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 10501, loss 0.1484, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 11001, loss 0.2013, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 11501, loss 0.1537, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 12001, loss 0.1877, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 12501, loss 0.2306, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 13001, loss 0.1918, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 13501, loss 0.1840, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 14001, loss 0.1865, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 14501, loss 0.1762, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 15001, loss 0.2310, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 15501, loss 0.1561, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 16001, loss 0.1565, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 16501, loss 0.1634, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 17001, loss 0.1363, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 17501, loss 0.1505, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 18001, loss 0.1473, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 18501, loss 0.1640, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 19001, loss 0.1367, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 19501, loss 0.1457, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 20001, loss 0.1099, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 20501, loss 0.1995, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 21001, loss 0.1545, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 21501, loss 0.1943, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 22001, loss 0.0964, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 22501, loss 0.1664, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 23001, loss 0.1135, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 23501, loss 0.1249, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 24001, loss 0.1896, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 24501, loss 0.1697, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
running_time is 25.571192724000003
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.25
normal_0.5
./test_glass0/model_MLP_25000_0.25/record_1/MLP_25000_0.25_3
./test_glass0/result_MLP_25000_0.25_normal_0.5/record_1/
----------------------



the AUC is 0.6970443349753696

the Fscore is 0.5714285714285714

the precision is 0.8571428571428571

the recall is 0.42857142857142855

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.25/record_1/MLP_25000_0.25_4
----------------------



epoch 1, loss 0.6611, train acc 56.14%, f1 0.5989, precision 0.4275, recall 1.0000, auc 0.6739
epoch 501, loss 0.3971, train acc 74.85%, f1 0.7152, precision 0.5684, recall 0.9643, auc 0.8039
epoch 1001, loss 0.4216, train acc 77.78%, f1 0.7361, precision 0.6023, recall 0.9464, auc 0.8210
epoch 1501, loss 0.3566, train acc 77.78%, f1 0.7361, precision 0.6023, recall 0.9464, auc 0.8210
epoch 2001, loss 0.2987, train acc 81.29%, f1 0.7681, precision 0.6463, recall 0.9464, auc 0.8471
epoch 2501, loss 0.2647, train acc 82.46%, f1 0.7857, precision 0.6548, recall 0.9821, auc 0.8650
epoch 3001, loss 0.2913, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 3501, loss 0.2447, train acc 87.13%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.8998
epoch 4001, loss 0.2404, train acc 88.30%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9085
epoch 4501, loss 0.2703, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 5001, loss 0.2509, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 5501, loss 0.2498, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 6001, loss 0.2279, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 6501, loss 0.1762, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 7001, loss 0.2270, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 7501, loss 0.1797, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 8001, loss 0.2306, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 8501, loss 0.1254, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9001, loss 0.1683, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 9501, loss 0.1996, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10001, loss 0.1792, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10501, loss 0.2258, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 11001, loss 0.1814, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 11501, loss 0.2231, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 12001, loss 0.1747, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 12501, loss 0.2490, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 13001, loss 0.1459, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 13501, loss 0.2388, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 14001, loss 0.1721, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 14501, loss 0.1324, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 15001, loss 0.1624, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 15501, loss 0.1297, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 16001, loss 0.1633, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 16501, loss 0.2431, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 17001, loss 0.2183, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 17501, loss 0.1146, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 18001, loss 0.1205, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 18501, loss 0.2501, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 19001, loss 0.1101, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 19501, loss 0.1749, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 20001, loss 0.2003, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 20501, loss 0.1984, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 21001, loss 0.1706, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 21501, loss 0.1502, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 22001, loss 0.1307, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 22501, loss 0.2144, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 23001, loss 0.2194, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 23501, loss 0.1343, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 24001, loss 0.1657, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 24501, loss 0.1885, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
running_time is 25.374317922000003
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.25
normal_0.5
./test_glass0/model_MLP_25000_0.25/record_1/MLP_25000_0.25_4
./test_glass0/result_MLP_25000_0.25_normal_0.5/record_1/
----------------------



the AUC is 0.5541871921182266

the Fscore is 0.23529411764705882

the precision is 0.6666666666666666

the recall is 0.14285714285714285

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.25/record_1/MLP_25000_0.25_5
----------------------



epoch 1, loss 0.5733, train acc 39.53%, f1 0.4800, precision 0.3333, recall 0.8571, auc 0.5148
epoch 501, loss 0.4238, train acc 71.51%, f1 0.6918, precision 0.5340, recall 0.9821, auc 0.7842
epoch 1001, loss 0.3240, train acc 76.74%, f1 0.7297, precision 0.5870, recall 0.9643, auc 0.8183
epoch 1501, loss 0.2806, train acc 79.07%, f1 0.7500, precision 0.6136, recall 0.9643, auc 0.8356
epoch 2001, loss 0.3382, train acc 81.40%, f1 0.7746, precision 0.6395, recall 0.9821, auc 0.8575
epoch 2501, loss 0.3158, train acc 83.72%, f1 0.8000, precision 0.6667, recall 1.0000, auc 0.8793
epoch 3001, loss 0.2546, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 3501, loss 0.3219, train acc 84.88%, f1 0.8088, precision 0.6875, recall 0.9821, auc 0.8833
epoch 4001, loss 0.3000, train acc 85.47%, f1 0.8148, precision 0.6962, recall 0.9821, auc 0.8876
epoch 4501, loss 0.4300, train acc 86.05%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8966
epoch 5001, loss 0.2185, train acc 86.05%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8966
epoch 5501, loss 0.2596, train acc 87.79%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9095
epoch 6001, loss 0.2071, train acc 88.95%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9181
epoch 6501, loss 0.2283, train acc 89.53%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9224
epoch 7001, loss 0.2280, train acc 91.28%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9353
epoch 7501, loss 0.1936, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 8001, loss 0.2286, train acc 92.44%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9393
epoch 8501, loss 0.2543, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 9001, loss 0.2493, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 9501, loss 0.2239, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 10001, loss 0.1994, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 10501, loss 0.2640, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 11001, loss 0.1872, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 11501, loss 0.2073, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 12001, loss 0.2190, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 12501, loss 0.1738, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 13001, loss 0.1926, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 13501, loss 0.1679, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 14001, loss 0.1831, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 14501, loss 0.2060, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 15001, loss 0.1466, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 15501, loss 0.2682, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 16001, loss 0.1524, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 16501, loss 0.1412, train acc 94.19%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9523
epoch 17001, loss 0.1958, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 17501, loss 0.1896, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 18001, loss 0.1824, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 18501, loss 0.1638, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 19001, loss 0.1441, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 19501, loss 0.1544, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 20001, loss 0.1373, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 20501, loss 0.2283, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 21001, loss 0.1587, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 21501, loss 0.1423, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 22001, loss 0.1945, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 22501, loss 0.1530, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 23001, loss 0.1733, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 23501, loss 0.1709, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 24001, loss 0.1788, train acc 94.77%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9612
epoch 24501, loss 0.1898, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
running_time is 25.40169844
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.25
normal_0.5
./test_glass0/model_MLP_25000_0.25/record_1/MLP_25000_0.25_5
./test_glass0/result_MLP_25000_0.25_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.11764705882352941

the precision is 0.3333333333333333

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.26/record_1/MLP_25000_0.26_1
----------------------



epoch 1, loss 0.5512, train acc 65.50%, f1 0.0635, precision 0.2857, recall 0.0357, auc 0.4961
epoch 501, loss 0.3751, train acc 74.85%, f1 0.7152, precision 0.5684, recall 0.9643, auc 0.8039
epoch 1001, loss 0.3266, train acc 78.36%, f1 0.7448, precision 0.6067, recall 0.9643, auc 0.8300
epoch 1501, loss 0.3400, train acc 81.29%, f1 0.7714, precision 0.6429, recall 0.9643, auc 0.8517
epoch 2001, loss 0.3222, train acc 81.29%, f1 0.7714, precision 0.6429, recall 0.9643, auc 0.8517
epoch 2501, loss 0.2455, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 3001, loss 0.1880, train acc 84.80%, f1 0.8060, precision 0.6923, recall 0.9643, auc 0.8778
epoch 3501, loss 0.3400, train acc 85.38%, f1 0.8148, precision 0.6962, recall 0.9821, auc 0.8867
epoch 4001, loss 0.1841, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 4501, loss 0.2156, train acc 85.38%, f1 0.8175, precision 0.6914, recall 1.0000, auc 0.8913
epoch 5001, loss 0.3505, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 5501, loss 0.2473, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 6001, loss 0.2872, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 6501, loss 0.1786, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 7001, loss 0.1788, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 7501, loss 0.3192, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 8001, loss 0.2175, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 8501, loss 0.2319, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 9001, loss 0.2284, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 9501, loss 0.2204, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 10001, loss 0.1592, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 10501, loss 0.1633, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11001, loss 0.2186, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11501, loss 0.2151, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 12001, loss 0.2069, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 12501, loss 0.1757, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 13001, loss 0.2079, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 13501, loss 0.2044, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 14001, loss 0.2046, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 14501, loss 0.2767, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 15001, loss 0.2857, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 15501, loss 0.1702, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 16001, loss 0.2553, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 16501, loss 0.2283, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 17001, loss 0.1153, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 17501, loss 0.1877, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18001, loss 0.1817, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 18501, loss 0.1394, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19001, loss 0.1526, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19501, loss 0.1796, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 20001, loss 0.1867, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 20501, loss 0.2268, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 21001, loss 0.1714, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 21501, loss 0.1844, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 22001, loss 0.1115, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 22501, loss 0.1743, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 23001, loss 0.1532, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 23501, loss 0.2104, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24001, loss 0.1379, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 24501, loss 0.2224, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
running_time is 25.275910550000003
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.26
normal_0.5
./test_glass0/model_MLP_25000_0.26/record_1/MLP_25000_0.26_1
./test_glass0/result_MLP_25000_0.26_normal_0.5/record_1/
----------------------



the AUC is 0.562807881773399

the Fscore is 0.4736842105263159

the precision is 0.375

the recall is 0.6428571428571429

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.26/record_1/MLP_25000_0.26_2
----------------------



epoch 1, loss 0.5972, train acc 44.44%, f1 0.5411, precision 0.3709, recall 1.0000, auc 0.5870
epoch 501, loss 0.3852, train acc 77.19%, f1 0.7347, precision 0.5934, recall 0.9643, auc 0.8213
epoch 1001, loss 0.4005, train acc 80.70%, f1 0.7692, precision 0.6322, recall 0.9821, auc 0.8519
epoch 1501, loss 0.2740, train acc 83.04%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8693
epoch 2001, loss 0.2935, train acc 83.63%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8737
epoch 2501, loss 0.2516, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 3001, loss 0.1962, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 3501, loss 0.2749, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 4001, loss 0.2542, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 4501, loss 0.2566, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 5001, loss 0.2233, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 5501, loss 0.2016, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 6001, loss 0.1807, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 6501, loss 0.1727, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 7001, loss 0.2121, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 7501, loss 0.1852, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 8001, loss 0.2055, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 8501, loss 0.2019, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9001, loss 0.1754, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9501, loss 0.1875, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10001, loss 0.2017, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 10501, loss 0.1660, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 11001, loss 0.1151, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11501, loss 0.1889, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12001, loss 0.1550, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 12501, loss 0.1363, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13001, loss 0.1689, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13501, loss 0.1583, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14001, loss 0.1709, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 14501, loss 0.1274, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.1150, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.1090, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.0976, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.0946, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.0815, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.1532, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.1377, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.0944, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.1120, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.1134, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20001, loss 0.1334, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20501, loss 0.1063, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21001, loss 0.1094, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21501, loss 0.1208, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22001, loss 0.1344, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22501, loss 0.1002, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23001, loss 0.1331, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23501, loss 0.1056, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24001, loss 0.1585, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
epoch 24501, loss 0.1199, train acc 98.83%, f1 0.9825, precision 0.9655, recall 1.0000, auc 0.9913
running_time is 25.435015565
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.26
normal_0.5
./test_glass0/model_MLP_25000_0.26/record_1/MLP_25000_0.26_2
./test_glass0/result_MLP_25000_0.26_normal_0.5/record_1/
----------------------



the AUC is 0.5

the Fscore is 0.0

the precision is 0.0

the recall is 0.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.26/record_1/MLP_25000_0.26_3
----------------------



epoch 1, loss 0.6894, train acc 35.09%, f1 0.5022, precision 0.3353, recall 1.0000, auc 0.5174
epoch 501, loss 0.4534, train acc 70.76%, f1 0.6875, precision 0.5288, recall 0.9821, auc 0.7780
epoch 1001, loss 0.3276, train acc 73.68%, f1 0.6980, precision 0.5591, recall 0.9286, auc 0.7860
epoch 1501, loss 0.3413, train acc 77.19%, f1 0.7347, precision 0.5934, recall 0.9643, auc 0.8213
epoch 2001, loss 0.3523, train acc 81.29%, f1 0.7714, precision 0.6429, recall 0.9643, auc 0.8517
epoch 2501, loss 0.4145, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 3001, loss 0.2413, train acc 85.96%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8911
epoch 3501, loss 0.3090, train acc 87.72%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9041
epoch 4001, loss 0.1986, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 4501, loss 0.2081, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 5001, loss 0.2711, train acc 90.06%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9215
epoch 5501, loss 0.2182, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 6001, loss 0.2158, train acc 91.23%, f1 0.8819, precision 0.7887, recall 1.0000, auc 0.9348
epoch 6501, loss 0.2259, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 7001, loss 0.2184, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 7501, loss 0.2317, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 8001, loss 0.2397, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 8501, loss 0.2180, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 9001, loss 0.2215, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 9501, loss 0.1828, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10001, loss 0.1806, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 10501, loss 0.1671, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 11001, loss 0.1677, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 11501, loss 0.1989, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12001, loss 0.1326, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 12501, loss 0.1329, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 13001, loss 0.1681, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 13501, loss 0.1743, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 14001, loss 0.1698, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14501, loss 0.1633, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 15001, loss 0.0736, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 15501, loss 0.1189, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16001, loss 0.1508, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 16501, loss 0.1785, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17001, loss 0.1805, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 17501, loss 0.1605, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18001, loss 0.1435, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 18501, loss 0.1631, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19001, loss 0.1657, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 19501, loss 0.1250, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20001, loss 0.0983, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20501, loss 0.1093, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21001, loss 0.0680, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21501, loss 0.1249, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22001, loss 0.1203, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22501, loss 0.1387, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23001, loss 0.0845, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23501, loss 0.0988, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24001, loss 0.1331, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24501, loss 0.1121, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 25.437443281
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.26
normal_0.5
./test_glass0/model_MLP_25000_0.26/record_1/MLP_25000_0.26_3
./test_glass0/result_MLP_25000_0.26_normal_0.5/record_1/
----------------------



the AUC is 0.5160098522167488

the Fscore is 0.4905660377358491

the precision is 0.3333333333333333

the recall is 0.9285714285714286

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.26/record_1/MLP_25000_0.26_4
----------------------



epoch 1, loss 0.6204, train acc 32.75%, f1 0.4934, precision 0.3275, recall 1.0000, auc 0.5000
epoch 501, loss 0.4337, train acc 69.01%, f1 0.6708, precision 0.5143, recall 0.9643, auc 0.7604
epoch 1001, loss 0.3750, train acc 75.44%, f1 0.7200, precision 0.5745, recall 0.9643, auc 0.8082
epoch 1501, loss 0.3357, train acc 78.36%, f1 0.7448, precision 0.6067, recall 0.9643, auc 0.8300
epoch 2001, loss 0.3266, train acc 79.53%, f1 0.7552, precision 0.6207, recall 0.9643, auc 0.8387
epoch 2501, loss 0.3150, train acc 81.29%, f1 0.7746, precision 0.6395, recall 0.9821, auc 0.8563
epoch 3001, loss 0.2756, train acc 80.70%, f1 0.7660, precision 0.6353, recall 0.9643, auc 0.8474
epoch 3501, loss 0.2818, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 4001, loss 0.2733, train acc 83.63%, f1 0.7941, precision 0.6750, recall 0.9643, auc 0.8691
epoch 4501, loss 0.2606, train acc 87.13%, f1 0.8308, precision 0.7297, recall 0.9643, auc 0.8952
epoch 5001, loss 0.2273, train acc 89.47%, f1 0.8571, precision 0.7714, recall 0.9643, auc 0.9126
epoch 5501, loss 0.1826, train acc 88.89%, f1 0.8527, precision 0.7534, recall 0.9821, auc 0.9128
epoch 6001, loss 0.2270, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 6501, loss 0.2436, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 7001, loss 0.1771, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 7501, loss 0.1760, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 8001, loss 0.1786, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 8501, loss 0.2274, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 9001, loss 0.2097, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 9501, loss 0.2193, train acc 91.23%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9302
epoch 10001, loss 0.2484, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 10501, loss 0.2947, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 11001, loss 0.2214, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 11501, loss 0.1612, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 12001, loss 0.1969, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 12501, loss 0.1715, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 13001, loss 0.1469, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 13501, loss 0.1894, train acc 92.98%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9432
epoch 14001, loss 0.1967, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 14501, loss 0.1809, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 15001, loss 0.1884, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 15501, loss 0.1754, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 16001, loss 0.2141, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16501, loss 0.1547, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 17001, loss 0.2502, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17501, loss 0.1954, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18001, loss 0.1416, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 18501, loss 0.1765, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19001, loss 0.1382, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19501, loss 0.1221, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 20001, loss 0.1881, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 20501, loss 0.1157, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 21001, loss 0.1192, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 21501, loss 0.1754, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 22001, loss 0.1855, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 22501, loss 0.1912, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 23001, loss 0.1591, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 23501, loss 0.2175, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 24001, loss 0.1646, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 24501, loss 0.1372, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
running_time is 25.353618409
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.26
normal_0.5
./test_glass0/model_MLP_25000_0.26/record_1/MLP_25000_0.26_4
./test_glass0/result_MLP_25000_0.26_normal_0.5/record_1/
----------------------



the AUC is 0.5012315270935961

the Fscore is 0.11764705882352941

the precision is 0.3333333333333333

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.26/record_1/MLP_25000_0.26_5
----------------------



epoch 1, loss 0.6434, train acc 42.44%, f1 0.5308, precision 0.3613, recall 1.0000, auc 0.5733
epoch 501, loss 0.4190, train acc 75.58%, f1 0.7200, precision 0.5745, recall 0.9643, auc 0.8097
epoch 1001, loss 0.3247, train acc 79.65%, f1 0.7552, precision 0.6207, recall 0.9643, auc 0.8399
epoch 1501, loss 0.4049, train acc 80.23%, f1 0.7606, precision 0.6279, recall 0.9643, auc 0.8442
epoch 2001, loss 0.3487, train acc 81.98%, f1 0.7801, precision 0.6471, recall 0.9821, auc 0.8618
epoch 2501, loss 0.2976, train acc 83.14%, f1 0.7914, precision 0.6627, recall 0.9821, auc 0.8704
epoch 3001, loss 0.2835, train acc 83.72%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8747
epoch 3501, loss 0.3014, train acc 83.72%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8747
epoch 4001, loss 0.3416, train acc 84.30%, f1 0.8029, precision 0.6790, recall 0.9821, auc 0.8790
epoch 4501, loss 0.2643, train acc 83.72%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8747
epoch 5001, loss 0.3169, train acc 83.72%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8747
epoch 5501, loss 0.2693, train acc 84.30%, f1 0.8029, precision 0.6790, recall 0.9821, auc 0.8790
epoch 6001, loss 0.2154, train acc 84.88%, f1 0.8088, precision 0.6875, recall 0.9821, auc 0.8833
epoch 6501, loss 0.2550, train acc 86.63%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8962
epoch 7001, loss 0.2638, train acc 86.63%, f1 0.8244, precision 0.7200, recall 0.9643, auc 0.8916
epoch 7501, loss 0.2564, train acc 88.37%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9092
epoch 8001, loss 0.2471, train acc 88.95%, f1 0.8504, precision 0.7606, recall 0.9643, auc 0.9089
epoch 8501, loss 0.2608, train acc 90.70%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9310
epoch 9001, loss 0.1944, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 9501, loss 0.2168, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 10001, loss 0.2126, train acc 91.86%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9350
epoch 10501, loss 0.1973, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 11001, loss 0.2882, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 11501, loss 0.2151, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 12001, loss 0.1831, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 12501, loss 0.1678, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 13001, loss 0.1778, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 13501, loss 0.2113, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 14001, loss 0.2355, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 14501, loss 0.2115, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 15001, loss 0.1697, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 15501, loss 0.2550, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 16001, loss 0.1626, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 16501, loss 0.1881, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 17001, loss 0.2446, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 17501, loss 0.1647, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 18001, loss 0.2532, train acc 93.02%, f1 0.9016, precision 0.8333, recall 0.9821, auc 0.9437
epoch 18501, loss 0.2119, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 19001, loss 0.2139, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 19501, loss 0.1239, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 20001, loss 0.1691, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 20501, loss 0.2215, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 21001, loss 0.1955, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 21501, loss 0.1945, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 22001, loss 0.1729, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 22501, loss 0.2113, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 23001, loss 0.1407, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 23501, loss 0.1709, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 24001, loss 0.1729, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 24501, loss 0.1549, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
running_time is 25.538932731
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.26
normal_0.5
./test_glass0/model_MLP_25000_0.26/record_1/MLP_25000_0.26_5
./test_glass0/result_MLP_25000_0.26_normal_0.5/record_1/
----------------------



the AUC is 0.6071428571428571

the Fscore is 0.4

the precision is 0.6666666666666666

the recall is 0.2857142857142857

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/model_MLP_25000_0.27/record_1/MLP_25000_0.27_1
----------------------



epoch 1, loss 0.5997, train acc 40.35%, f1 0.5234, precision 0.3544, recall 1.0000, auc 0.5565
epoch 501, loss 0.3398, train acc 76.02%, f1 0.7248, precision 0.5806, recall 0.9643, auc 0.8126
epoch 1001, loss 0.4032, train acc 80.70%, f1 0.7660, precision 0.6353, recall 0.9643, auc 0.8474
epoch 1501, loss 0.3112, train acc 78.95%, f1 0.7500, precision 0.6136, recall 0.9643, auc 0.8343
epoch 2001, loss 0.2705, train acc 81.87%, f1 0.7770, precision 0.6506, recall 0.9643, auc 0.8561
epoch 2501, loss 0.4207, train acc 83.63%, f1 0.7910, precision 0.6795, recall 0.9464, auc 0.8645
epoch 3001, loss 0.2340, train acc 84.80%, f1 0.8088, precision 0.6875, recall 0.9821, auc 0.8824
epoch 3501, loss 0.2845, train acc 86.55%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8954
epoch 4001, loss 0.3385, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 4501, loss 0.2058, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 5001, loss 0.2309, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 5501, loss 0.2662, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 6001, loss 0.2841, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 6501, loss 0.2876, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 7001, loss 0.3984, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 7501, loss 0.2455, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 8001, loss 0.1971, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 8501, loss 0.2305, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 9001, loss 0.2882, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 9501, loss 0.2072, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 10001, loss 0.1920, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 10501, loss 0.2871, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 11001, loss 0.2447, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 11501, loss 0.1705, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 12001, loss 0.1741, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 12501, loss 0.1753, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 13001, loss 0.2295, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 13501, loss 0.2105, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 14001, loss 0.2527, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 14501, loss 0.1752, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 15001, loss 0.2020, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 15501, loss 0.2795, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 16001, loss 0.1321, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 16501, loss 0.1093, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 17001, loss 0.1595, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 17501, loss 0.1611, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 18001, loss 0.2529, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 18501, loss 0.2204, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 19001, loss 0.1605, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 19501, loss 0.2138, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 20001, loss 0.0977, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 20501, loss 0.2257, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 21001, loss 0.2259, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 21501, loss 0.2391, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 22001, loss 0.1692, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 22501, loss 0.1873, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 23001, loss 0.1147, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 23501, loss 0.1993, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 24001, loss 0.2388, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
epoch 24501, loss 0.2186, train acc 94.15%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9565
running_time is 25.652076361000002
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_1.csv
./test_glass0/standlization_data/glass0_std_test_1.csv
MLP_25000_0.27
normal_0.5
./test_glass0/model_MLP_25000_0.27/record_1/MLP_25000_0.27_1
./test_glass0/result_MLP_25000_0.27_normal_0.5/record_1/
----------------------



the AUC is 0.603448275862069

the Fscore is 0.5490196078431372

the precision is 0.3783783783783784

the recall is 1.0

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/model_MLP_25000_0.27/record_1/MLP_25000_0.27_2
----------------------



epoch 1, loss 0.6479, train acc 40.35%, f1 0.5189, precision 0.3526, recall 0.9821, auc 0.5519
epoch 501, loss 0.4608, train acc 73.10%, f1 0.7051, precision 0.5500, recall 0.9821, auc 0.7954
epoch 1001, loss 0.2959, train acc 81.87%, f1 0.7801, precision 0.6471, recall 0.9821, auc 0.8606
epoch 1501, loss 0.3373, train acc 84.21%, f1 0.8058, precision 0.6747, recall 1.0000, auc 0.8826
epoch 2001, loss 0.2770, train acc 87.13%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9043
epoch 2501, loss 0.1768, train acc 88.30%, f1 0.8485, precision 0.7368, recall 1.0000, auc 0.9130
epoch 3001, loss 0.2688, train acc 91.81%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9391
epoch 3501, loss 0.2398, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 4001, loss 0.2077, train acc 92.40%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9435
epoch 4501, loss 0.1954, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 5001, loss 0.1548, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 5501, loss 0.1258, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 6001, loss 0.1633, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 6501, loss 0.2380, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 7001, loss 0.1359, train acc 92.98%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9478
epoch 7501, loss 0.1971, train acc 93.57%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9522
epoch 8001, loss 0.1350, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 8501, loss 0.1221, train acc 94.74%, f1 0.9256, precision 0.8615, recall 1.0000, auc 0.9609
epoch 9001, loss 0.1732, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 9501, loss 0.1552, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10001, loss 0.1289, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 10501, loss 0.1463, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11001, loss 0.1510, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 11501, loss 0.1964, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12001, loss 0.1415, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 12501, loss 0.1018, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13001, loss 0.1484, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 13501, loss 0.1522, train acc 97.66%, f1 0.9655, precision 0.9333, recall 1.0000, auc 0.9826
epoch 14001, loss 0.1124, train acc 97.08%, f1 0.9573, precision 0.9180, recall 1.0000, auc 0.9783
epoch 14501, loss 0.1768, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15001, loss 0.1572, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 15501, loss 0.1532, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16001, loss 0.1363, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 16501, loss 0.1703, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17001, loss 0.1237, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 17501, loss 0.1377, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18001, loss 0.1530, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 18501, loss 0.1121, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19001, loss 0.1497, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 19501, loss 0.1625, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20001, loss 0.1115, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 20501, loss 0.1535, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21001, loss 0.1114, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 21501, loss 0.1292, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22001, loss 0.0827, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 22501, loss 0.1499, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23001, loss 0.1144, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 23501, loss 0.1739, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24001, loss 0.1778, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
epoch 24501, loss 0.1002, train acc 98.25%, f1 0.9739, precision 0.9492, recall 1.0000, auc 0.9870
running_time is 25.574258535
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_2.csv
./test_glass0/standlization_data/glass0_std_test_2.csv
MLP_25000_0.27
normal_0.5
./test_glass0/model_MLP_25000_0.27/record_1/MLP_25000_0.27_2
./test_glass0/result_MLP_25000_0.27_normal_0.5/record_1/
----------------------



the AUC is 0.5012315270935961

the Fscore is 0.11764705882352941

the precision is 0.3333333333333333

the recall is 0.07142857142857142

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/model_MLP_25000_0.27/record_1/MLP_25000_0.27_3
----------------------



epoch 1, loss 0.7205, train acc 33.92%, f1 0.4978, precision 0.3314, recall 1.0000, auc 0.5087
epoch 501, loss 0.3408, train acc 71.93%, f1 0.6962, precision 0.5392, recall 0.9821, auc 0.7867
epoch 1001, loss 0.4405, train acc 73.68%, f1 0.7059, precision 0.5567, recall 0.9643, auc 0.7952
epoch 1501, loss 0.3844, train acc 77.19%, f1 0.7347, precision 0.5934, recall 0.9643, auc 0.8213
epoch 2001, loss 0.3837, train acc 82.46%, f1 0.7826, precision 0.6585, recall 0.9643, auc 0.8604
epoch 2501, loss 0.2922, train acc 83.63%, f1 0.7971, precision 0.6707, recall 0.9821, auc 0.8737
epoch 3001, loss 0.2520, train acc 85.96%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8911
epoch 3501, loss 0.2629, train acc 87.13%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.8998
epoch 4001, loss 0.2271, train acc 87.13%, f1 0.8333, precision 0.7237, recall 0.9821, auc 0.8998
epoch 4501, loss 0.2571, train acc 87.72%, f1 0.8397, precision 0.7333, recall 0.9821, auc 0.9041
epoch 5001, loss 0.2440, train acc 89.47%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9172
epoch 5501, loss 0.1983, train acc 91.81%, f1 0.8871, precision 0.8088, recall 0.9821, auc 0.9345
epoch 6001, loss 0.1880, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 6501, loss 0.1609, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 7001, loss 0.2133, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 7501, loss 0.1731, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 8001, loss 0.2113, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 8501, loss 0.1815, train acc 92.40%, f1 0.8943, precision 0.8209, recall 0.9821, auc 0.9389
epoch 9001, loss 0.1718, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 9501, loss 0.1720, train acc 93.57%, f1 0.9091, precision 0.8462, recall 0.9821, auc 0.9476
epoch 10001, loss 0.1470, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 10501, loss 0.1199, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 11001, loss 0.1759, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 11501, loss 0.2150, train acc 94.15%, f1 0.9167, precision 0.8594, recall 0.9821, auc 0.9519
epoch 12001, loss 0.2000, train acc 94.74%, f1 0.9244, precision 0.8730, recall 0.9821, auc 0.9563
epoch 12501, loss 0.1942, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 13001, loss 0.1747, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 13501, loss 0.1742, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 14001, loss 0.1124, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 14501, loss 0.1839, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 15001, loss 0.1598, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 15501, loss 0.1615, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 16001, loss 0.1913, train acc 95.32%, f1 0.9333, precision 0.8750, recall 1.0000, auc 0.9652
epoch 16501, loss 0.1899, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 17001, loss 0.1725, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 17501, loss 0.1403, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 18001, loss 0.1723, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 18501, loss 0.1776, train acc 95.91%, f1 0.9402, precision 0.9016, recall 0.9821, auc 0.9650
epoch 19001, loss 0.1710, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 19501, loss 0.1251, train acc 95.32%, f1 0.9322, precision 0.8871, recall 0.9821, auc 0.9606
epoch 20001, loss 0.1806, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 20501, loss 0.1439, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 21001, loss 0.0811, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 21501, loss 0.1486, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 22001, loss 0.1825, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 22501, loss 0.2103, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 23001, loss 0.1524, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 23501, loss 0.1206, train acc 96.49%, f1 0.9492, precision 0.9032, recall 1.0000, auc 0.9739
epoch 24001, loss 0.1545, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
epoch 24501, loss 0.1448, train acc 95.91%, f1 0.9412, precision 0.8889, recall 1.0000, auc 0.9696
running_time is 25.481738531
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_3.csv
./test_glass0/standlization_data/glass0_std_test_3.csv
MLP_25000_0.27
normal_0.5
./test_glass0/model_MLP_25000_0.27/record_1/MLP_25000_0.27_3
./test_glass0/result_MLP_25000_0.27_normal_0.5/record_1/
----------------------



the AUC is 0.7216748768472907

the Fscore is 0.631578947368421

the precision is 0.5

the recall is 0.8571428571428571

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/model_MLP_25000_0.27/record_1/MLP_25000_0.27_4
----------------------



epoch 1, loss 0.5997, train acc 38.60%, f1 0.5161, precision 0.3478, recall 1.0000, auc 0.5435
epoch 501, loss 0.3972, train acc 71.93%, f1 0.6923, precision 0.5400, recall 0.9643, auc 0.7821
epoch 1001, loss 0.3196, train acc 76.02%, f1 0.7211, precision 0.5824, recall 0.9464, auc 0.8080
epoch 1501, loss 0.3575, train acc 77.78%, f1 0.7361, precision 0.6023, recall 0.9464, auc 0.8210
epoch 2001, loss 0.4616, train acc 81.87%, f1 0.7770, precision 0.6506, recall 0.9643, auc 0.8561
epoch 2501, loss 0.2606, train acc 81.87%, f1 0.7801, precision 0.6471, recall 0.9821, auc 0.8606
epoch 3001, loss 0.3206, train acc 81.87%, f1 0.7801, precision 0.6471, recall 0.9821, auc 0.8606
epoch 3501, loss 0.3348, train acc 84.21%, f1 0.8029, precision 0.6790, recall 0.9821, auc 0.8780
epoch 4001, loss 0.3084, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 4501, loss 0.3623, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 5001, loss 0.1784, train acc 86.55%, f1 0.8296, precision 0.7089, recall 1.0000, auc 0.9000
epoch 5501, loss 0.3158, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 6001, loss 0.2333, train acc 85.96%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8957
epoch 6501, loss 0.2912, train acc 87.72%, f1 0.8421, precision 0.7273, recall 1.0000, auc 0.9087
epoch 7001, loss 0.3228, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 7501, loss 0.2223, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 8001, loss 0.1956, train acc 88.89%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9174
epoch 8501, loss 0.3081, train acc 89.47%, f1 0.8615, precision 0.7568, recall 1.0000, auc 0.9217
epoch 9001, loss 0.2707, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 9501, loss 0.2986, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 10001, loss 0.3376, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 10501, loss 0.1873, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 11001, loss 0.3239, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 11501, loss 0.1752, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 12001, loss 0.1512, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 12501, loss 0.2838, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 13001, loss 0.2427, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 13501, loss 0.2046, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 14001, loss 0.2107, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 14501, loss 0.2586, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 15001, loss 0.1998, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 15501, loss 0.1786, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 16001, loss 0.1783, train acc 90.06%, f1 0.8682, precision 0.7671, recall 1.0000, auc 0.9261
epoch 16501, loss 0.2300, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 17001, loss 0.2354, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 17501, loss 0.2998, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 18001, loss 0.2015, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 18501, loss 0.3762, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 19001, loss 0.1894, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 19501, loss 0.3052, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 20001, loss 0.2791, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 20501, loss 0.2727, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 21001, loss 0.2474, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 21501, loss 0.2175, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 22001, loss 0.1836, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 22501, loss 0.1972, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 23001, loss 0.2161, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 23501, loss 0.2646, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 24001, loss 0.2275, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
epoch 24501, loss 0.2159, train acc 90.64%, f1 0.8750, precision 0.7778, recall 1.0000, auc 0.9304
running_time is 25.540611201
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_4.csv
./test_glass0/standlization_data/glass0_std_test_4.csv
MLP_25000_0.27
normal_0.5
./test_glass0/model_MLP_25000_0.27/record_1/MLP_25000_0.27_4
./test_glass0/result_MLP_25000_0.27_normal_0.5/record_1/
----------------------



the AUC is 0.8078817733990148

the Fscore is 0.7272727272727273

the precision is 0.631578947368421

the recall is 0.8571428571428571

Done
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/model_MLP_25000_0.27/record_1/MLP_25000_0.27_5
----------------------



epoch 1, loss 0.6724, train acc 32.56%, f1 0.4912, precision 0.3256, recall 1.0000, auc 0.5000
epoch 501, loss 0.4271, train acc 69.77%, f1 0.6829, precision 0.5185, recall 1.0000, auc 0.7759
epoch 1001, loss 0.3577, train acc 76.74%, f1 0.7297, precision 0.5870, recall 0.9643, auc 0.8183
epoch 1501, loss 0.3463, train acc 78.49%, f1 0.7448, precision 0.6067, recall 0.9643, auc 0.8313
epoch 2001, loss 0.3134, train acc 81.40%, f1 0.7714, precision 0.6429, recall 0.9643, auc 0.8528
epoch 2501, loss 0.3215, train acc 83.14%, f1 0.7943, precision 0.6588, recall 1.0000, auc 0.8750
epoch 3001, loss 0.3363, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 3501, loss 0.2564, train acc 84.88%, f1 0.8116, precision 0.6829, recall 1.0000, auc 0.8879
epoch 4001, loss 0.2298, train acc 86.05%, f1 0.8235, precision 0.7000, recall 1.0000, auc 0.8966
epoch 4501, loss 0.3216, train acc 86.05%, f1 0.8209, precision 0.7051, recall 0.9821, auc 0.8919
epoch 5001, loss 0.2505, train acc 86.63%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8962
epoch 5501, loss 0.2550, train acc 86.63%, f1 0.8271, precision 0.7143, recall 0.9821, auc 0.8962
epoch 6001, loss 0.2629, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 6501, loss 0.2771, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 7001, loss 0.2511, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 7501, loss 0.3849, train acc 87.21%, f1 0.8358, precision 0.7179, recall 1.0000, auc 0.9052
epoch 8001, loss 0.1833, train acc 88.95%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9181
epoch 8501, loss 0.2491, train acc 88.95%, f1 0.8550, precision 0.7467, recall 1.0000, auc 0.9181
epoch 9001, loss 0.2139, train acc 88.37%, f1 0.8462, precision 0.7432, recall 0.9821, auc 0.9092
epoch 9501, loss 0.2281, train acc 90.12%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9221
epoch 10001, loss 0.2618, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 10501, loss 0.2367, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 11001, loss 0.2611, train acc 89.53%, f1 0.8594, precision 0.7639, recall 0.9821, auc 0.9178
epoch 11501, loss 0.2181, train acc 90.12%, f1 0.8661, precision 0.7746, recall 0.9821, auc 0.9221
epoch 12001, loss 0.2183, train acc 91.28%, f1 0.8800, precision 0.7971, recall 0.9821, auc 0.9307
epoch 12501, loss 0.2217, train acc 91.86%, f1 0.8889, precision 0.8000, recall 1.0000, auc 0.9397
epoch 13001, loss 0.2359, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 13501, loss 0.1990, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 14001, loss 0.1782, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 14501, loss 0.2094, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 15001, loss 0.1413, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 15501, loss 0.2118, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 16001, loss 0.2165, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 16501, loss 0.2242, train acc 92.44%, f1 0.8960, precision 0.8116, recall 1.0000, auc 0.9440
epoch 17001, loss 0.1027, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 17501, loss 0.2160, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 18001, loss 0.2422, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 18501, loss 0.1923, train acc 93.02%, f1 0.9032, precision 0.8235, recall 1.0000, auc 0.9483
epoch 19001, loss 0.1734, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 19501, loss 0.2401, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 20001, loss 0.2065, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 20501, loss 0.2276, train acc 93.60%, f1 0.9106, precision 0.8358, recall 1.0000, auc 0.9526
epoch 21001, loss 0.1895, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 21501, loss 0.2095, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 22001, loss 0.1508, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 22501, loss 0.2483, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 23001, loss 0.2458, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 23501, loss 0.1488, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 24001, loss 0.1959, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
epoch 24501, loss 0.1946, train acc 94.19%, f1 0.9180, precision 0.8485, recall 1.0000, auc 0.9569
running_time is 25.324081312
/home/z5102138/anaconda3/envs/py36/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
./test_glass0/standlization_data/glass0_std_train_5.csv
./test_glass0/standlization_data/glass0_std_test_5.csv
MLP_25000_0.27
normal_0.5
./test_glass0/model_MLP_25000_0.27/record_1/MLP_25000_0.27_5
./test_glass0/result_MLP_25000_0.27_normal_0.5/record_1/
----------------------



the AUC is 0.767857142857143

the Fscore is 0.6842105263157894

the precision is 0.5416666666666666

the recall is 0.9285714285714286

Done
